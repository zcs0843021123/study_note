# 网络信息分析与 PageRank 分析

# 网络信息分析

聚类分析

```
聚类分析：
聚类是统计学上的概念，现在属于机器学习中非监督学习的范畴
物以类聚 => 同类相同、异类相异
聚类与分类的区别：
分类是按照已定的模式和标准进行划分，也就是说，在进行分类之前，我们事先已经有了一套划分的标准
聚类，并不知道具体的划分标准，要靠算法进行判断数据之间的相似性，把相似的数据放在一起
```

Project A：给18支亚洲球队聚类
连通图与强连通分量

```
K-Means工作原理

Step1, 选取K个点作为初始的类中心点，这些点一般都是从数据集中随机抽取的；
Step2, 将每个点分配到最近的类中心点，这样就形成了K个类，然后重新计算每个类的中心点；
重复Step2，直到类不发生变化，或者你也可以设置最大迭代次数，这样即使类中心点发生变化，但是只要达到最大迭代次数就会结束。

距离的定义
两点之间距离的定义：
欧氏距离
曼哈顿距离

国家	2019国际排名	2018世界杯排名	2015亚洲杯排名
中国	0.3	0.714285714	0.375
日本	0.2	0	0.25
韩国	0.207692308	0.114285714	0.0625
伊朗	0	0.085714286	0.3125
沙特	0.253846154	0.314285714	0.5625
伊拉克	0.438461538	0.714285714	0.1875
卡塔尔	0.515384615	0.714285714	0.75
阿联酋	0.361538462	0.714285714	0.3125
乌兹别克斯坦	0.415384615	0.714285714	0.4375
泰国	0.676923077	0.714285714	1
越南	0.523076923	1	1
阿曼	0.407692308	1	0.6875
朝鲜	0.584615385	1	0.8125
印尼	1	1	1
澳洲	0.046153846	0.428571429	0
叙利亚	0.323076923	0.714285714	1
约旦	0.646153846	1	0.5
科威特	0.969230769	1	0.875

数据规范化的方式：
Min-max规范化
将原始数据投射到指定的空间[min,max]
新数值 = （原数值-极小值）/ (极大值 - 极小值)
当min=0, max=1时，为[0,1]规范化 
sklearn中的MinMaxScaler

Z-Score规范化
将原始数据转换为正态分布的形式
新数值 = （原数值 - 均值）/ 标准差
sklearn中的preprocessing.scale()

小数定标规范化
通过移动小数点的位置来进行规范化
使用numpy

数据规范化的方式：
Min-max规范化
进行[0, 1]规范化之后，可以让不同维度的特征数据，在同一标准下可以进行比较

# 数据加载
data = pd.read_csv('team_cluster_data.csv', encoding='gbk')
train_x = data[["2019国际排名","2018世界杯排名","2015亚洲杯排名"]]
kmeans = KMeans(n_clusters=3)
# 规范化到[0,1]空间
min_max_scaler=preprocessing.MinMaxScaler()

聚类是无监督的学习，具体含义需要我们指定

什么时候使用聚类：
缺乏足够的先验知识
人工打标签太贵

有向图：边有方向的图，也就是A->B != B->A
无向图：没有方向的图，也就是有向图去掉所有方向
入度：有向图某个顶点作为终点的次数和。
出度：有向图某个顶点作为起点的次数和。
入度+出度称之为节点的度

无向图的连通图：
顶点的连通性，在无向图G中，若从顶点vi到顶点vj有路径(同样从vj到vi也一定有路径)，则称vi和vj是连通的
在无向图G中，若V(G)中任意两个不同的顶点vi和vj都连通(即有路径)，则称G为连通图


有向图的连通图：
强连通，在有向图G中，如果两个顶点vi,vj间（vi>vj）有一条从vi到vj的有向路径，同时还有一条从vj到vi的有向路径，则称两个顶点强连通
强连通图，有向图G的每两个顶点都强连通
强连通分量，有向非强连通图的极大强连通子图，称为强连通分量（strongly connected components，scc）
极大强连通子图G是一个极大强连通子图，当且仅当G是一个强连通子图且不存在另一个强连通子图G’，使得G是G’的真子集

Kosaraju算法：
有向图G中的每一条边反向形成的图称为G的转置     。原图G和      的强连通分量是相等的
正向遍历一次再反向遍历一次，如果发现这些节点还是可以互相访问那么就说明这是一个强连通图：
Step1，深度优先遍历G,算出每个结点u的结束时间f[u]（起点如何选择无所谓）
Step2，深度优先遍历    （G的转置图）选择遍历的起点时，按照结点的结束时间从大到小进行。一边遍历，一边给结点做分类标记（每找到一个新的起点，分类标记值就加1）
标记值相同的节点即为一个强连通分量

Kosaraju算法证明：
一个强连通分量C如果有到达另一个强连通分量C’的路径，则C’比C先被搜索完。
如果C中有路径到C’，那么根据DFS性质一定会先搜索到C，再搜索完C’，再搜索完C
如果一个强连通分量C有到达另一个强连通分量C’的路径，则将图转置后，C比C’先被搜索完
第一次DFS，后序遍历的顺序，子节点一定比其父节点先结束 => 大体上可以认为深度大的节点先结束
第二次DFS，每个顶点能够搜索到的点就是一个强连通分量

igraph：处理复杂网络问题，提供Python, R, C语言接口
性能强大，效率比NetworkX高
NetworkX：基于python的复杂网络库
对于Python使用者友好

import networkx as nx
图的创建
无向图，使用nx.Graph()来创建
有向图，使用nx.DiGraph()来创建
节点的增加、删除和查询
添加节点：使用G.add_node('A')，也可以使用G.add_nodes_from(['B','C','D','E'])
删除节点：使用G.remove_node(node)，也可以使用G.remove_nodes_from(['B','C','D','E'])

边的增加
G.add_edge("A", "B")添加指定的从A到B的边
G.add_edges_from 从边集合中添加
G.add_weighted_edges_from 从带有权重的边的集合中添加
参数为1个或多个三元组[u,v,w]作为参数，u、v、w分别代表起点、终点和权重
边的删除
G.remove_edge，G.remove_edges_from

import networkx as nx
import matplotlib.pyplot as plt
# 创建有向图
G = nx.DiGraph()   
# 设置有向图的边集合
edges = [("A", "B"), ("A", "C"), ("A", "D"), ("B", "A"), ("B", "D"), ("C", "A"), ("D", "B"), ("D", "C")]
# 在有向图G中添加边集合
for edge in edges:
    G.add_edge(edge[0], edge[1]) 
    
import networkx as nx
import matplotlib.pyplot as plt
# 创建有向图
G = nx.DiGraph()   
# 设置有向图的边集合
edges = [("A", "B"), ("A", "C"), ("A", "D"), ("B", "A"), ("B", "D"), ("C", "A"), ("D", "B"), ("D", "C")]
# 在有向图G中添加边集合
for edge in edges:
    G.add_edge(edge[0], edge[1]) 
    
NetworkX的可视化布局：
spring_layout：中心放射状
circular_layout：在一个圆环上均匀分布节点
random_layout：随机分布节点 
shell_layout：节点都在同心圆上 

import networkx as nx
import matplotlib.pyplot as plt
# 创建有向图
G = nx.DiGraph() 
# 在图中添加点
G.add_nodes_from(['a','b','c','d','e','f','g','h'])
G.add_edges_from([('a','b'),('b','c'),('b','c'),('c','d'),('d','c'), ('e','a'),('b','e'),('b','f'),('e','f'),('f','g'),('g','f'),('c','g'),('h','g'),('d','h'),('h','d')])
# 有向图可视化
layout = nx.spring_layout(G)
nx.draw(G, pos=layout, with_labels=True, hold=False)
plt.show()
for c in nx.strongly_connected_components(G):
    print(c)
    
Python爬虫工具：
BeautifulSoup，Python Html 解析库，BeautifulSoup 将复杂 HTML 文档转换成一个复杂的树形结构，每个节点都是 Python 对象。
Selenium，Web应用的自动测试工具，可以直接运行在浏览器中，原理是模拟用户在进行操作，支持当前多种主流的浏览器
Scrapy，Python爬虫框架，把基础爬虫功能抽象出来做成的脚手架，Selenium可以应用在Scrapy爬虫框架中
Puppeteer，可以控制Headless Chrome，即无界面的自动化测试框架，支持模拟键盘输入、截图、表单提交等特殊场景操作，Google2018年推出的爬虫神器

BeautifulSoup：
特点是简单易用，相比之下如果使用正则和 xPath 则需要记住特定语法
使用流程：
Step1，通过文本初始化 bs 对象
Step2，通过 find/find_all 查找信息
可以迭代式的查找，比如先定位出一段内容，再其上继续检索

BeautifulSoup的解析器：
html.parse，python 自带，容错性不高，对于不太规范的网页会丢失部分内容
lxml，解析速度快，需额外安装
xml，同属 lxml 库，支持 XML 文档
html5lib，最好的容错性，但速度稍慢

#待分析字符串
html_doc = """
<html>
<head>
    <title>BeautifulSoup Demo</title>
</head>
<body>
<p class="title aq">
    <b>Here is the content</b>
</p>
<p class="story">Python爬虫有很多优秀的工具
    <a href="https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/" class="tools" id="link1">BeautifulSoup</a>,
    <a href="http://www.selenium.org.cn/" class="sisttoolser" id="link2">Selenium</a> and
    <a href="https://scrapy.org/" class="tools" id="link3">Scrapy</a>
    他们都可以帮你爬虫想要的页面内容
</p>
<p class="story">...</p>
"""

# 通过html字符串创建BeautifulSoup对象
soup = BeautifulSoup(html_doc, 'html.parser', from_encoding='utf-8')
#输出第一个 title 标签
print(soup.title)
#输出第一个 title 标签的标签名称
print(soup.title.name)
#输出第一个 title 标签的包含内容
print(soup.title.string)
#输出第一个 title 标签的父标签的标签名称
print(soup.title.parent.name)

import requests
from bs4 import BeautifulSoup
# 请求URL
url = 'https://beijing.anjuke.com/sale/p1'
# 得到页面的内容
headers={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'}
html=requests.get(url,headers=headers,timeout=10)
content = html.text
# 通过content创建BeautifulSoup对象
soup = BeautifulSoup(content, 'html.parser', from_encoding='utf-8')

#输出第一个 title 标签
print(soup.title)
#输出第一个 title 标签的标签名称
print(soup.title.name)
#输出第一个 title 标签的包含内容
print(soup.title.string)

#输出第一个p标签
print(soup.p)
 #输出第一个  p 标签的 class 属性内容
print(soup.p['class'])
#输出第一个  a 标签的  href 属性内容
print(soup.a['href'])
# soup的属性操作方法与字典一样，可以被添加,删除或修改. 
# 修改第一个 a 标签的href属性为 http://www.baidu.com/
soup.a['href'] = 'http://www.baidu.com/'
#给第一个 a 标签添加 name 属性
soup.a['name'] = u'百度'
print(soup.a)
#删除第一个 a 标签的 class 属性为
del soup.a['class']
print(soup.a)

<p class="title aq">
<b>
        Here is the content
    </b>
</p>
['title', 'aq']
https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/
<a class="tools" href="http://www.baidu.com/" id="link1" name="百度">BeautifulSoup</a>
<a href="http://www.baidu.com/" id="link1" name="百度">BeautifulSoup</a>


抓取安居客北京二手房的信息
Step1，确定URL，https://beijing.anjuke.com/sale/p1
Step2，确定要提取的元素
标题name，详细信息 details，地址 address，标签 tags
经纪人broker ，总价 price，每平米价格 unit
Step3，基于要提取的元素，定位html的标签

# 找到class="list-item"下面的所有li标签
house_list=soup.find_all('li',class_="list-item")
# 提取房源信息
for house in house_list:
    # 提取房源信息
    name=house.find('div',class_="house-title").a.text.strip()
    information=house.find('div',class_='details-item').text.strip()
    address=house.find('span',class_="comm-address").text.strip()
    tags=house.find('div',class_='tags-bottom').text.strip()
    broker=house.find('div',class_="broker-item").text.strip()
    price=house.find('span',class_="price-det").text.strip()
    unit=house.find('span',class_="unit-price").text.strip()
    print(name, information, address, tags, broker, price, unit)
    
    
急售房价可议，新兴家园南北通透大两居，一梯两户，中高楼层。 2室1厅|90m²|高层(共10层)|2012年建造 新兴家园(西区)  
                    大兴-磁各庄-海鑫北路15号  秦广存 255万 28215元/m²
满五一套 高楼层采光充足 户型方正 双阳台 2室2厅|86m²|高层(共6层)|2004年建造 佳运园(三区)  
                    昌平-立水桥-立军路  龙其周 390万 44900元/m²
……
中关村壹号旁边，北清路上，百旺府两居室，全南向，中高楼层 2室1厅|86m²|中层(共9层)|2008年建造 永丰嘉园(百旺杏林湾)  
                    海淀-西北旺-北清路 品质小区素质住户低密度 高颜敏 545万 62796元/m²
                    

```

Python Request构建爬虫
BeautifulSoup使用
Selenium使用

```
Selenium工具：
from selenium import webdriver
下载对应版本的ChromeDriver http://chromedriver.storage.googleapis.com/index.html
将下载的ChromeDriver放到某目录下，Python程序中指定chrome_driver路径即可driver=webdriver.Chrome(executable_path=chrome_driver)
WebDriver实际上就是Selenium 2，是一种用于Web应用程序的自动测试工具，提供了一套方便操作的API
通过WebDriver创建一个Chrome浏览器的drive，再通过drive获取访问页面的完整HTML
```

Project B：安居客房价抓取

```
# 抓取10页北京房价数据
page_num = 10
base_url = 'https://beijing.anjuke.com/community/p'
houses = pd.DataFrame(columns = ['title', 'link', 'address', 'price', 'to_last_month', 'completion_date'])
for i in range(page_num):
    request_url = base_url+str(i+1)
    print(request_url)
    # 抓取该页的房源信息
    temp = work(request_url)
    houses = houses.append(temp)
    print(temp)
houses.to_csv('house_prices_bj.csv')

# 对页面进行抓取分析
def work(request_url):
    driver.get(request_url)
    time.sleep(1)
    html = driver.find_element_by_xpath("//*").get_attribute("outerHTML")
    html = etree.HTML(html)
    # 设置需要抓取字段的xpath
    titles = html.xpath("/html/body/div[@class='w1180']/div[@class='maincontent']/div[@id='list-content']/div[@class='li-itemmod']/div[@class='li-info']//a/@title")
    links = html.xpath("/html/body/div[@class='w1180']/div[@class='maincontent']/div[@id='list-content']/div[@class='li-itemmod']/div[@class='li-info']//a/@href")
    addresses = html.xpath("/html/body/div[@class='w1180']/div[@class='maincontent']/div[@id='list-content']/div[@class='li-itemmod']/div[@class='li-info']/address/text()")
    prices = html.xpath("/html/body/div[@class='w1180']/div[@class='maincontent']/div[@id='list-content']/div[@class='li-itemmod']/div[@class='li-side']/p[1]/strong/text()")
    to_last_months = html.xpath("/html/body/div[@class='w1180']/div[@class='maincontent']/div[@id='list-content']/div[@class='li-itemmod']/div[@class='li-side']/p[2]/text()")
    completion_dates = html.xpath("/html/body/div[@class='w1180']/div[@class='maincontent']/div[@id='list-content']/div[@class='li-itemmod']/div[@class='li-info']/p[1]/text()[1]")

```

Python自动登录验证表单

```
    houses = pd.DataFrame(columns = ['title', 'link', 'address', 'price', 'to_last_month', 'completion_date'])
    for i in range(len(titles)):
        # 设置抓取的房源
        temp = {}
        temp['title'] = format_str(titles[i])
        temp['link'] = format_str(links[i])
        temp['address'] = format_str(addresses[i])
        temp['price'] = format_str(prices[i])
        temp['to_last_month'] = format_str(to_last_months[i])
        temp['completion_date'] = format_str(completion_dates[i])
        houses = houses.append(temp,ignore_index=True)
    return houses
    
自动登录开课吧学习中心：
Step1，确定URL，http://student.kaikeba.com/user/login
Step2，找到要填写的表单，分析对应的HTML标签
Step3，模拟操作（点击，填写）
driver.find_element_by_id('username').send_keys('XXXX')
driver.find_element_by_id('password').send_keys('XXXX')
driver.find_element_by_class_name('submit').click()

分类是按照已定的模式和标准进行划分，有监督学习
聚类要靠算法，把相似的数据放在一起，无监督学习
NetworkX的使用（基于python的复杂网络库）
数据采集：
可视化工具，八爪鱼、火车头采集器
Python工具，requests+beautifulsoup, Selenium，Scrapy，Puppeteer

自动化运营以及自动化的价值：
自动发微博
自动加关注，私信，评论
通过规律持续性创造价值，数据采集，自动化运营
技术点：
流程设计，规定每一步做什么
XPath，通过元素的XPath路径获取元素信息
puppet工具，微信、微博Robot

```

# PageRank与图模型

什么是PageRank

```
在PageRank提出之前，搜索引擎面临的问题：
返回结果质量不高
容易被作弊
=> 如何找到优质的网页
匹配用户查找的内容
按照权重排序输出给用户

1998年，Stanford大学博士生Larry Page和Sergey Brin创立了Google，使用PageRank对海量的网页进行重要性分析。
受到论文影响力的启发
图论模型

出链：链接出去的链接。
入链：链接进来的链接。
一个网页u的影响力=所有入链集合的页面的加权影响力之和

转移矩阵：统计网页对于其他网页的跳转概率
原因：出链会给被链接的页面赋予影响力，关键在于统计他们出链的数量

PageRank过程推导：
Step1，假设A、B、C、D的初始影响力相同
Step2，进行第一次转移之后，页面的影响力变为

PageRank过程推导：
Step3，进行n次迭代后，直到页面的影响力不再发生变化，也就是页面的影响力收敛=>最终的影响力

PageRank的代码推导：
import numpy as np
a = np.array([[0, 1/2, 1, 0], 
	      [1/3, 0, 0, 1/2],
	      [1/3, 0, 0, 1/2],
	      [1/3, 1/2, 0, 0]])
b = np.array([1/4, 1/4, 1/4, 1/4])
w = b
for i in range(100):
	w = np.dot(a, w)
	print(w)
	
简化版的模型在实际应用中是否Work？
Rank Leak（等级泄露）
Rank Sink（等级沉没）
不是所有的节点都满足：既有出度，又有入度

Rank Leak（等级泄露）
一个网页没有出链，就像是一个黑洞一样，吸收了别人的影响力而不释放，最终会导致其他网页的PR值为0

```

PageRank简化模型

```
Rank Sink（等级沉没）
一个网页只有出链，没有入链。比如节点A
迭代下来，会导致这个网页的PR值为0

在简化模型上处理Rank Leak和Rank Sink
比如针对Rank Leak，把没有出链的节点，先从图中去掉，等所有节点计算完Rank值之后，再加上该节点进行计算。不过这种方法会导致新的Rank Leak节点的产生。
=> 治标不治本

Larry Page提出了PageRank的随机浏览模型
假设场景：用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意的页面
引入阻尼因子：d
通常取值为0.85（默认）
```

PageRank的随机浏览模型

```
使用随机浏览模型进行Python模拟
def random_work(a, w, n):
	d = 0.85
	for i in range(100):
		w = (1-d)/n + d*np.dot(a, w)
		print(w)
		
```

使用PageRank分析邮件中的人物关系及影响力

```

社交网络领域
如何计算博主影响力（粉丝数=影响力么？）
如何计算职场影响力（脉脉的影响力计算）
生物领域：
基因、蛋白研究，通过PageRank确定七个与遗传有关的肿瘤基因

推荐系统：
用户行为转化为图的形式
对用户u进行推荐，转化为计算用户u和与所有物品i之间的相关性，取与用户没有直接边相连的物品，按照相关性的高低生成推荐列表
交通网络
预测城市的交通流量和人流动向

import networkx as nx
# 创建有向图
G = nx.DiGraph() 
# 有向图之间边的关系
edges = [("A", "B"), ("A", "C"), ("A", "D"), ("B", "A"), ("B", "D"), ("C", "A"), ("D", "B"), ("D", "C")]
for edge in edges:
    G.add_edge(edge[0], edge[1])
pagerank_list = nx.pagerank(G, alpha=1)
print("pagerank值是：", pagerank_list)

```

PageRank相关算法：

```

# 有向图可视化
layout = nx.spring_layout(G)
nx.draw(G, pos=layout, with_labels=True, hold=False)
plt.show()
# 计算简化模型的PR值
pr = nx.pagerank(G, alpha=1)
print("简化模型的PR值：", pr)
# 计算随机模型的PR值
pr = nx.pagerank(G, alpha=0.8)
print("随机模型的PR值：", pr)

数据集：https://github.com/cystanford/PageRank（希拉里邮件丑闻）
包括了9306封邮件和513个人名
Emails.csv：记录了所有公开邮件的内容，发送者和接受者的信息。
Persons.csv：统计了邮件中所有人物的姓名及对应的ID。
Aliases.csv：因为姓名存在别名的情况，为了将邮件中的人物进行统一，我们还需要用Aliases文件来查询别名和人物的对应关系。

# 数据加载
emails = pd.read_csv("./input/Emails.csv")
# 读取别名文件
file = pd.read_csv("./input/Aliases.csv")
aliases = {}
for index, row in file.iterrows():
    aliases[row['Alias']] = row['PersonId']
# 读取人名文件
file = pd.read_csv("./input/Persons.csv")
persons = {}
for index, row in file.iterrows():
    persons[row['Id']] = row['Name']
    
# 针对别名进行转换        
def unify_name(name):
# 画网络图
def show_graph(graph):
# 将寄件人和收件人的姓名进行规范化
emails.MetadataFrom = emails.MetadataFrom.apply(unify_name)
emails.MetadataTo = emails.MetadataTo.apply(unify_name)
# 设置遍的权重等于发邮件的次数
edges_weights_temp = defaultdict(list)


for row in zip(emails.MetadataFrom, emails.MetadataTo, emails.RawText):
    temp = (row[0], row[1])
    if temp not in edges_weights_temp:
        edges_weights_temp[temp] = 1
    else:
        edges_weights_temp[temp] = edges_weights_temp[temp] + 1
# 转化格式 (from, to), weight => from, to, weight
edges_weights = [(key[0], key[1], val) for key, val in edges_weights_temp.items()]

# 创建一个有向图
graph = nx.DiGraph()
# 设置有向图中的路径及权重(from, to, weight)
graph.add_weighted_edges_from(edges_weights)
# 计算每个节点（人）的PR值，并作为节点的pagerank属性
pagerank = nx.pagerank(graph)
# 获取每个节点的pagerank数值
pagerank_list = {node: rank for node, rank in pagerank.items()}
# 将pagerank数值作为节点的属性
nx.set_node_attributes(graph, name = 'pagerank', values=pagerank_list)
# 画网络图
show_graph(graph)

# 将完整的图谱进行精简
# 设置PR值的阈值，筛选大于阈值的重要核心节点
pagerank_threshold = 0.005
# 复制一份计算好的网络图
small_graph = graph.copy()
# 剪掉PR值小于pagerank_threshold的节点
for n, p_rank in graph.nodes(data=True):
    if p_rank['pagerank'] < pagerank_threshold: 
        small_graph.remove_node(n)
# 画网络图
show_graph(small_graph, 'circular_layout')
```

TextRank算法

```
PageRank不仅仅是一个算法，而是一种思想：
TextRank算法
EdgeRank算法
PersonalRank算法

TextRank算法
一种用于文本的基于图的排序算法
根据词之间的共现关系构造网络
构造的网络中的边是无向有权边

Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.

什么是分词和词性
文本：土耳其国防部9日晚宣布，土军队已对叙利亚北部的库尔德武装展开军事行动。
进行分词：seg_list = jieba.cut(sentence, cut_all=False)
获取词性：words = pseg.cut(sentence)

都有哪些词性：
n 名词
    nr 人名
        nr1 汉语姓氏
        nr2 汉语名字
        nrj 日语人名
        nrf 音译人名
    ns 地名
    　nsf 音译地名
    nt 机构团体名
    nz 其它专名
    nl 名词性惯用语
    ng 名词性语素
 
 t 时间词
　　tg 时间词性语素
s 处所词
 
f 方位词
 
v 动词
    vd 副动词
    vn 名动词
    vshi 动词“是”
    vyou 动词“有”
    vf 趋向动词
    vx 形式动词
    vi 不及物动词（内动词）
    vl 动词性惯用语
    vg 动词性语素
    
    a 形容词
    ad 副形词
    an 名形词
    ag 形容词性语素
    al 形容词性惯用语
b 区别词
    bl 区别词性惯用语
z 状态词
r 代词
    rr 人称代词
    rz 指示代词
        rzt 时间指示代词
        rzs 处所指示代词
        rzv 谓词性指示代词
    ry 疑问代词
        ryt 时间疑问代词
        rys 处所疑问代词
        ryv 谓词性疑问代词
    rg 代词性语素

m 数词
    mq 数量词
q 量词
    qv 动量词
    qt 时量词
d 副词
p 介词
    pba 介词“把”
    pbei 介词“被”
c 连词
    cc 并列连词
u 助词
    uzhe 着
    ule 了 喽
    uguo 过
    ude1 的 底
    ude2 地
    ude3 得
    usuo 所
    udeng 等 等等 云云
    uyy 一样 一般 似的 般
    udh 的话
    uls 来讲 来说 而言 说来
 
    uzhi 之
    ulian 连 （“连小学生都会”）
    

e 叹词
y 语气词(delete yg)
o 拟声词
h 前缀
k 后缀
x 字符串
    xx 非语素字
    xu 网址URL
w 标点符号
    wkz 左括号，全角：（ 〔  ［  ｛  《 【  〖 〈   半角：( [ { <
    wky 右括号，全角：） 〕  ］ ｝ 》  】 〗 〉 半角： ) ] { >
    wyz 左引号，全角：“ ‘ 『
    wyy 右引号，全角：” ’ 』
    wj 句号，全角：。
    ww 问号，全角：？ 半角：?
    wt 叹号，全角：！ 半角：!
    wd 逗号，全角：， 半角：,
    wf 分号，全角：； 半角： ;
    wn 顿号，全角：、
    wm 冒号，全角：： 半角： :
    ws 省略号，全角：……  …
    wp 破折号，全角：——   －－   ——－   半角：---  ----
    wb 百分号千分号，全角：％ ‰   半角：%
    wh 单位符号，全角：￥ ＄ ￡  °  ℃  半角：$
    
文本：王者荣耀典韦连招是使用一技能+大招+晕眩+二技能+普攻。这套连招主要用于先手强开团，当发现对面走位失误或撤退不及时，我们就可以利用一技能的加速。
进行分词：seg_list = jieba.cut(sentence, cut_all=False)
获取词性：words = pseg.cut(sentence)

TextRank流程：
Step1，进行分词和词性标注，将单词添加到图中
Step2，出现在一个窗口中的词形成一条边
Step3，基于PageRank原理进行迭代（20-30次）
Step4，顶点（词）按照分数进行排序，可以筛选指定的词性

jieba中使用TextRank提取关键词
jieba.analyse.textrank(string, topK=20, withWeight=True, allowPOS=())
string：待处理语句
topK：输出topK个词，默认20
withWeight：是否返回权重值，默认false
allowPOS：是否仅返回指定类型，默认为空

jieba中使用TF-IDF提取关键词
jieba.analyse.extract_tags(sentence, topK=20, withWeight=True, allowPOS=())
效果好于TextRank，考虑了IDF的情况，而TextRank倾向使用频繁词
效率高于TextRank，TextRank基于图的计算和迭代较慢

textrank4zh工具
TextRank除了可以找到关键词，还可以生成摘要（关键句子）
依赖分词，分词提取结果及TextRank结果与jieba存在差异

from textrank4zh import TextRank4Keyword, TextRank4Sentence
# 输出关键词，设置文本小写，窗口为2
tr4w = TextRank4Keyword()
tr4w.analyze(text=text, lower=True, window=2)
print('关键词：')
for item in tr4w.get_keywords(20, word_min_len=1):
    print(item.word, item.weight)
    
# 输出重要的句子
tr4s = TextRank4Sentence()
tr4s.analyze(text=text, lower=True, source = 'all_filters')
print('摘要：')
# 重要性较高的三个句子
for item in tr4s.get_key_sentences(num=3):
    # index是语句在文本中位置，weight表示权重
    print(item.index, item.weight, item.sentence)
    
TextRank生成摘要的原理：
每个句子作为图中的节点
如果两个句子相似，则节点之间存在一条无向有权边
相似度=同时出现在两个句子中的单词的个数/句子中单词个数求对数之和
（分母使用对数可以降低长句在相似度计算上的优势）

2017年底，微博采用了类似FaceBook的EdgeRank算法。
权重高低按：
粉丝亲密度
内容质量
原创程度等来区分
结果：微博算法调整，100万真实粉丝的账号，阅读量不到10万

```

EdgeRank算法

```
基于图的推荐算法
将用户行为转化为图模型
哪些不与用户u相连的item节点，对用户u的影响力大？

基于图的推荐算法
将用户行为转化为图模型
哪些不与用户u相连的item节点，对用户u的影响力大？

基于图的推荐算法
从用户u对应的节点开始游走
PageRank随机模型改成以(1-d)的概率固定从u重新开始
d的概率继续游走
当收敛的时候，计算item节点影响力排名，即为用户u感兴趣的item

经典的模型往往很简单，但影响力巨大
可以衍生出很多相关/改进模型
PageRank提出了一个经典的基于图论的影响力模型
如何将工程转化为模型
关联性：能否转化为图论，类似Word Embedding中的word和sentence
需求点：影响力排序，item embedding 相似度计算

经典的模型往往很简单，但影响力巨大
可以衍生出很多相关/改进模型
PageRank提出了一个经典的基于图论的影响力模型
如何将工程转化为模型
关联性：能否转化为图论，类似Word Embedding中的word和sentence
需求点：影响力排序，item embedding 相似度计算

如果我们把图看成工具的话，它可以满足我们很多需求
节点影响力：PageRank
强连通图：Kasaraju
社区发现：LPA
最短路径：Dijkstra，Floyd
节点可以是任何事物：words, sentences, images, users


```

PersonalRank算法

```
```
