# 文本信息自动化处理

## 1/2 文本信息自动化处理

Project A：新闻自动化处理

```
自动化处理：
提取新闻中的人物、地点
关键词
关键句 => 新闻摘要
词云可视化

```
正则表达式

```
正则表达式：
对字符串（包括普通字符和特殊字符）操作的一种逻辑公式
好处：可以迅速地用极简单的方式达到字符串的复杂操作
不足：比较晦涩难懂
Python中的re 模块
import re

re.match(pattern, string, flags=0)
尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none
pattern	匹配的正则表达式
string	要匹配的字符串。
flags	标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等

import re
# 在起始位置匹配
print(re.match('www', 'www.baidu.com').span())
print(re.match('com', 'www.baidu.com'))

输出结果：
(0, 3)
None

re.search(pattern, string, flags=0)
扫描整个字符串并返回第一个成功的匹配
pattern	匹配的正则表达式
string	要匹配的字符串。
flags	标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等
search与match区别：
re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，返回None
re.search匹配整个字符串，直到找到一个匹配

import re
#扫描整个字符串并返回第一个成功的匹配
print(re.search('www', 'www.baidu.com').span())
print(re.search('com', 'www.baidu.com').span())

输出结果：
(0, 3)
(10, 13)

re.compile(pattern, flags=0)
将正则表达式模式编译为正则表达式对象
可使用match()，search()方法将其用于匹配
示例：
# 正则对象，匹配两个数字
prog = re.compile('\d{2}') 
# 下面两个作用是相同的
print(prog.search('12abc'))
print(re.search('\d{2}', '12abc'))
# 通过调用group()方法得到匹配的字符串,如果字符串没有匹配，则返回None
print(prog.search('12abc').group())

输出结果：
<re.Match object; span=(0, 2), match='12'>
12
<re.Match object; span=(0, 2), match='12'>

re.sub(pattern, repl, string, count=0, flags=0)
用于替换字符串中的匹配项
pattern : 正则中的模式字符串。
repl : 替换的字符串，也可为一个函数。
string : 要被查找替换的原始字符串。
count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。
re.findall(pattern, string, flags=0)
返回string中所有与pattern相匹配的全部字串，返回形式为数组

phone = "001-609-7267 # ETS的TOEFL查询电话" 
# 删除字符串中的注释 
num = re.sub(r'#.*$', "", phone)
print("电话号码是: ", num)
# 删除非数字(-)的字符串 
num = re.sub(r'\D', "", phone)
print("电话号码是 : ", num)

输出结果：
电话号码是:  001-609-7267 
电话号码是 :  0016097267

 元字符	 匹配内容
. 	匹配除换行符以外的任意字符
\w	匹配字母或数字或下划线
\s	匹配任意的空白符
\d	匹配数字
\n	匹配一个换行符
\t	匹配一个制表符
\b	匹配一个单词的结尾
^	匹配字符串的开始

 元字符	 匹配内容
$	匹配字符串的结尾
\W	匹配非字母或数字或下划线
\D	匹配非数字
\S	匹配非空白符
a|b	匹配字符a或字符b
()	匹配括号内的表达式，也表示一个组
[...]	匹配字符组中的字符
[^...]	匹配除了字符组中字符的所有字符

量词	用法说明
*	重复零次或更多次
+	重复一次或更多次
?	重复零次或一次
{n}	重复n次
{n,}	重复n次或更多次
{n,m}	重复n到m次

符号	匹配内容
[\u4e00-\u9fa5]	中文字符
[^\x00-\xff]	双字节字符(包括汉字在内)

电话号码验证
规则11位，首位为1，第二位不为2，4，9
比如158 1234 5678

def is_phone(phone):
	# 电话号码正则对象
	prog = re.compile('^1[35678]\d{9}$') 
	result = prog.match(phone)
	if result:
		return True
	else:
		return False
print(is_phone('15801234567'))

邮箱验证
@之前必须有内容且只能是字母、数字、下划线(_)、减号（-）、点（.）
@和最后一个点（.）之间必须有内容，可以是字母、数字、点（.）、减号（-），不能使下划线(_)，且两个点不能挨着
最后一个点（.）之后必须有内容，而且内容只能是字母（大小写）、数字且长度为大于等于2个字节，小于等于4个字节
比如 1580123@qq.com => True
1580123@qq => False

def is_email(email):
	# 邮箱正则对象
	prog = re.compile('^[a-zA-Z0-9_.-]+@[a-zA-Z0-9-]+(\.[a-zA-Z0-9-]+)*\.[a-zA-Z0-9]{2,4}$') 
	result = prog.match(email)
	if result:
		return True
	else:
		return False
print(is_email("1580123@qq.com"))  #True
print(is_email("1580123@qq"))   #False

数据集：https://github.com/cystanford/PageRank（希拉里邮件丑闻）
包括了9306封邮件和513个人名
Emails.csv：记录了所有公开邮件的内容，发送者和接受者的信息。
Persons.csv：统计了邮件中所有人物的姓名及对应的ID。
Aliases.csv：因为姓名存在别名的情况，为了将邮件中的人物进行统一，我们还需要用Aliases文件来查询别名和人物的对应关系。
ToDo：提取往来邮件中的内容，即针对Emails.csv中的ExtractedBodyText字段进行清洗（使用正则表达式）

分析ExtractedBodyText字段：
存在换行 \n => 删除
text = text.replace('\n'," ")
去掉日期
text = re.sub(r"\d+/\d+/\d+", "", text) 
去掉时间
text = re.sub(r"[0-2]?[0-9]:[0-6][0-9]", "", text) 
去掉邮件地址
text = re.sub(r"[\w]+@[\.\w]+", "", text) 
去掉网址
text = re.sub(r"/[a-zA-Z]*[:\//\]*[A-Za-z0-9\-_]+\.+[A-Za-z0-9\.\/%&=\?\-_]+/i", "", text) 
把 "-" 的两个单词，分开
text = re.sub(r"-", " ", text)

# 对邮件正文进行清洗
def clean_email_text(text):
    # 换行，可以删除
    text = text.replace('\n'," ") 
    # 把 "-" 的两个单词，分开（比如：meet-the-right ==> meet the right）
    text = re.sub(r"-", " ", text) 
    # 日期，对主题模型没什么意义
    text = re.sub(r"\d+/\d+/\d+", "", text) 
    # 时间，没意义
    text = re.sub(r"[0-2]?[0-9]:[0-6][0-9]", "", text) 
    # 邮件地址，没意义
    text = re.sub(r"[\w]+@[\.\w]+", "", text) 
    # 网址，没意义
    text = re.sub(r"/[a-zA-Z]*[:\//\]*[A-Za-z0-9\-_]+\.+[A-Za-z0-9\.\/%&=\?\-_]+/i", "", text) 
    pure_text = ''
    # 检查每个字母，去掉其他特殊字符等
    for letter in text:
        # 只留下字母和空格
        if letter.isalpha() or letter==' ':
            pure_text += letter
    # 最终得到的都是有意义的单词
    text = ' '.join(word for word in pure_text.split() if len(word)>1)
    return text
    
df = pd.read_csv("./Emails.csv") # 数据加载
# 原邮件数据中有很多Nan的值，直接扔了。
df = df[['Id', 'ExtractedBodyText']].dropna()
docs = df['ExtractedBodyText']
docs = docs.apply(lambda s: clean_email_text(s))
doclist = docs.values # 转化为列表List
print(doclist)
print(len(doclist))
```

基于Pandas的文本正则化
中文切词，地点实体识别

```
分词：
NLTK库，对英文文本预处理
jieba库，对中文文本预处理
停用词（Stop Words）：
需要自动过滤掉某些字或词
语料（Corpus）：
一组原始文本的集合，用于无监督地训练文本

import jieba

text = '张飞和关羽'
words = jieba.cut(text)
words = list(words)
print(words)

输出结果：
['张飞', '和', '关羽']

都有哪些词性：
n 名词
    nr 人名
        nr1 汉语姓氏
        nr2 汉语名字
        nrj 日语人名
        nrf 音译人名
    ns 地名
    　nsf 音译地名
    nt 机构团体名
    nz 其它专名
    nl 名词性惯用语
    ng 名词性语素
 
 t 时间词
　　tg 时间词性语素
s 处所词
 
f 方位词
 
v 动词
    vd 副动词
    vn 名动词
    vshi 动词“是”
    vyou 动词“有”
    vf 趋向动词
    vx 形式动词
    vi 不及物动词（内动词）
    vl 动词性惯用语
    vg 动词性语素
    
a 形容词
    ad 副形词
    an 名形词
    ag 形容词性语素
    al 形容词性惯用语
b 区别词
    bl 区别词性惯用语
z 状态词
r 代词
    rr 人称代词
    rz 指示代词
        rzt 时间指示代词
        rzs 处所指示代词
        rzv 谓词性指示代词
    ry 疑问代词
        ryt 时间疑问代词
        rys 处所疑问代词
        ryv 谓词性疑问代词
    rg 代词性语素

m 数词
    mq 数量词
q 量词
    qv 动量词
    qt 时量词
d 副词
p 介词
    pba 介词“把”
    pbei 介词“被”
c 连词
    cc 并列连词
u 助词
    uzhe 着
    ule 了 喽
    uguo 过
    ude1 的 底
    ude2 地
    ude3 得
    usuo 所
    udeng 等 等等 云云
    uyy 一样 一般 似的 般
    udh 的话
    uls 来讲 来说 而言 说来
 
    uzhi 之
    ulian 连 （“连小学生都会”）
    

e 叹词
y 语气词(delete yg)
o 拟声词
h 前缀
k 后缀
x 字符串
    xx 非语素字
    xu 网址URL
w 标点符号
    wkz 左括号，全角：（ 〔  ［  ｛  《 【  〖 〈   半角：( [ { <
    wky 右括号，全角：） 〕  ］ ｝ 》  】 〗 〉 半角： ) ] { >
    wyz 左引号，全角：“ ‘ 『
    wyy 右引号，全角：” ’ 』
    wj 句号，全角：。
    ww 问号，全角：？ 半角：?
    wt 叹号，全角：！ 半角：!
    wd 逗号，全角：， 半角：,
    wf 分号，全角：； 半角： ;
    wn 顿号，全角：、
    wm 冒号，全角：： 半角： :
    ws 省略号，全角：……  …
    wp 破折号，全角：——   －－   ——－   半角：---  ----
    wb 百分号千分号，全角：％ ‰   半角：%
    wh 单位符号，全角：￥ ＄ ￡  °  ℃  半角：$
    
jieba分词的三种模式：
精确模式：把文本精确的切分开，不存在冗余单词
全模式：把文本中所有可能的词语都扫描出来，有冗余
搜索引擎模式：在精确模式基础上，对长词再次切分


美国新冠肺炎确诊超57万，纽约州死亡人数过万。当地时间4月13日，世卫组织发布最新一期新冠肺炎每日疫情报告。截至欧洲中部时间4月13日10时（北京时间4月13日16时），全球确诊新冠肺炎1773084例（新增76498例），死亡111652例（新增5702例）。其中，疫情最为严重的欧洲区域已确诊913349例（新增33243例），死亡77419例（新增3183例）。


对句子进行分词
1）分词
2）获取每个单词的词性


add_word(word, freq=None, tag=None)
添加word，后续不被切分
add_word只是一次性的添加分词字典，不是直接将内容添加到结巴库中了
del_word(word)
删除word

Thinking：如果只用词频来做词语重要性统计，会是怎样？
s1: 我非常喜欢看电视剧
s2: 我非常喜欢旅行
s3: 我非常喜欢吃苹果
s4: 我非常喜欢跑步
s5: 王者荣耀KPL春季赛开战啦

关键词提取Model：
TF-IDF
TextRank

什么是TF-IDF：
TF：Term Frequency，词频
一个单词的重要性和它在文档中出现的次数呈正比。
IDF：Inverse Document Frequency，逆向文档频率

Thinking：使用TF-IDF，单词的TF-IDF会是多少？
s1: 我非常喜欢看电视剧
s2: 我非常喜欢旅行
s3: 我非常喜欢吃苹果
s4: 我非常喜欢跑步
s5: 王者荣耀KPL春季赛开战啦

test: 我喜欢看电视剧
和哪个句子相似？

TF-IDF作用：
获得文档向量和提取文档关键词
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer
CountVectorizer，只考虑词汇在文本中出现的频率
TfidfTransformer，统计CountVectorizer中每个词语的tf-idf权值。除了考量某词汇在文本出现的频率，还关注包含这个词汇的所有文本的数量
TfidfVectorizer，把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值

常用函数：
get_feature_names(), 得到所有文本的关键字
fit_transform(), 训练并且转换

Gensim工具
一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达
corpora
gensim中的一个基本概念，是文档集的表现形式。
corpora本质上是个二维矩阵，但在实际运行中，因为单词数量极多（上万甚至更多），而一篇文档的单词数是有限的，所以gensim内部是用稀疏矩阵的形式来表示的 

TF-IDF生成步骤
# Step1, 生成字典
dictionary = corpora.Dictionary(train)
# Step2, 生成语料 
corpus = [dictionary.doc2bow(text) for text in train]
# Step3, 定义TFIDF模型 
tfidf_model = models.TfidfModel(corpus, dictionary=dictionary)
# Step4, 生成TFIDF矩阵 
corpus_tfidf = tfidf_model[corpus]

```

词云构建与可视化
TFIDF与TextRank

```
Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.

TextRank流程：
Step1，进行分词和词性标注，将单词添加到图中
Step2，出现在一个窗口中的词形成一条边
Step3，基于PageRank原理进行迭代（20-30次）
Step4，顶点（词）按照分数进行排序，可以筛选指定的词性

jieba中使用TextRank提取关键词
jieba.analyse.textrank(string, topK=20, withWeight=True, allowPOS=())
string：待处理语句
topK：输出topK个词，默认20
withWeight：是否返回权重值，默认false
allowPOS：是否仅返回指定类型，默认为空

jieba中使用TF-IDF提取关键词
jieba.analyse.extract_tags(sentence, topK=20, withWeight=True, allowPOS=())
效果好于TextRank，考虑了IDF的情况，而TextRank倾向使用频繁词
效率高于TextRank，TextRank基于图的计算和迭代较慢

textrank4zh工具
TextRank除了可以找到关键词，还可以生成摘要（关键句子）
依赖分词，分词提取结果及TextRank结果与jieba存在差异

from textrank4zh import TextRank4Keyword, TextRank4Sentence
# 输出关键词，设置文本小写，窗口为2
tr4w = TextRank4Keyword()
tr4w.analyze(text=text, lower=True, window=2)
print('关键词：')
for item in tr4w.get_keywords(20, word_min_len=1):
    print(item.word, item.weight)
    
# 输出重要的句子
tr4s = TextRank4Sentence()
tr4s.analyze(text=text, lower=True, source = 'all_filters')
print('摘要：')
# 重要性较高的三个句子
for item in tr4s.get_key_sentences(num=3):
    # index是语句在文本中位置，weight表示权重
    print(item.index, item.weight, item.sentence)
    
TextRank生成摘要的原理：
每个句子作为图中的节点
如果两个句子相似，则节点之间存在一条无向有权边
相似度=同时出现在两个句子中的单词的个数/句子中单词个数求对数之和
（分母使用对数可以降低长句在相似度计算上的优势）

def create_word_cloud(f):
	f = remove_stop_words(f)
	cut_text = word_tokenize(f)
	cut_text = " ".join(cut_text)
	wc = WordCloud(
		max_words=100,
		width=2000,
		height=1200,
    )
	wordcloud = wc.generate(cut_text)
	wordcloud.to_file("wordcloud.jpg")
```


# 主题模型
什么是主题模型

```
词袋模型（Bag-of-words ）
NLP中的一个基本假设。在这个模型中，一个文档(document)被表示为一组单词(word)的无序组合，而忽略了语法或者词序的部分
Thinking：在词袋模型中 下面两个句子是否等价？
s1: 我喜欢你 
s2: 你喜欢我

主题（Topic）
在单词和文档之间还有一层隐含的关系
主题模型（Topic Model）
通过无监督的学习方法发现文本中隐含的主题信息“Topic”


```

pLSA模型

LDA模型

```
LDA模型：
隐含狄利克雷分布（Latent Dirichlet Allocation，简称LDA）
在主题模型中具有重要的地位，常用来文本分类
2003年提出，用于推测文档的主题分布。它可以将文档集中每篇文档的主题以概率分布的形式给出，然后通过分析一些文档抽取出它们的主题分布后，就可以根据主题分布进行文本分类
LDA 采用词袋模型。词袋模型就是对于一篇文档，我们仅考虑一个词汇是否出现，而不考虑它出现的顺序
目的就是要识别主题，即把文档—词汇矩阵变成文档—主题矩阵（分布）和主题—词汇矩阵（分布）

文本建模：
一篇文档，可以看成是一组有序的词的序列

一篇文档包含多个主题，每一个词都由其中的一个主题生成
从统计学角度来看，文档的生成可以看成是上帝抛掷骰子生成的结果，每次抛掷骰子都生成一个词汇，抛掷N词生成一篇文档。于是我们关心两个问题：
上帝都有什么样的骰子 => 选哪个主题
上帝是如何抛掷这些骰子的 => 选哪个词

文本建模：
Step1，假设事先给定了这几个主题：Arts、Budgets、Children、Education，然后通过学习训练，获取每个主题Topic对应的词语
Step2，以一定的概率选取上述某个主题，再以一定的概率选取该主题下的某个单词，不断的重复这两步，最终生成一篇文章

pLSA生成文档的过程：
两种类型的骰子，一种是doc-topic骰子，另一个是另一种是topic-word骰子。
每个doc-topic骰子有K个面，每个面一个topic的编号
每个topic-word骰子有V个面，每个面对应一个词
这样我们会有K个topic-word骰子，编号为1到K
生成每篇文档之前，先为这篇文章制造一个特定的doc-topic骰子，重复如下过程生成文档中的词：
投掷这个doc-topic骰子，得到一个topic编号z
选择K个topic-word骰子中编号为z的那个，投掷这个骰子，得到一个词

pLSA生成文档的过程：
Step1，按照概率            选择一篇文档
Step2，根据选择的文档      ，从主题分布中按照概率                选择一个隐含的主题类别
Step3，根据选择的主题       , 从词分布中按照概率                 选择一个词

LDA 在 PLSA 的基础上，：
为主题分布和词分布分别加了两个 Dirichlet 先验
根据给定的一篇文档，反推其主题分布
使用LDA推测分析网络上各篇文章分别都写了什么主题，以及每篇文章中各个主题出现的概率分布
LDA的文档生成方式：
按照先验概率             选择一篇文档
从狄利克雷分布     中取样生成文档      的主题分布
从主题的多项式分布    中取样生成文档       第n个词的主题
从狄利克雷分布     中取样生成主题 k 对应的词语分布
从词语的多项式分布         中采样最终生成词语

pLSA文档生成：
以固定的概率来抽取一个主题词，比如0.5的概率抽取教育，然后根据抽取出来的主题词，找其对应的词分布，再根据词分布，抽取一个词汇
在pLSA中，主题分布和词分布都是唯一确定的
LDA文档生成：
在LDA中，主题分布和词分布是不确定的，采用贝叶斯思想，认为它们应该服从一个分布（主题分布和词分布都是多项式分布）
LDA 就是 pLSA 的贝叶斯化版本
Step1, 现有一个装有无穷多个骰子的坛子，里面装有各式各样的骰子，每个骰子有V个面
Step2, 现从坛子中抽取一个骰子出来，然后使用这个骰子进行抛掷，直到产生语料库中的所有词汇

LDA模型：
潜在狄利克雷分布，pLSA 的贝叶斯版本
使用狄利克雷先验来处理“文档-主题”和“主题-单词”分布，这样可以更好的泛化。
可以将狄利克雷看为 分布的分布
它解决了：给定某种分布，我看到的实际概率分布可能是什么样子？
假设语料库有 3 个完全不同主题领域的文档。我们希望：对于一个文档，在其中一个主题上有着极高的权重，而在其他的主题上权重不大。
具体概率分布可能会是：
混合 X：90% 主题 A，5% 主题 B，5% 主题 C
混合 Y：5% 主题 A，90% 主题 B，5% 主题 C
混合 Z：5% 主题 A，5% 主题 B，90% 主题 C

models.ldamodel.LdaModel(corpus=None, num_topics=100, id2word=None)
corpus：传入的文档语料
num_topics：需要提取的潜在主题数
id2word：用于设置构建模型的词典，决定了词汇数量
inference(corpus)
主题推断

get_document_topics(bow, minimum_probability=None)
给定一个文档，得到它的主题概率分布，按照概率的大小按降序排列。
minimum_probability：忽略概率小于该参数值的主题
get_term_topics(word_id, minimum_probability=None)
给定一个指定的word，得到与这个word最相关的topics
minimum_probability 为返回主题的最小概率限定
get_topic_terms(topicid, topn=10)
给定一个指定的topic，得到与这个topic最相关的words,按照word的概率大小排列
get_topics()
得到的是一个矩阵，矩阵里的元素是每一个topic下每一个word的概率，shape(num_topics,vacabulary)

西雅图酒店数据集：
下载地址：https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Seattle_Hotels.csv
字段：name, address, desc
基于用户选择的酒店，推荐相似度高的Top10个其他酒店
方法：计算当前酒店特征向量与整个酒店特征矩阵的余弦相似度，取相似度最大的Top-k个

余弦相似度：
通过测量两个向量的夹角的余弦值来度量它们之间的相似性。
判断两个向量⼤致方向是否相同，方向相同时，余弦相似度为1；两个向量夹角为90°时，余弦相似度的值为0，方向完全相反时，余弦相似度的值为-1。
两个向量之间夹角的余弦值为[-1, 1]
给定属性向量A和B，A和B之间的夹角θ余弦值可以通过点积和向量长度计算得出

计算A和B的余弦相似度：
句子A：这个程序代码太乱，那个代码规范
句子B：这个程序代码不规范，那个更规范
Step1，分词
句子A：这个/程序/代码/太乱，那个/代码/规范
句子B：这个/程序/代码/不/规范，那个/更/规范

计算A和B的余弦相似度：
Step4，计算词频向量的余弦相似度
句子A：（1，1，2，1，1，1，0，0）
句子B：（1，1，1，0，1，

什么是N-Gram（N元语法）：
基于一个假设：第n个词出现与前n-1个词相关，而与其他任何词不相关.
N=1时为unigram，N=2为bigram，N=3为trigram
N-Gram指的是给定一段文本，其中的N个item的序列
比如文本：A B C D E，对应的Bi-Gram为A B, B C, C D, D E
当一阶特征不够用时，可以用N-Gram做为新的特征。比如在处理文本特征时，一个关键词是一个特征，但有些情况不够用，需要提取更多的特征，采用N-Gram => 可以理解是相邻两个关键词的特征组合

plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
df = pd.read_csv('Seattle_Hotels.csv', encoding="latin-1")
# 得到酒店描述中n-gram特征中的TopK个
def get_top_n_words(corpus, n=1, k=None):
    # 统计ngram词频矩阵
    vec = CountVectorizer(ngram_range=(n, n), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    # 按照词频从大到小排序
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:k]
common_words = get_top_n_words(df['desc'], 1, 20)
df1 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
df1.groupby('desc').sum()['count'].sort_values().plot(kind='barh', title='去掉停用词后，酒店描述中的Top20单词')
plt.show()

# Bi-Gram
common_words = get_top_n_words(df['desc'], 2, 20)


# Tri-Gram
common_words = get_top_n_words(df['desc'], 3, 20)


def clean_text(text):
    # 全部小写
    text = text.lower()
    ……
    return text
df['desc_clean'] = df['desc'].apply(clean_text)
# 使用TF-IDF提取文本特征
tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')
tfidf_matrix = tf.fit_transform(df['desc_clean'])
print(tfidf_matrix)
print(tfidf_matrix.shape)
# 计算酒店之间的余弦相似度（线性核函数）
cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)
print(cosine_similarities)


# 基于相似度矩阵和指定的酒店name，推荐TOP10酒店
def recommendations(name, cosine_similarities = cosine_similarities):
    recommended_hotels = []
    # 找到想要查询酒店名称的idx
    idx = indices[indices == name].index[0]
    print('idx=', idx)
    # 对于idx酒店的余弦相似度向量按照从大到小进行排序
    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending = False)
    # 取相似度最大的前10个（除了自己以外）
    top_10_indexes = list(score_series.iloc[1:11].index)
    # 放到推荐列表中
    for i in top_10_indexes:
        recommended_hotels.append(list(df.index)[i])
    return recommended_hotels
print(recommendations('Hilton Seattle Airport & Conference Center'))
print(recommendations('The Bacon Mansion Bed and Breakfast'))

CountVectorizer：
将文本中的词语转换为词频矩阵
fit_transform：计算各个词语出现的次数
get_feature_names：可获得所有文本的关键词
toarray()：查看词频矩阵的结果



    vec = CountVectorizer(ngram_range=(n, n), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    print('feature names:')
    print(vec.get_feature_names())
    print('bag of words:')
    print(bag_of_words.toarray())
    
TF-IDF：
TF：Term Frequency，词频
一个单词的重要性和它在文档中出现的次数呈正比。

TfidfVectorizer:
将文档集合转化为tf-idf特征值的矩阵
构造函数
analyzer：word或者char，即定义特征为词（word）或n-gram字符
ngram_range: 参数为二元组(min_n, max_n)，即要提取的n-gram的下限和上限范围
max_df：最大词频，数值为小数[0.0, 1.0],或者是整数，默认为1.0
min_df：最小词频，数值为小数[0.0, 1.0],或者是整数，默认为1.0
stop_words：停用词，数据类型为列表
功能函数：
fit_transform：进行tf-idf训练，学习到一个字典，并返回Document-term的矩阵，也就是词典中的词在该文档中出现的频次



# 使用TF-IDF提取文本特征
tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')
tfidf_matrix = tf.fit_transform(df['desc_clean'])
print(tfidf_matrix)
print(tfidf_matrix.shape)

基于内容的推荐：
Step1，对酒店描述（Desc）进行特征提取
N-Gram，提取N个连续字的集合，作为特征
TF-IDF，按照(min_df, max_df)提取关键词，并生成TFIDF矩阵
Step2，计算酒店之间的相似度矩阵
余弦相似度
Step3，对于指定的酒店，选择相似度最大的Top-K个酒店进行输出

LDA 概率主题模型
用来推测文档的主题分布，根据主题分布进行主题聚类或文本分类
基于内容的推荐：
将你看的item，相似的item推荐给你
通过物品表示Item Representation => 抽取特征
TF-IDF => 返回给某个文本的“关键词-TFIDF值”的词数对
TF-IDF可以帮我们抽取文本的重要特征，做成item embedding
计算item之间的相似度矩阵
对于指定的item，选择相似度最大的Top-K个进行输出

```

Project A：为酒店建立内容推荐系统
N-Gram
余弦相似度计算
