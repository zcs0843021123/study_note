# Python 格式化数据处理 - Pandas

## Python 读写

* Python 文件读写

```
# 打开文件
f = open('test.txt', 'r')

# mode
# A string, define which mode you want to open the file in:

"r" - Read - Default value. Opens a file for reading, error if the file does not exist
"a" - Append - Opens a file for appending, creates the file if it does not exist
"w" - Write - Opens a file for writing, creates the file if it does not exist
"x" - Create - Creates the specified file, returns an error if the file exist

# In addition you can specify if the file should be handled as binary or text mode
"t" - Text - Default value. Text mode
"b" - Binary - Binary mode (e.g. images)

# 读文件
python 文件对象提供了三个"读"方法： 
f.read() # 每次读取整个文件
f.readlines() # 自动将文件内容分析成一个行的列表
f.readline() # 每次只读取一行，比 readlines() 慢


# 写文件
f.write('Hello, world!')

# python 文件对象提供了两个写方法： 
# f.write()  与 f.writelines() 
# writelines() 和 readlines() 方法一样，是针对列表的操作。接收一个字符串列表作为参数，换行符不会自动的加入，需要显式写换行符 \n

# 关闭文件
f.close() #使用完毕后进行关闭，避免系统资源占用
```

* Pandas CSV，Excel 文件读写

> 数据常用 CSV 和 Excel 文件存储
> 
> 后缀 .csv .xls .xlsx
> 
> 如果有中文时，需要加 encoding 相关参数

```
# pandas 读取 imagenet classID 表示

import pandas as pd
infos = pd.read_csv('imagenet_class.csv', sep=',', header=None) # sep 默认逗号，header 是表格的表头

print(infos)
# 查看 id=207 的含义
print(infos[infos[0]==207])    # 这是个 Filter


# 从 csv, excel 中导入和导出
# pd.read_excel
# pd.read_csv

# 读取 xlsx 文件
score = DataFrame(pd.read_excel('heros.xlsx'))
score.to_excel('result.xlsx')  # 写入
print(score)

# 读取 csv 文件
score = pd.read_csv('heros.csv')
score.to_csv('result.csv')  # 写入
```

* 面向对象方法及有序字典使用

```
# 面向对象方法：
# 定义类 class
# 作用：属性引用和实例化
# 实例化：类名加括号就是实例化，会自动触发 __init__ 函数的运行，可以用它来为每个实例定制自己的特征

from collections import OrderedDict
class LRUCache(OrderedDict):

    def __init__(self, capacity):
        self.capacity = capacity

    def get(self, key):
        if key not in self:
            return - 1
        # 将 key 放到最后
        self.move_to_end(key)
        return self[key]

    def put(self, key, value):
	    if key in self:
	        self.move_to_end(key)
	    self[key] = value
	    if len(self) > self.capacity:
	        # 弹出第一个
	        self.popitem(last = False)
	        

obj = LRUCache(5)
obj.put('a',1)
obj.put('b',2)
obj.put('c',3)
print(obj.get('a'))

```

* Python 时间与日期函数

```
# Python时间处理：
# time，提供的功能是更加接近于操作系统层面
# datetime，基于 time 进行了高级封装，提供了更多实用的函数

import time
timestamp = time.time()
print("当前时间戳为:", timestamp)
localtime = time.localtime(timestamp)
print("本地时间为 :", localtime)


# 当前时间戳为: 1584625344.960277
# 本地时间为 : time.struct_time(tm_year=2020, tm_mon=3, tm_mday=19, tm_hour=21, tm_min=42, tm_sec=24, # tm_wday=3, tm_yday=79, tm_isdst=0)

# 时间元组，用一个元组装起来的9组数字处理时间:
tm_year	年
tm_mon	月
tm_mday	日期
tm_hour	小时
tm_min	分钟
tm_sec	秒
tm_wday	星期几
tm_yday	一年中第几天
tm_isdst	是否为夏令时

#
import datetime
date = datetime.date(2020, 3, 1)
print(date)

time_now = datetime.datetime.now()
delta1 = datetime.timedelta(days=30)
print(time_now)
print(time_now + delta1)

#
2020-03-01
2020-03-19 21:53:06.137915
2020-04-18 21:53:06.137915

```

* map()，lambda 函数，列表解析

```

# map()使用
# map(function, iterable, ...)
# Python 内置的函数，它接收一个函数 f 和一个 list，把函数 f 依次作用在 list 的每个元素上，得到一个新的 list 并返回
# 计算列表各个元素的平方
def square(x):
	return x * x
print(list(map(square, [1,2,3,4,5])))

#
[1, 4, 9, 16, 25]

#
# lambda使用
# lambda 参数:操作(参数)
# 也称为匿名函数。
# 如果你不想在程序中对一个函数使用两次，可以用 lambda 表达式，
# 作用和普通函数一样 => 一个只用一行就能解决问题的函数

add = lambda x, y: x + y
print(add(5, 6))

# 按照 x[1] 进行列表排序
a = [(2, 56), (3, 12), (6, 10), (9, 13)]
a.sort(key=lambda x: x[1])
print(a)

```
* Project A: 基于 Syntax Tree 的对话机器人

```
四则运算表达式：
expr = a+5*b
如果当a=3，b=6，expr = 33

# 每个函数，都有env参数
Num = lambda env, n: n 
Var = lambda env, x: env[x] 
Add = lambda env, a, b:_eval(env, a) + _eval(env, b) 
Mul = lambda env, a, b:_eval(env, a) * _eval(env, b) 

# 对表达式进行处理，expr[0]为符号，*expr[1:]为传入的参数
_eval = lambda env, expr:expr[0](env, *expr[1:])   

env = {'a':3, 'b':6} 
tree = (
	Add, (Var, 'a'), 
	(Mul, (Num, 5), (Var, 'b'))
) 

print(_eval(env, (Var, 'a')))
print(_eval(env, (Num, 5)))
print(Num(env, 5))
print(_eval(env, tree))


# 定语从句语法
grammar = '''
战斗 => 施法  ， 结果 。
施法 => 主语 动作 技能 
结果 => 主语 获得 效果
主语 => 张飞 | 关羽 | 赵云 | 典韦 | 许褚 | 刘备 | 黄忠 | 曹操 | 鲁班七号 | 貂蝉
动作 => 施放 | 使用 | 召唤 
技能 => 一骑当千 | 单刀赴会 | 青龙偃月 | 刀锋铁骑 | 黑暗潜能 | 画地为牢 | 守护机关 | 狂兽血性 | 龙鸣 | 惊雷之龙 | 破云之龙 | 天翔之龙
获得 => 损失 | 获得 
效果 => 数值 状态
数值 => 1 | 1000 | 5000 | 100 
状态 => 法力 | 生命
'''

# 得到语法字典
def getGrammarDict(gram, linesplit = "\n", gramsplit = "=>"):
    #定义字典
    result = {}
    for line in gram.split(linesplit):
        # 去掉首尾空格后，如果为空则退出
        if not line.strip(): 
            continue
        expr, statement = line.split(gramsplit)
        result[expr.strip()] = [i.split() for i in statement.split("|")]
    return result
    
# 生成句子
def generate(gramdict, target, isEng = False):
    if target not in gramdict: 
        return target
    find = random.choice(gramdict[target])
    blank = ''
    # 如果是英文中间间隔为空格
    if isEng: 
        blank = ' '
    return blank.join(generate(gramdict, t, isEng) for t in find)
    
gramdict = getGrammarDict(grammar)
print(generate(gramdict,"战斗"))
print(generate(gramdict,"战斗", True))

#
貂蝉施放龙鸣，黄忠损失100法力。
典韦 使用 黑暗潜能 ， 黄忠 损失 1 生命 。
```


## Pandas 使用
* 什么是格式化数据

```
在数据分析中，Pandas 的使用频率很高
Pandas 可以说是基于 NumPy 构建的含有更高级数据结构和分析能力的工具包
Series 和 DataFrame 是两个核心数据结构，分别代表着一维的序列和二维的表结构
基于这两种数据结构，Pandas可以对数据进行导入、清洗、处理、统计和输出
```

* Series，DataFrame 使用

```
Series是个定长的字典序列
在存储的时候，相当于两个 ndarray，这也是和字典结构最大的不同。
因为字典结构，元素个数是不固定的

Series有两个基本属性：index 和 values

from pandas import Series, DataFrame

x1 = Series([1,2,3,4])
x2 = Series(data=[1,2,3,4], index=['a', 'b', 'c', 'd'])

# 使用字典来进行创建
d = {'a':1, 'b':2, 'c':3, 'd':4}
x3 = Series(d)
print(x1)
print(x2)
print(x3)

DataFrame使用
类似数据库表，包括了行索引和列索引，可以将 DataFrame 看成是由相同索引的 Series 组成的字典类型

from pandas import Series, DataFrame
data = {'Chinese': [66, 95, 93, 90,80], 'Math': [30, 98, 96, 77, 90], 'English': [65, 85, 92, 88, 90]}
df1 = DataFrame(data)
df2 = DataFrame(data, 
	index=['ZhangFei', 'GuanYu', 'LiuBei', 'DianWei', 'XuChu'], 
	columns=['Chinese', 'Math', 'English'])
print(df1)
print(df2)


删除 DataFrame 中的不必要的列或行
df2 = df2.drop(columns=['Chinese']) # 删除列
df2 = df2.drop(index=['ZhangFei']) # 删除行
重命名列名columns，让列表名更容易识别
df2.rename(columns={'Chinese': '语文', 'English': 'Yingyu'}, inplace = True)

去掉重复的值
df = df.drop_duplicates()
更改数据格式
df2['Chinese'].astype('str') 
df2['Chinese'].astype(np.int64) 

去掉数据间的空格
# 删除左右两边空格
df2['Chinese'] = df2['Chinese'].map(str.strip)

大小写转换
# 全部大写
df2.columns = df2.columns.str.upper()
# 全部小写
df2.columns = df2.columns.str.lower()
# 首字母大写
df2.columns = df2.columns.str.title()


使用apply函数进行数据清洗
apply 函数是 Pandas 中自由度非常高的函数，使用频率高
比如对 name 列的数值都进行大写转化

df['name'] = df['name'].apply(str.upper)
也可以定义个函数，在apply中进行使用
def double_df(x):
           return 2*x
df1[u'语文'] = df1[u'语文'].apply(double_df)

Thinking: apply和map的区别是什么？
apply 用在 dataframe上，用于对 row 或者 column 进行计算
applymap 用于 dataframe 上，是元素级别的操作
map ，是 python 自带的，用于 series 上，是元素级别的操作

Pandas中的统计函数
count()	统计个数，空值NaN不计算
describe() 一次性输出多个统计指标，包括：count, mean, std, min, max等
min()最小值
max()最大值
sum()总和
mean()平均值
median()中位数
var()方差
std()标准差
argmin()	 # 统计最小值的索引位置
argmax()  # 统计最大值的索引位置
idxmin()	 # 统计最小值的索引值
idxmax()	  # 统计最大值的索引值

Pandas中的统计函数
print(df2.describe())

数据表合并
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(1,6)})
df2 = DataFrame({'name':['ZhangFei', 'GuanYu', 'A', 'B', 'C'], 'data2':range(1,6)})
1）基于name这列进行连接
df3 = pd.merge(df1, df2, on='name')

inner内连接
df3 = pd.merge(df1, df2, how='inner')

left左连接
df3 = pd.merge(df1, df2, how='left'

right右连接
df3 = pd.merge(df1, df2, how='right')


outer外连接
df3 = pd.merge(df1, df2, how='outer')

```

```
pip install xlrd
pip国内的镜像：
阿里云 http://mirrors.aliyun.com/pypi/simple/
中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/
豆瓣(douban) http://pypi.douban.com/simple/
清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/
中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/

使用方法：
pip install tensorflow -i https://pypi.tuna.tsinghua.edu.cn/simple
```

* Index 索引 Groupby 方法

```
loc函数：通过行索引 "Index" 中的具体值来取行数据（如取"Index"为"A"的行）
iloc函数：通过行号来取行数据（如取第二行的数据）

from pandas import Series, DataFrame
data = {'Chinese': [66, 95, 93, 90,80], 'Math': [30, 98, 96, 77, 90], 'English': [65, 85, 92, 88, 90]}
df = DataFrame(data, index=['ZhangFei', 'GuanYu', 'LiuBei', 'DianWei', 'XuChu'], columns=['Chinese', 'Math', 'English'])
# 提取Index为ZhangFei的行
print(df.loc['ZhangFei'])
# 提取第0行
print(df.iloc[0])

# 提取列为English的所有行
print(df.loc[:,['English']])
# 提取第2列的所有行
print(df.iloc[:,2])

# 查看ZhangFei, GuanYu的Chinese Math成绩
print(df.loc[['ZhangFei','GuanYu'], ['Chinese','Math']])
print(df.iloc[[0,1],[0,1]])

Pandas中的groupby使用
作用是进行数据的分组以及分组后地组内运算
import numpy as np
import pandas as pd
# 因为文件中有中文，所以采用gbk编码读取
data = pd.read_csv('heros2.csv', encoding='gbk')
result = data.groupby('role').agg([np.sum, np.mean])
print(result)

显示各元素间的相关性
DataFrame.corr(method='pearson', min_periods=1)
method参数 
pearson，衡量两个数据集合是否在一条线上面，针对线性数据的相关系数计算，对于非线性数据有误差
kendall，反映分类变量相关性的指标，通常用于评分数据一致性水平研究，比如评委打分，数据排名等
spearman：非线性的，非正太分布的数据的相关系数

pearson系数，使用最广泛的相关性统计量，用于测量两组连续变量之间的线性关联程度

# 构造一元二次方程，y=2x*x+1 非线性关系
def compute(x):
    return 2*x*x+1
x=[i for i in range(100)]
y=[compute(i) for i in x]
data = DataFrame({'x':x,'y':y})

# 查看pearson系数
print(data.corr())
print(data.corr(method='spearman'))
print(data.corr(method='kendall'))

```

* 表格数据的函数化
* Pandas 数据相关性分析
* Pandas 数据可视化

```
工具：matplotlib，seaborn
import matplotlib.pyplot as plt
df = pd.DataFrame(np.random.rand(10,4), columns=['a','b','c','d'])
# 使用bar()生成直方图，barh()生成水平条形图（要生成一个堆积条形图，需要指定stacked=True）
df.plot.bar()
df.plot.bar(stacked=True)
df.plot.barh(stacked=True)
plt.show()

# 数据加载
df = pd.read_csv('./shanghai_index_1990_12_19_to_2020_03_12.csv')
df = df[['Timestamp', 'Price']]
# 将时间作为df的索引
df.Timestamp = pd.to_datetime(df.Timestamp)
df.index = df.Timestamp
# 数据探索
print(df.head())
# 按照月，季度，年来统计
df_month = df.resample('M').mean()
df_Q = df.resample('Q-DEC').mean()
df_year = df.resample('A-DEC').mean()
print(df_month)

# 按照天，月，季度，年来显示沪市指数的走势
fig = plt.figure(figsize=[15, 7])
plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签
plt.suptitle('沪市指数', fontsize=20)
plt.subplot(221)
plt.plot(df.Price, '-', label='按天')
plt.legend()
...
plt.subplot(224)
plt.plot(df_year.Price, '-', label='按年')
plt.legend()
plt.show()

线性回归模型

损失函数
损失函数可以衡量模型的好坏
MSE，均方误差，是在回归问题中比较常用的损失函数
```

* sklearn 中的回归分析

```
 sklearn中的线性回归模型
clf = linear_model.LinearRegression()
fit(X,y)，训练，拟合参数
predict(X) ，预测
score(X,y)， 得到评分结果
coef_ ，存放回归系数
intercept_，存放截距

import random
from sklearn import linear_model
reg = linear_model.LinearRegression()
def generate(x):
	y = 2*x+10+random.random()
	return y

train_x = []
train_y = []
for x in range(1000):
	train_x.append([x])
	y = generate(x)
	train_y.append([y])

reg.fit (train_x, train_y)
# coef_ 保存线性模型的系数w
print(reg.coef_)
print(reg.intercept_)


糖尿病回归分析
数据集Diabetes，包含442个患者的10个生理特征（年龄，性别、体重、血压）和一年以后疾病级数指标
10项特征：
年龄，性别，体质指数，血压
s1,s2,s3,s4,s4,s6  (六种血清的化验数据)

Step1，数据加载
Step2，训练集、测试集切分
Step3，使用回归分析模型进行学习
输出回归分析模型的系数coef
Step4，使用测试集进行评价

数据集切分 train_test_split
from sklearn.model_selection import train_test_split  
train_x, test_x, train_y, test_y =train_test_split(train_data,train_target,test_size=0.3, random_state=33)
train_data：样本特征集
train_target：样本标签
test_size：如果是浮点数[0,1]表示样本占比，如果是整数表示样本的数量
random_state：随机数的种子

from sklearn import datasets
from sklearn import linear_model
from sklearn.model_selection import train_test_split  
# 加载数据
diabetes = datasets.load_diabetes()
data = diabetes.data
# 训练集 70%，测试集30%
train_x, test_x, train_y, test_y = train_test_split(diabetes.data, diabetes.target, test_size=0.3, random_state=14)
print(len(train_x))

#回归训练及预测
clf = linear_model.LinearRegression()
clf.fit(train_x, train_y)
print(clf.coef_)
pred_y = clf.predict(test_x)
print(mean_squared_error(test_y, pred_y))

```
* Project B：沪市指数可视化

```
DataReader
Pandas提供了专门从财经网站获取金融数据的API接口
from pandas_datareader.data import DataReader
data_tlz = DataReader("300005.SZ", "yahoo",start,end)
print(data_tlz.head())

# 读取上证综指 及 探路者数据
def load_data():
	if os.path.exists('000001.csv'):
		data_ss = pd.read_csv('000001.csv')
		data_tlz = pd.read_csv('300005.csv')
	else:
		# 上证综指
		data_ss = DataReader("000001.SS", "yahoo",start,end)
		# 300005 探路者股票 深证
		data_tlz = DataReader("300005.SZ", "yahoo",start,end)
		data_ss.to_csv('000001.csv')
		data_tlz.to_csv('300005.csv')
	return data_ss, data_tlz
	
DataFrame.diff()函数
用来将数据进行某种移动之后与原数据进行比较得出的差异数据
DataFrame.shift()函数
可以把数据移动指定的位数
periods=-1 往上移动或往左移动
periods=1 往下移动或往右移动


# 探路者与上证综指
close_ss = data_ss["Close"]
close_tlz = data_tlz["Close"]
# 将探路者与上证综指进行数据合并
stock = pd.merge(data_ss, data_tlz, left_index = True, right_index = True)
stock = stock[["Close_x","Close_y"]]
stock.columns = ["上证综指","探路者"]
# 统计每日收益率
daily_return = (stock.diff()/stock.shift(periods = 1)).dropna()
print(daily_return.head())
# 找出当天收益率大于10%的，应该是没有，因为涨停为10%
print(daily_return[daily_return["探路者"] > 0.1])


# 每日收益率可视化
fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(15,6))
daily_return["上证综指"].plot(ax=ax[0])
ax[0].set_title("上证综指")
daily_return["探路者"].plot(ax=ax[1])
ax[1].set_title("探路者")
plt.show()
# 散点图
fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(12,6))
plt.scatter(daily_return["探路者"],daily_return["上证综指"])
plt.title("每日收益率散点图 from 探路者 & 上证综指")
plt.show()

# 回归分析
import statsmodels.api as sm
# 加入截距项
daily_return["intercept"]=1.0
model = sm.OLS(daily_return["探路者"],daily_return[["上证综指","intercept"]])
results = model.fit()
print(results.summary())

```



## 附: 补充知识
* 推荐书籍: 《推荐系统》弗朗西斯科·里奇 2018 第二版
* 学习要带着需求去学习
* 数据结构与算法是基础，必须完完全全精通
* 可视化: matplotlib、seaborn、pyecharts
* 人工智能的知识体系
	- 贯穿始终的是数据分析/机器学习（通用性）
	- 根据应用场景进行细分（领域知识）
		+ NLP
		+ CV
		+ 推荐系统
		+ 数据挖掘
		+ 强化学习
		+ 等等
* AI 与大数据的相同点和不同的
	- 相同点: 都是与数据打交道
	- 不同点: 
		+ 大数据的数据是海量级的，涉及分布式的存储、性能问题等
		+ AI 是通过数据挖掘出其中的规律，本质是计算（算法）

	>  大数据是 AI 的基础，大数据为 AI 提供数据和计算资源。
	>
	> AI 的 model 考虑的是算法本身，不是分布式架构的设计。
	
* Pyhton 知识补充

* CSV 文件也是一个文本文件
	- 可以按照文本读取
	- pd.read_csv

* ImageNet 是个海量数据集网站 [http://www.image-net.org/](http://www.image-net.org/)

* LRU 最近最少缓存