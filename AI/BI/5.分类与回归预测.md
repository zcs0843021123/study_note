# 分类与回归预测

```
TIPS：我们要PK的是和我们一起竞争的人

实习/工作（公司背书）
比赛
论文
Blog
公众号

```

# 预测全家桶
Project A：员工离职预测

```
员工离职预测
In Class Competition
https://www.kaggle.com/c/bi-attrition-predict/
我们有员工的各种统计信息，以及该员工是否已经离职，统计的信息包括了（工资、出差、工作环境满意度、工作投入度、是否加班、是否升职、工资提升比例等）
现在需要你来通过训练数据得出 员工离职预测，并给出你在测试集上的预测结果。 我们将给出课程上公开的榜单

字段	定义
Age	员工年龄
Attrition	员工是否已经离职，Yes表示离职，No表示未离职
BusinessTravel	商务差旅频率，Non-Travel不出差，TravelRarely不经常出差，TravelFrequently经常出差
DailyRate	平均日工资
Department	员工所在部门，Sales销售部，Research & Development研发部，Human Resources人力资源部
DistanceFromHome	公司跟家庭住址的距离，从1到29，1表示最近，29表示最远
Education	员工的教育程度，从1到5，5表示教育程度最高
EducationField	员工所学习的专业领域，Life Sciences表示生命科学，Medical表示医疗，Marketing表示市场营销，Technical Degree表示技术学位，Human Resources表示人力资源，Other表示其他
EmployeeNumber	员工号码
EnvironmentSatisfaction	员工对于工作环境的满意程度，从1到4，1的满意程度最低，4的满意程度最高
Gender	员工性别，Male表示男性，Female表示女性

字段	定义
JobInvolvement	员工工作投入度，从1到4，1为投入度最低，4为投入度最高
JobLevel	职业级别，从1到5，1为最低级别，5为最高级别
JobRole	工作角色：Sales Executive销售主管，Research Scientist科学研究员，Laboratory Technician实验室技术员，Manufacturing Director制造总监，Healthcare Representative医疗代表，Manager经理，Sales Representative销售代表，Research Director研究总监，Human Resources人力资源
JobSatisfaction	工作满意度，从1到4，1代表满意度最低，4代表最高
MaritalStatus	员工婚姻状况，Single单身，Married已婚，Divorced离婚
MonthlyIncome	员工月收入，范围在1009到19999之间
NumCompaniesWorked	员工曾经工作过的公司数
Over18	年龄是否超过18岁
OverTime	是否加班，Yes表示加班，No表示不加班
PercentSalaryHike	工资提高的百分比

字段	定义
PerformanceRating	绩效评估
RelationshipSatisfaction	关系满意度，从1到4，1表示满意度最低，4表示满意度最高
StandardHours	标准工时
StockOptionLevel	股票期权水平
TotalWorkingYears	总工龄
TrainingTimesLastYear	上一年的培训时长，从0到6，0表示没有培训，6表示培训时间最长
WorkLifeBalance	工作与生活平衡程度，从1到4，1表示平衡程度最低，4表示平衡程度最高
YearsAtCompany	在目前公司工作年数
YearsInCurrentRole	在目前工作职责的工作年数
YearsSinceLastPromotion	距离上次升职时长
YearsWithCurrManager	跟目前的管理者共事年数

常用预测（分类，回归）模型：
分类算法：LR , SVM，KNN
矩阵分解：FunkSVD，BiasSVD，SVD++
FM模型：FM, FFM, DeepFM, NFM，AFM
树模型：GBDT, XGBoost, LightGBM, CatBoost，NGBoost
Attention模型：DIN, DIEN, DSIN

数据预处理
分类算法：LR , SVM，KNN
矩阵分解：FunkSVD，BiasSVD，SVD++
FM模型：FM, FFM, DeepFM, NFM，AFM
树模型：GBDT, XGBoost, LightGBM, CatBoost，NGBoost
Attention模型：DIN, DIEN, DSIN

Step1，对数据进行探索
#工离职预测
import pandas as pd
train=pd.read_csv('train.csv',index_col=0)
test=pd.read_csv('test.csv',index_col=0)
print(train['Attrition'].value_counts())
# 处理Attrition字段
train['Attrition']=train['Attrition'].map(lambda x:1 if x=='Yes' else 0)
# 查看数据中每列是否有空值
print(train.isna().sum())

Step2，去掉无用特征，处理分类特征
# 去掉没用的列 员工号码，标准工时（=80）
train = train.drop(['EmployeeNumber', 'StandardHours'], axis=1)
test = test.drop(['EmployeeNumber', 'StandardHours'], axis=1)
# 对于分类特征进行特征值编码
from sklearn.preprocessing import LabelEncoder
attr=['Age','BusinessTravel','Department','Education','EducationField','Gender','JobRole','MaritalStatus','Over18','OverTime']
for feature in attr:
    lbe=LabelEncoder()
    train[feature]=lbe.fit_transform(train[feature])
    test[feature]=lbe.transform(test[feature])
train.to_csv('temp.csv')

LR工具：
from sklearn.linear_model.logistic import LogisticRegression
参数：
penalty，惩罚项，正则化参数，防止过拟合，l1或l2，默认为l2
C，正则化系数λ的倒数，float类型，默认为1.0
solver，损失函数优化方法，liblinear（默认），lbfgs， newton-cg，sag
random_state，随机数种子
max_iter，算法收敛的最大迭代次数，默认为100
tol=0.0001 : 优化算法停止条件，迭代前后函数差小于tol则终止
verbose=0 : 日志冗长度int：冗长度；0：不输出训练过程；1：偶尔输出； >1：对每个子模型都输出
n_jobs=1 : 并行数，int：个数；-1：跟CPU核数一致；1:默认值


常用方法：
fit(X, y, sample_weight=None)
fit_transform(X, y=None, **fit_params)
predict(X)，用来预测样本，也就是分类
predict_proba(X)，输出分类概率。返回每种类别的概率，按照分类类别顺序给出。
score(X, y, sample_weight=None)，返回给定测试集合的平均准确率（mean accuracy）

Step3，模型参数配置
model = LogisticRegression(max_iter=100, 
                           verbose=True, 
                           random_state=33,
                           tol=1e-4
                          )
model.fit(X_train, y_train)
predict = model.predict_proba(test)[:, 1]
test['Attrition']=predict
# 转化为二分类输出
test['Attrition']=test['Attrition'].map(lambda x:1 if x>=0.5 else 0)
test[['Attrition']].to_csv('submit_lr.csv')

SVM工具：
sklearn中支持向量分类主要有三种方法：SVC、NuSVC、LinearSVC
sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)　
sklearn.svm.NuSVC(nu=0.5, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None) 
sklearn.svm.LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)


常用参数：
C，惩罚系数，类似于LR中的正则化系数，C越大惩罚越大
nu，代表训练集训练的错误率的上限（用于NuSVC）
kernel，核函数类型，RBF, Linear, Poly, Sigmoid，precomputed，默认为RBF径向基核（高斯核函数）
gamma，核函数系数，默认为auto
degree，当指定kernel为'poly'时，表示选择的多项式的最高次数，默认为三次多项式
probability，是否使用概率估计
shrinking，是否进行启发式，SVM只用少量训练样本进行计算
penalty，正则化参数，L1和L2两种参数可选，仅LinearSVC有
loss，损失函数，有‘hinge’和‘squared_hinge’两种可选，前者又称L1损失，后者称为L2损失
tol: 残差收敛条件，默认是0.0001，与LR中的一致

SVM工具：
SVC，Support Vector Classification，支持向量机用于分类
SVR，Support Vector Regression，支持向量机用于回归
sklearn中支持向量分类主要有三种方法：SVC、NuSVC、LinearSVC
基于libsvm工具包实现，台湾大学林智仁教授在2001年开发的一个简单易用的SVM工具包
SVC，C-Support Vector Classification，支持向量分类
NuSVC，Nu-Support Vector Classification，核支持向量分类，和SVC类似，不同的是可以使用参数来控制支持向量的个数
LinearSVC，Linear Support Vector Classification
线性支持向量分类，使用的核函数是linear


libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核
Kernel 核的选择技巧的：
如果样本数量 < 特征数：
方法1：简单的使用线性核就可以，不用选择非线性核
方法2：可以先对数据进行降维，然后使用非线性核
如果样本数量 >= 特征数
可以使用非线性核，将样本映射到更高维度，可以得到比较好的结果

SVM思想：一些线性不可分的问题可能是非线性可分的，也就是在高维空间中存在分离超平面（separating hyperplane）
使用非线性函数从原始的特征空间映射至更高维的空间，转化为线性可分问题

Step3，模型参数配置
model = LinearSVC(max_iter=1000,
		random_state=33,
		verbose=True,
	   )
model.fit(X_train, y_train)
predict = model.predict(test)
print(predict)
test1['Attrition']=predict
test1[['Attrition']].to_csv('submit_svc.csv')
```

Project B：男女声音识别

```

男女声音识别
数据集：voice.csv
3168个录制的声音样本（来自男性和女性演讲者），采集的频率范围是0hz-280hz，已经对数据进行了预处理
一共有21个属性值，请判断该声音是男还是女？
使用Accuracy作为评价标准

字段+	定义
meanfreq	平均频率（单位kHz）
sd	频率标准差
median	中位频率（单位kHz)
Q25	频率第一个四分位数（单位kHz)
Q75	频率第三个四分位数（单位kHz)
IQR	四分位数间距
skew	歪斜
kurt	峰度
sfm	频谱平坦度
mode	波模频率
centroid	质心频率
字段	定义
peakf	峰值频率
meanfun	测量声信号的基频平均值
minfun	测量声信号的最小基频
maxfun	测量声波信号的最大基频
meandom	通过声学信号测量的主导频率的平均值
mindom	通过声学信号测量的最小频率
maxdom	通过声学信号测量的最大频率
dfrange	声学信号测量的主频范围
modindx	调制指数（计算基频相邻测量值之间的累积绝对差除以频率范围）
label	男 or 女

男女声音识别
Step1，数据加载
Step2，数据预处理
分离特征X 和Target y
使用标签编码，male -> 1, female -> 0
将特征X矩阵进行规范化
#标准差标准化，处理后的数据符合标准正态分布
scaler = StandardScaler()

Step3，数据集切分，train_test_split
Step4，模型训练
SVM，Linear SVM
Step5，模型预测

数据预处理
分类算法：LR , SVM，KNN
矩阵分解：FunkSVD，BiasSVD，SVD++
FM模型：FM, FFM, DeepFM, NFM，AFM
树模型：GBDT, XGBoost, LightGBM, CatBoost，NGBoost
Attention模型：DIN, DIEN, DSIN
```

分类算法：LR , SVM，KNN

```
```

矩阵分解：FunkSVD，BiasSVD，SVD++

```
```

FM模型：FM, FFM, DeepFM, NFM，AFM

```
libFM（FM软件）
下载地址：https://github.com/srendle/libfm
使用文档：http://www.libfm.org/libfm-1.42.manual.pdf
不仅适用于推荐系统，还可以用于机器学习（分类问题）
实现三种优化算法：SGD，ALS，MCMC
注意格式：
1、没有列头
2、train.csv中的target
3、test.csv中的target（添加新列）


libFM数据格式
每一行都都包含一个训练集（x，y），首先规定y的值，然后是x的非零值。
对于二分类问题，y>0的类型被认为是正分类，y<=0被认为是负分类。

使用libFM自带的libsvm格式转换
triple_format_to_libfm.pl （perl文件）
-target 目标变量
-delete_column 不需要的变量
triple_format_to_libfm.pl -in ./attrition/train1.csv -target 0 -separator ","
triple_format_to_libfm.pl -in ./attrition/test1.csv -target 0 -separator ","
自动将.csv文件 => .libfm文件
（.csv文件不能有列名）

使用libFM训练FM模型
-train 指定训练集，libfm格式或者二进制文件
-test 指定测试集，libfm格式或者二进制文件
-task，说明任务类型classification还是regression
dim，指定k0，k1，k2，
-iter，迭代次数，默认100
-method，优化方式，可以使用SGD, SGDA, ALS, MCMC，默认为MCMC
-out，指定输出文件
libFM -task r -train ./attrition/train1.csv.libfm -test ./attrition/test1.csv.libfm -dim '1,1,8' -out submit_libfm.txt

使用XLearn
import xlearn as xl
支持LR、FM、FFM
输入数据格式
对于 LR 和 FM ，必须是 CSV 或者 libsvm
对于 FFM，必须是 libffm 格式

#使用XLearn，需要按照数据格式准备
import xlearn as xl
# 设置Model
model = xl.create_fm()
#model = xl.create_ffm()
#model = xl.create_linear()
def train():
	model.setTrain("./attrition/train1.csv")
	# 设置参数
	param = {'task':'binary', 'lr':0.01, 'lambda':0.002, 'metric':'auc', 'epoch':100}
	# 模型训练
	#model.setTXTModel("./model.txt")
	model.fit(param, "./model.out")

def test():
	model.setTest("./attrition/test1.csv")
	# 通过Sigmoid转化到0-1之间
	#model.setSigmoid()
	# 通过setSign转化为0或1
	model.setSign()
	model.predict("./model.out", "./submit_xlearn_fm.txt")
train()
test()

FFM格式转换
https://github.com/SongDark/libffm_converter
参考run.py，将train.csv和test.csv分别转化为 train.ffm和test.ffm
start = time.time()
cmd = 'python {cvt} {src} {des}'.format(cvt='libffm_converter.py', src='test1.csv', des='test1.ffm', nr_thread=16)
subprocess.call(cmd, shell=True)
print(time.time() - start, 'sec')

DeepCTR工具
文档：https://deepctr-doc.readthedocs.io/en/latest/deepctr.models.deepfm.html
deepctr.models.deepfm.DeepFM(linear_feature_columns, dnn_feature_columns, fm_group=['default_group'], dnn_hidden_units=(128, 128), l2_reg_linear=1e-05, l2_reg_embedding=1e-05, l2_reg_dnn=0, init_std=0.0001, seed=1024, dnn_dropout=0, dnn_activation='relu', dnn_use_bn=False, task='binary')

DeepCTR工具
model.compile(optimizer='Adam', loss=None, metrics=None)
optimizer：优化器，如Adam
loss：计算损失，比如binary_crossentropy 二分类交叉熵损失
metrics: 列表，包含评估模型在训练和测试时的性能的指标，比如metrics=[‘accuracy’]，如果要在多输出模型中为不同的输出指定不同的指标，可向该参数传递一个字典，例如metrics={‘output_a’: ‘accuracy’}

通过examples快速上手工具
https://github.com/shenweichen/DeepCTR/blob/master/examples/run_classification_criteo.py

Project A：员工离职预测
sparse_features = cate
# 除了分类特征以外，其余都是稠密类型
dense_features = list(set([i if i not in cate else '' for i in train.drop('Attrition', axis=1).columns]))
dense_features.remove('')
# 处理缺失值
train[sparse_features] = train[sparse_features].fillna('-1', )
train[dense_features] = train[dense_features].fillna(0, )
# 对离散特征进行标签编码
target = ['Attrition']
for feat in sparse_features:
    lbe = LabelEncoder()
    data[feat] = lbe.fit_transform(data[feat])
print(data)

# 对数据进行0-1规范化
mms = MinMaxScaler(feature_range=(0, 1))
train[dense_features] = mms.fit_transform(train[dense_features])
test[dense_features] = mms.fit_transform(test[dense_features])
# 处理定长离散特征
fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())
                          for feat in sparse_features] + [DenseFeat(feat, 1,)
                          for feat in dense_features]
dnn_feature_columns = fixlen_feature_columns
linear_feature_columns = fixlen_feature_columns
print(fixlen_feature_columns)

# 得到所有特征名称
feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)
train_model_input = {name:train[name] for name in feature_names}
model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary')
model.compile("adam", "binary_crossentropy", metrics=['binary_crossentropy'], )
history = model.fit(train_model_input, train[target].values,
                    batch_size=256, epochs=50, verbose=2, validation_split=0.2, )

# 对测试集进行预测
test_model_input = {name:test[name] for name in feature_names}
predict = model.predict(test_model_input, batch_size=256)
# 转化为二分类输出
test1['Attrition'] = predict
test1['Attrition']=test1['Attrition'].map(lambda x:1 if x>=0.5 else 0)
# 使用user_id作为索引
test1.set_index(["user_id"], inplace=True)
test1[['Attrition']].to_csv('submit_deepfm.csv')

from deepctr.models import NFM
……
model = NFM(linear_feature_columns, dnn_feature_columns, task='binary')
……
# 对测试集进行预测
test_model_input = {name:test[name] for name in feature_names}
predict = model.predict(test_model_input, batch_size=256)
# 转化为二分类输出
test1['Attrition'] = predict
test1['Attrition']=test1['Attrition'].map(lambda x:1 if x>=0.5 else 0)
# 使用user_id作为索引
test1.set_index(["user_id"], inplace=True)
test1[['Attrition']].to_csv('submit_nfm.csv')

LR优点：
 实现简单，广泛的应用于工业问题上；
分类时计算量非常小，速度很快，使用资源低；
方便观测样本概率分数；
LR缺点：
当特征空间很大时，LR的性能不是很好；
容易欠拟合，准确度不太高；
不能很好地处理大量多类特征或变量；
通常只处理二分类问题，多分类需要使用softmax（LR在多分类的推广），且必须线性可分；
对于非线性特征，需要进行转换；

SVM优点：
可以解决高维问题，即大型特征空间；
能够处理非线性特征的相互作用；
需要先对数据进行归一化，因为计算是基于距离的模型，所以SVM和LR都需要对数据进行归一化处理
SVM缺点：
当样本很多时，效率并不是很高；
对非线性问题没有通用解决方案，可能会很难找到合适核函数
对缺失数据敏感；
SVM核的选择是有技巧的，样本数量<特征数，线性核，大于特征数使用非线性核

FM，考虑了特征交叉，即使数据非常稀疏的情况下，依然能估计出可靠的参数并进行预测
FFM，在FM基础上引入了field（场），使得每两组特征交叉的隐向量都是独立的，预测效果更好
embedding + MLP，深度学习CTR预估的通用框架
Wide & Deep，LR+DNN，记忆能力+泛化能力
DeepFM，并行结构，FM和DNN分开计算
NFM，串行架构，将FM的结果作为DNN的输入
AFM，在FM的基础上增加Attention机制，对简化版NFM进行加权求和
DIN，Attention机制，对用户历史行为序列进行挖掘

embedding + MLP，深度学习CTR预估的通用框架：
Step1，对不同领域的one-hot特征进行嵌入（embedding），使其降维成低维度稠密特征
Step2，将这些特征向量拼接（concatenate）成一个隐含层
Step3，再不断堆叠全连接层，也就是多层感知机(Multilayer Perceptron, MLP，也称为前馈神经网络)
Step4，最终输出预测的点击率
FNN，使用FM对embedding层进行初始化

Wide & Deep模型
Wide模型，采用Linear Regression，解决模型的记忆能力（特征组合需要人来设计）
Deep模型，即FNN，解决模型的泛化能力
Joint Training，同时训练Wide模型和Deep模型，并将两个模型的结果的加权作为最终的预测结果


DeepFM模型
DeepFM是将Wide & Deep模型中的Wide替换成了FM模型

NFM模型
FNN, Wide & Deep, DeepFM都是在DNN部分，对embedding之后的特征进行concatenate，没有充分进行特征交叉计算。
NFM算法是对embedding直接采用对位相乘（element-wise）后相加起来作为交叉特征，然后通过DNN直接将特征压缩，最后concatenate linear部分和deep部分的特征。
两种FM和DNN的结合方式：
DeepFM, 并行结构，FM和DNN分开计算
NFM,串行架构，将FM的结果作为DNN的输入

FM和SVM的区别：
 SVM的二元特征交叉参数是独立的，而FM的二元特征交叉参数是两个k维的向量vi、vj，交叉参数就不是独立的，而是相互影响的
在数据稀疏的情况下，线性SVM在和多项式SVM效果比较差，线性svm只有一维特征，不能挖掘深层次的组合特征；而多项式svm，交叉的多个特征需要在训练集上共现才能被学习到，否则该对应的参数就为0
而FM不一样，通过向量化的交叉，可以学习到不同特征之间的交互，进行提取到更深层次的抽象意义

FM和LR的区别
LR是从组合特征的角度去描述单特征之间的交互组合；FM实际上是从模型（latent factor model）的角度来做的。即FM中特征的交互是模型参数的一部分。
FM是通过MF的思想，基于latent factor，来降低交叉项参数学习不充分的影响
FM能很大程度上避免了数据稀疏行造成参数估计不准确的影响

每种模型都有适用的场景
可以使用LR模型作为预测的Baseline
FM衍生模型在推荐系统，尤其是CTR预估中有广泛应用，弥补了LR模型的不足（需要人工组合特征，耗费大量时间和人力）
Attention机制，对于Diversity多样性的情况，Attention机制可以提升效率，并且得出更好的结果
Tree Ensemble模型，比如GBDT，使用广泛，因为训练模型更可控
对于 LR 模型，如果欠拟合，需要增加feature，才能提高准确率。而对于 tree-ensemble 来说，解决方法是训练更多的决策树 tree。Kaggle比赛中使用很多

常用预测模型：
分类算法：LR , SVM，KNN
矩阵分解：FunkSVD，BiasSVD，SVD++
FM模型：FM, FFM, DeepFM, NFM，AFM
树模型：GBDT, XGBoost, LightGBM, CatBoost，NGBoost
Attention模型：DIN, DIEN, DSIN

```

树模型：GBDT, XGBoost, LightGBM, CatBoost，NGBoost

```
```

Attention模型：DIN, DIEN, DSIN

```
```



# 机器学习神器

什么是集成学习

```
集成学习：
思想，将多个弱分类器按照某种方式组合起来，形成一个强分类器（三个臭皮匠赛过诸葛亮）
Bagging，把数据集通过有放回的抽样方式，划分为多个数据集，分别训练多个模型。针对分类问题，按照少数服从多数原则进行投票，针对回归问题，求多个测试结果的平均值
Stacking，通常是不同的模型，而且每个分类都用了全部训练数据，得到预测结果y1, y2, ..., yk，然后再训练一个分类器 Meta Classifier，将这些预测结果作为输入，得到最终的预测结果


集成学习：
Boosting，与Bagging一样，使用的相同的弱学习器，不过是以自适应的方法顺序地学习这些弱学习器，即每个新学习器都依赖于前面的模型，并按照某种确定性的策略将它们组合起来
两个重要的 Boosting 算法：AdaBoost（自适应提升）和Gradient Boosting（梯度提升）
AdaBoost，使用前面的学习器用简单的模型去适配数据，然后分析错误。然后会给予错误预测的数据更高权重，然后用后面的学习器去修复
Boosting通过把一些列的弱学习器串起来，组成一个强学习器


集成学习：训练弱学习器，并添加到集成模型中
更新：基于当前的集成学习结果，更新训练集（值或权重）

Boosting与Bagging：
结构上，Bagging是基分类器并行处理，而Boosting是串行处理
训练集，Bagging的基分类器训练是独立的，而Boosting的训练集是依赖于之前的模型
作用，Bagging的作用是减少variance，而Boosting在于减少bias
对于Bagging，对样本进行重采样，通过重采样得到的子样本集训练模型，最后取平均。因为子样本集的相似性，而且使用相同的弱学习器，因此每个学习器有近似相等的bias和variance，因为每个学习器相互独立，所以可以显著降低variance，但是无法降低bias
对于Boosting，采用顺序的方式最小化损失函数，所以bias自然是逐步下降，子模型之和不能显著降低variance

Gradient Boosting集成学习：
XGBoost, LightGBM, CatBoost, NGBoost实际上是对GBDT方法的不同实现，针对同一目标、做了不同的优化处理

XGBoost：
https://arxiv.org/abs/1603.02754
对于一个问题，INPUT X: age, gender, occupation, .... 


Target y: How does the person like computer games?



XGBoost：
目标函数=损失函数 + 正则化项


误差函数尽量拟合训练数据，正则化项鼓励简单的模型
            用于控制树的复杂度，防止过拟合，使得模型更简化，也使得最终的模型的预测结果更稳定


Tree Ensemble 集成学习 ：
单个CART回归树过于简单，可以通过多个CART回归树组成一个强学习器
预测函数，样本的预测结果=每棵树预测分数之和

目标函数优化


正则项是由叶子结点的数量和叶子结点权重的平方和决定



XGBoost的分裂节点算法：
贪心方法，获取最优分割节点（split point）
将所有样本按照gi从小到大排序，通过遍历，查看每个节点是否需要分割
对于特征值的个数为n时，总共有n−1种划分
Step1，对样本扫描一遍，得出GL，GR
Step2，根据Gain的分数进行分割
通过贪心法，计算效率得到大幅提升，XGBoost重新定义划分属性，即Gain，而Gain的计算是由目标损失函数obj决定的



XGBoost的分裂节点算法（近似算法，Histogram 2016 paper）：
对于连续型特征值，样本数量非常大，该特征取值过多时，遍历所有取值会花费很多时间，且容易过拟合
方法，在寻找split节点的时候，不会枚举所有的特征值，而会对特征值进行聚合统计，然后形成若干个bucket(桶)，只将bucket边界上的特征值作为split节点的候选，从而获得性能提升
从算法伪代码中该流程还可以分为两种，全局的近似是在新生成一棵树之前就对各个特征计算分位点并划分样本，之后在每次分裂过程中都采用近似划分，而局部近似就是在具体的某一次分裂节点的过程中采用近似算法


XGBoost算法特点：
XGBoost将树模型的复杂度加入到正则项中，从而避免过拟合，泛化性能好
损失函数是用泰勒展开式展开的，用到了一阶导和二阶导，可以加快优化速度
在寻找最佳分割点时，采用近似贪心算法，用来加速计算
不仅支持CART作为基分类器，还支持线性分类器，在使用线性分类器的时候可以使用L1，L2正则化
支持并行计算，XGBoost的并行是基于特征计算的并行，将特征列排序后以block的形式存储在内存中，在后面的迭代中重复使用这个结构。在进行节点分裂时，计算每个特征的增益，选择增益最大的特征作为分割节点，各个特征的增益计算可以使用多线程并行
优点：速度快、效果好、能处理大规模数据、支持自定义损失函数等
缺点：算法参数过多，调参复杂，不适合处理超高维特征数据


XGBoost工具：
https://github.com/dmlc/xgboost
参数分为：
通用参数：对系统进行控制
Booster参数：控制每一步的booster(tree/regression)
学习目标参数：控制训练目标的表现


通用参数：
booster，模型选择，gbtree或者gblinear。gbtree使用基于树的模型进行提升计算，gblinear使用线性模型进行提升计算。[default=gbtree]
silent，缄默方式，0表示打印运行时，1表示以缄默方式运行，不打印运行时信息。[default=0]
nthread，XGBoost运行时的线程数，[default=缺省值是当前系统可以获得的最大线程数]
num_feature，boosting过程中用到的特征个数，XGBoost会自动设置

Booster参数：
eta [default=0.3]，为了防止过拟合，更新过程中用到的收缩步长。在每次提升计算之后，算法会直接获得新特征的权重。 eta通过缩减特征的权重使提升计算过程更加保守，取值范围为[0,1]
gamma [default=0]，分裂节点时，损失函数减小值只有大于等于gamma节点才分裂，gamma值越大，算法越保守，越不容易过拟合，但性能就不一定能保证，需要trade off，取值范围 [0,∞]
max_depth [default=6] ，树的最大深度，取值范围为[1,∞]，典型值为3-10
min_child_weight [default=1]，一个子集的所有观察值的最小权重和。如果新分裂的节点的样本权重和小于min_child_weight则停止分裂 。这个可以用来减少过拟合，但是也不能太高，会导致欠拟合，取值范围为[0,∞]



Booster参数：
subsample [default=1]，构建每棵树对样本的采样率，如果设置成0.5，XGBoost会随机选择50%的样本作为训练集
colsample_bytree [default=1]，列采样率，也就是特征采样率
lambda [default=1, alias: reg_lambda]，L2正则化，用来控制XGBoost的正则化部分
alpha [default=0, alias: reg_alpha]，L1正则化，增加该值会让模型更加收敛
scale_pos_weight [default=1]，在类别高度不平衡的情况下，将参数设置大于0，可以加快收敛




学习目标参数：
objective [ default=reg:linear ]，定义学习目标，reg:linear，reg:logistic，binary:logistic，binary:logitraw，count:poisson，multi:softmax，multi:softprob，rank:pairwise
eval_metric，评价指标，包括rmse，logloss，error，merror，mlogloss，auc，ndcg，map等
seed[ default=0 ]，随机数的种子
dtrain，训练的数据
num_boost_round，提升迭代的次数，也就是生成多少基模型
early_stopping_rounds，早停法迭代次数
evals：这是一个列表，用于对训练过程中进行评估列表中的元素。形式是evals = [(dtrain,'train'),(dval,'val')]或者是evals = [(dtrain,'train')]，对于第一种情况，它使得我们可以在训练过程中观察验证集的效果
verbose_eval ，如果为True，则对evals中元素的评估输出在结果中；如果输入数字，比如5，则每隔5个迭代输出一次
learning_rates：每一次提升的学习率的列表



# 天猫用户复购预测（XGBoost使用示意）
X_train, X_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=.2)
# 使用XGBoost
model = xgb.XGBClassifier(
    max_depth=8, #树的最大深度
    n_estimators=1000, #提升迭代的次数，也就是生成多少基模型
    min_child_weight=300, #一个子集的所有观察值的最小权重和
    colsample_bytree=0.8, #列采样率，也就是特征采样率
    subsample=0.8, #构建每棵树对样本的采样率
    eta=0.3,    # eta通过缩减特征的权重使提升计算过程更加保守，防止过拟合
    seed=42    #随机数种子
)

model.fit(
    X_train, y_train,
    eval_metric='auc', eval_set=[(X_train, y_train), (X_valid, y_valid)],
    verbose=True,
    #早停法，如果auc在10epoch没有进步就stop
    early_stopping_rounds=10 
)
model.fit(X_train, y_train)
prob = model.predict_proba(test_data)


param = {'boosting_type':'gbdt',
                         'objective' : 'binary:logistic', #任务目标
                         'eval_metric' : 'auc', #评估指标
                         'eta' : 0.01, #学习率
                         'max_depth' : 15, #树最大深度
                         'colsample_bytree':0.8, #设置在每次迭代中使用特征的比例
                         'subsample': 0.9, #样本采样比例
                         'subsample_freq': 8, #bagging的次数
                         'alpha': 0.6, #L1正则
                         'lambda': 0, #L2正则
        }
        

X_train, X_valid, y_train, y_valid = train_test_split(train.drop('Attrition',axis=1), train['Attrition'], test_size=0.2, random_state=42)
train_data = xgb.DMatrix(X_train, label=y_train)
valid_data = xgb.DMatrix(X_valid, label=y_valid)
test_data = xgb.DMatrix(test)
model = xgb.train(param, train_data, evals=[(train_data, 'train'), (valid_data, 'valid')], num_boost_round = 10000, early_stopping_rounds=200, verbose_eval=25)
predict = model.predict(test_data)
test['Attrition']=predict
# 转化为二分类输出
test['Attrition']=test['Attrition'].map(lambda x:1 if x>=0.5 else 0)
test[['Attrition']].to_csv('submit_lgb.csv')

LightGBM：
2017年经微软推出，XGBoost的升级版
Kaggle竞赛使用最多的模型之一，必备机器学习神器
Light => 在大规模数据集上运行效率更高
GBM => Gradient Boosting Machine


Motivation：
常用的机器学习算法，例如神经网络等算法，都可以以mini-batch的方式训练，训练数据的大小不会受到内存限制
GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。对于工业级海量的数据，普通的GBDT算法是不能满足其需求的
LightGBM的提出是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地用于工业场景


数据集	XGBoost	XGBoost_approx	LightGBM
Higgs	4.853GB	4.875GB	0.822GB
Yahoo LTR	1.907GB	2.221GB	0.831GB
MS LTR	5.469GB	5.600GB	0.745GB
Expo	1.553GB	1.560GB	0.450GB

数据集	指标	XGBoost	XGBoost_approx	LightGBM
Higgs	AUC	0.839528	0.840533	0.845123
Yahoo LTR	NDCG@5	0.740316	0.739363	0.756369
MS LTR	NDCG@5	0.476245	0.474441	0.510153
Expo	AUC	0.75548	0.757071	0.781061

LightGBM与XGBoost：
模型精度：两个模型相当
训练速度：LightGBM训练速度更快 => 1/10
内存消耗：LightGBM占用内存更小 => 1/6
特征缺失值：两个模型都可以自动处理特征缺失值
分类特征：XGBoost不支持类别特征，需要对其进行OneHot编码，而LightGBM支持分类特征

XGBoost模型的复杂度：
模型复杂度 = 树的棵数 X 每棵树的叶子数量 X 每片叶子生成复杂度
每片叶子生成复杂度 = 特征数量 X 候选分裂点数量 X 样本的数量
针对XGBoost的优化：
Histogram算法，直方图算法 => 减少候选分裂点数量
GOSS算法，基于梯度的单边采样算法 => 减少样本的数量
EFB算法，互斥特征捆绑算法 => 减少特征的数量
LightGBM = XGBoost + Histogram + GOSS + EFB

XGBoost的预排序（pre-sorted）算法：
将样本按照特征取值排序，然后从全部特征取值中找到最优的分裂点位
预排序算法的候选分裂点数量=样本特征不同取值个数减1



LightGBM的Histogram算法：
替代XGBoost的预排序算法
思想是先连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图，即将连续特征值离散化到k个bins上（比如k=255）
当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点
XGBoost需要遍历所有离散化的值，LightGBM只要遍历k个直方图的值
候选分裂点数量 = k-1

GOSS算法：
Gradient-based One-Side Sampling，基于梯度的单边采样算法
思想是通过样本采样，减少目标函数增益Gain的计算复杂度
单边采样，只对梯度绝对值较小的样本按照一定比例进行采样，而保留了梯度绝对值较大的样本
因为目标函数增益主要来自于梯度绝对值较大的样本 => GOSS算法在性能和精度之间进行了很好的trade off

EFB算法：
Exclusive Feature Bundling，互斥特征绑定算法
思想是特征中包含大量稀疏特征的时候，减少构建直方图的特征数量，从而降低计算复杂度
数据集中通常会有大量的稀疏特征（大部分为0，少量为非0）我们认为这些稀疏特征是互斥的，即不会同时取非零值
EFB算法可以通过对某些特征的取值重新编码，将多个这样互斥的特征绑定为一个新的特征
类别特征可以转换成onehot编码，这些多个特征的onehot编码是互斥的，可以使用EFB将他们绑定为一个特征
在LightGBM中，可以直接将每个类别取值和一个bin关联，从而自动地处理它们，也就无需预处理成onehot编码

LightGBM工具：
import lightgbm as lgb
官方文档：http://lightgbm.readthedocs.io/en/latest/Python-Intro.html
参数：
boosting_type，训练方式，gbdt
objective，目标函数，可以是binary，regression
metric，评估指标，可以选择auc, mae，mse，binary_logloss，multi_logloss
max_depth，树的最大深度，当模型过拟合时，可以降低 max_depth
min_data_in_leaf，叶子节点最小记录数，默认20

Bagging参数：bagging_fraction+bagging_freq（需要同时设置）
bagging_fraction，每次迭代时用的数据比例，用于加快训练速度和减小过拟合
bagging_freq：bagging的次数。默认为0，表示禁用bagging，非零值表示执行k次bagging，可以设置为3-5
feature_fraction，设置在每次迭代中使用特征的比例，例如为0.8时，意味着在每次迭代中随机选择80％的参数来建树
early_stopping_round，如果一次验证数据的一个度量在最近的round中没有提高，模型将停止训练

参数：
lambda，正则化项，范围为0～1
min_gain_to_split，描述分裂的最小 gain，控制树的有用的分裂
max_cat_group，在 group 边界上找到分割点，当类别数量很多时，找分割点很容易过拟合时
num_boost_round，迭代次数，通常 100+
num_leaves，默认 31
device，指定cpu 或者 gpu
max_bin，表示 feature 将存入的 bin 的最大数量
categorical_feature，如果 categorical_features = 0,1,2， 则列 0，1，2是 categorical 变量
ignore_column，与 categorical_features 类似，只不过不是将特定的列视为categorical，而是完全忽略



param = {'boosting_type':'gbdt',
                         'objective' : 'binary', #任务类型
                         'metric' : 'auc', #评估指标
                         'learning_rate' : 0.01, #学习率
                         'max_depth' : 15, #树的最大深度
                         'feature_fraction':0.8, #设置在每次迭代中使用特征的比例
                         'bagging_fraction': 0.9, #样本采样比例
                         'bagging_freq': 8, #bagging的次数
                         'lambda_l1': 0.6, #L1正则
                         'lambda_l2': 0, #L2正则
        }



X_train, X_valid, y_train, y_valid = train_test_split(train.drop('Attrition',axis=1), train['Attrition'], test_size=0.2, random_state=42)
trn_data = lgb.Dataset(X_train, label=y_train)
val_data = lgb.Dataset(X_valid, label=y_valid)
model = lgb.train(param,train_data,valid_sets=[train_data,valid_data],num_boost_round = 10000 ,early_stopping_rounds=200,verbose_eval=25, categorical_feature=attr)
predict=model.predict(test)
test['Attrition']=predict
# 转化为二分类输出
test['Attrition']=test['Attrition'].map(lambda x:1 if x>=0.5 else 0)
test[['Attrition']].to_csv('submit_lgb.csv')

CatBoost算法：
俄罗斯科技公司Yandex开源的机器学习库（2017年）
https://arxiv.org/pdf/1706.09516.pdf
CatBoost = Catgorical + Boost
高效的处理分类特征（categorical features），首先对分类特征做统计，计算某个分类特征（category）出现的频率，然后加上超参数，生成新的数值型特征（numerical features）
同时使用组合类别特征，丰富了特征维度
采用的基模型是对称决策树，算法的参数少、支持分类变量，通过可以防止过拟合


CatBoost，LightGBM，XGBoost对比：
2015 年航班延误数据，包含分类和数值变量
https://www.kaggle.com/usdot/flight-delays/data
一共有约 500 万条记录，使用10%的数据，即50条记录
CatBoost 过拟合程度最小，在测试集上准确度最高0.816，同时预测用时最短，但这个表现仅仅在有分类特征，而且调节了one-hot最大量时才会出现
如果不利用 CatBoost 算法在这些特征上的优势，表现效果就会变成最差，AUC 0.752 
使用CatBoost需要数据中包含分类变量，同时适当地调节这些变量时， 才会表现不错

模型	XGBoost	LightGBM		CatBoost	
使用参数	"max_depth:50
learning_rate:0.16
min_child_weight:1
n_estimators:200"	"max_depth:50
learning_rate:0.1
num_leaves:900
n_estimators:300"		"max_depth:10
learning_rate:0.15
l2_leaf_reg=9
iteration:500
one_hot_max_size=50"	
训练集AUC	0.999	没有使用分类特征索引	使用分类特征索引	没有使用分类特征索引	使用分类特征索引
		0.992	0.999	0.842	0.887
测试集AUC	0.789	0.785	0.772	0.752	0.816
训练用时	970秒	153秒	326秒	180秒	390秒
预测用时	184秒	40秒	156秒	2秒	14秒


CatBoost，LightGBM，XGBoost对比：
处理特征为分类的神器
支持即用的分类特征，因此我们不需要对分类特征进行预处理（比如使用LabelEncoding 或 OneHotEncoding）
CatBoost 设计了一种算法验证改进，避免了过拟合。因此处理分类数据比LightGBM 和XGBoost 强
准确性比 XGBoost 更高，同时训练时间更短
支持 GPU 训练
可以处理缺失的值


CatBoost工具：
https://github.com/dmlc/xgboost
https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html
构造参数：
learning_rate，学习率
depth， 树的深度
l2_leaf_reg，L2正则化系数
n_estimators，树的最大数量，即迭代次数
one_hot_max_size，one-hot编码最大规模，默认值根据数据和训练环境的不同而不同
loss_function ，损失函数，包括Logloss，RMSE，MAE，CrossEntropy，回归任务默认RMSE，分类任务默认Logloss
eval_metric，优化目标，包括RMSE，Logloss，MAE，CrossEntropy，Recall，Precision，F1，Accuracy，AUC，R2



fit函数参数：
X，输入数据数据类型可以是：list; pandas.DataFrame; pandas.Series
y=None
cat_features=None，用于处理分类特征
sample_weight=None，输入数据的样本权重
logging_level=None，控制是否输出日志信息，或者其他信息
plot=False，训练过程中，绘制，度量值，所用时间等
eval_set=None，验证集合，数据类型list(X, y)tuples
baseline=None
use_best_model=None
verbose=None


model = CatBoostClassifier(iterations=1000, #最大树数，即迭代次数
                              depth = 6, #树的深度
                               learning_rate = 0.03, #学习率
                               custom_loss='AUC', #训练过程中，用户自定义的损失函数
                               eval_metric='AUC', #过拟合检验（设置True）的评估指标，用于优化
                               bagging_temperature=0.83, #贝叶斯bootstrap强度设置
                               rsm = 0.78, #随机子空间
                               od_type='Iter', #过拟合检查类型
                               od_wait=150, #使用Iter时，表示达到指定次数后，停止训练
                               metric_period = 400, #计算优化评估值的频率
                               l2_leaf_reg = 5, #l2正则参数
                               thread_count = 20, #并行线程数量
                               random_seed = 967 #随机数种子
                              )
                              
model = cb.CatBoostClassifier(iterations=1000, 
                              depth=7, 
                              learning_rate=0.01, 
                              loss_function='Logloss', 
                              eval_metric='AUC',
                              logging_level='Verbose', 
                              metric_period=50)
# 得到分类特征的列号
categorical_features_indices = []
for i in range(len(X_train.columns)):
    if X_train.columns.values[i] in attr:
        categorical_features_indices.append(i)
print(categorical_features_indices)

attr=['Age','BusinessTravel','Department','Education','EducationField','Gender','JobRole','MaritalStatus','Over18','OverTime']


model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=categorical_features_indices)
predict = model.predict(test)
test['Attrition']=predict
test[['Attrition']].to_csv('submit_cb.csv')

LighGBM效率高，在Kaggle比赛中应用多
CatBoost对于分类特征多的数据，可以高效的处理，过拟合程度小，效果好
XGBoost, LightGBM, CatBoost参数较多，调参需要花大量时间
Boosting集成学习包括AdaBoosting和Gradient Boosting
Boosting只是集成学习中的一种（Bagging, Stacking）





```

GBDT原理

```
```

XGBoost

```
```

LightGBM

```
```

CatBoost

```
```

在Project中使用机器学习神器

```
```

