# EDA 与数据可视化

# 初识矩阵分解
推荐系统的算法都有哪些
```
协同过滤是推荐系统的主流思想之一：
基于邻域的协同过滤
UserCF
ItemCF
基于模型的协同过滤
隐语义模型（LFM, Latent Factor Model）
矩阵分解（MF）
LDA, LSA, pLSA
基于贝叶斯网络
基于SVM

什么是隐语义模型：
用户与物品之间存在着隐含的联系
通过隐含特征（Latent Factor）联系用户兴趣和物品，基于用户行为的自动聚类
我们可以指定隐特征的个数，粒度可粗（隐特征少），可细（隐特征多）
计算物品属于每个隐特征的权重，物品有多个隐特征的权重
可解释性差，隐含特征计算机能理解就好，相比之下ItemCF可解释性强

基于模型与基于邻域的推荐之间的区别：
基于邻域的协同过滤包括UserCF, ItemCF，将用户的所有数据读入到内存中进行运算，也称之为基于内存的协同过滤（Memory-based）。数据量少的情况下，可以在线实时推荐
基于模型的推荐（Model-based），采用机器学习的方式，分成训练集和测试集。离线训练时间比较长，但训练完成后，推荐过程比较快。


```

什么是矩阵分解

```

推荐系统的两大应用场景
评分预测（Rating Prediction）
主要用于评价网站，比如用户给自己看过的电影评多少分（MovieLens），或者用户给自己看过的书籍评价多少分（Douban）。矩阵分解技术主要应用于评分预测问题。
Top-N推荐（Item Ranking）
常用于购物网站，拿不到显式评分，通过用户的隐式反馈为用户提供一个可能感兴趣的Item列表。排序任务，需要排序模型进行建模。

为用户找到其感兴趣的item推荐给他
用矩阵表示收集到的用户行为数据，12个用户，9部电影

矩阵分解要做的是预测出矩阵中缺失的评分，使得预测评分能反映用户的喜欢程度
可以把预测评分最高的前K个电影推荐给用户了。

如何从评分矩阵中分解出User矩阵和Item矩阵
只有左侧的评分矩阵R是已知的
User矩阵和Item矩阵是未知
学习出User矩阵和Item矩阵，使得User矩阵*Item矩阵与评分矩阵中已知的评分差异最小 => 最优化问题

观察User矩阵：用户的听歌爱好体现在User向量上
观察Item矩阵，电影的风格也会体现在Item向量上
MF用user向量和item向量的内积去拟合评分矩阵中该user对该item的评分，内积的大小反映了user对item的喜欢程度。内积大匹配度高，内积小匹配度低。
隐含特征个数k，k越大，隐类别分得越细，计算量越大。

某个用户u对电影i的预测评分 = User向量和Item向量的内积
把这两个矩阵相乘，就能得到每个用户对每部电影的预测评分了，评分值越大，表示用户喜欢该电影的可能性越大，该电影就越值得推荐给用户。

目标函数最优化问题的工程解法：
ALS，Alternating Least Squares，交替最小二乘法
SGD，Stochastic Gradient Descent，随机梯度下降

```

矩阵分解中的ALS算法

```

ALS, Alternative Least Square, ALS，交替最小二乘法
Step1，固定Y 优化X
Step2，固定X 优化Y
重复Step1和2，直到X 和Y 收敛。每次固定一个矩阵，优化另一个矩阵，都是最小二乘问题

N次试验，每次观测值略有不同，实际观测值应该为多少？
观测值x=(9.8+9.9+9.8+10.2+10.3)/5=10
为什么要用平均数，而不是中位数，或者是几何平均数？
最小二乘法由统计学家道尔顿(F.Gallton)提出

spark mllib库（spark3.0版本后废弃）
支持常见的算法，包括 分类、回归、聚类和协同过滤
from pyspark.mllib.recommendation import ALS, Rating, MatrixFactorizationModel
spark ml库（官方推荐）
功能更全面更灵活，ml在DataFrame上的抽象级别更高，数据和操作耦合度更低，使用起来像sklearn
Spark安装很多坑，需要慢慢来
Python代码
https://github.com/tushushu/imylu/blob/master/imylu/recommend/als.py

数据集：MovieLens
下载地址：https://www.kaggle.com/jneupane12/movielens/download
主要使用的文件：ratings.csv
格式：userId, movieId, rating, timestamp
记录了用户在某个时间对某个movieId的打分情况
我们需要补全评分矩阵，然后对指定用户，比如userID为1-5进行预测

直接使用Python
class Matrix(object):
class ALS(object):
X = load_movie_ratings()
model = ALS()
model.fit(X, k=3, max_iter=3)
print("对用户进行推荐")
user_ids = range(1, 5)
predictions = model.predict(user_ids, n_items=2)
for user_id, prediction in zip(user_ids, predictions):
    _prediction = [format_prediction(item_id, score)
                   for item_id, score in prediction]
    print("User id:%d recommedation: %s" % (user_id, _prediction))
    

```

目标函数最优化问题（ALS，SGD）

```

目标函数最优化问题的工程解法：
ALS，Alternating Least Squares，交替最小二乘法
SGD，Stochastic Gradient Descent，随机梯度下降
基本思路是以随机方式遍历训练集中的数据，并给出每个已知评分的预测评分。用户和物品特征向量的调整就沿着评分误差越来越小的方向迭代进行，直到误差达到要求。所以，SGD不需要遍历所有的样本即可完成特征向量的求解。

批量梯度下降：
在每次更新时用所有样本
稳定，收敛慢
随机梯度下降
每次更新时用1个样本，用1个样本来近似所有的样本
更快收敛，最终解在全局最优解附近
mini-batch梯度下降
每次更新时用b个样本，折中方法
速度较快
```

推荐系统工具：Surprise与LightFM

```
```

Baseline算法

```

ALS和SGD作为优化方法，应用于很多优化问题
Factor in the Neighbors: Scalable and Accurate Collaborative Filtering，ACM Transactions on Knowledge Discovery from Data, 2010
Baseline算法：基于统计的基准预测线打分
bui 预测值
bu 用户对整体的偏差
bi 商品对整体的偏差

Baseline算法
使用ALS进行优化
Step1，固定bu，优化bi
Step2，固定bi，优化bu

Surprise
Surprise是scikit系列中的一个推荐系统库
文档：https://surprise.readthedocs.io/en/stable/
LightFM
Python推荐算法库，具有隐式和显式反馈的多种推荐算法实现。
易用、快速（通过多线程模型估计），能够产生高质量的结果。

Surprise中的常用算法：
Baseline算法
基于邻域的协同过滤
矩阵分解：SVD，SVD++，PMF，NMF
SlopeOne 协同过滤算法

算法	描述
NormalPredictor()	基于统计的推荐系统预测打分，假定用户打分的分布是基于正态分布的
BaselineOnly	基于统计的基准预测线打分
knns.KNNBasic	基本的协同过滤算法
knns.KNNWithMeans	协同过滤算法的变种，考虑每个用户的平均评分
knns.KNNWithZScore	协同过滤算法的变种，考虑每个用户评分的归一化操作
knns.KNNBaseline	协同过滤算法的变种，考虑每个用户评分的基线
matrix_factorzation.SVD	SVD 矩阵分解算法
matrix_factorzation.SVDpp	SVD++ 矩阵分解算法
matrix_factorzation.NMF	一种非负矩阵分解的协同过滤算法
SlopeOne	SlopeOne 协同过滤算法

基准算法包含两个主要的算法NormalPredictor和BaselineOnly
Normal Perdictor 认为用户对物品的评分是服从正态分布的，从而可以根据已有的评分的均值和方差 预测当前用户对其他物品评分的分数。
Baseline算法的思想就是设立基线，并引入用户的偏差以及item的偏差

μ为所有用户对电影评分的均值
bui：待求的基线模型中用户u给物品i打分的预估值
bu：user偏差（如果用户比较苛刻，打分都相对偏低， 则bu<0；反之，bu>0）；
bi为item偏差，反映商品受欢迎程度

from surprise import Dataset
from surprise import Reader
from surprise import BaselineOnly, KNNBasic
from surprise import accuracy
from surprise.model_selection import KFold
# 数据读取
reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)
data = Dataset.load_from_file('./ratings.csv', reader=reader)
train_set = data.build_full_trainset()

# Baseline算法，使用ALS进行优化
bsl_options = {'method': 'als','n_epochs': 5,'reg_u': 12,'reg_i': 5}
algo = BaselineOnly(bsl_options=bsl_options)
# 定义K折交叉验证迭代器，K=3
kf = KFold(n_splits=3)
for trainset, testset in kf.split(data):
    algo.fit(trainset)
    predictions = algo.test(testset)
    accuracy.rmse(predictions, verbose=True)
uid = str(196)
iid = str(302)
pred = algo.predict(uid, iid, r_ui=4, verbose=True)

# Baseline算法，使用SGD进行优化
bsl_options = {'method': 'sgd','n_epochs': 5}
algo = BaselineOnly(bsl_options=bsl_options)
# 定义K折交叉验证迭代器，K=3
kf = KFold(n_splits=3)
for trainset, testset in kf.split(data):
    algo.fit(trainset)
    predictions = algo.test(testset)
    accuracy.rmse(predictions, verbose=True)
uid = str(196)
iid = str(302)
pred = algo.predict(uid, iid, r_ui=4, verbose=True)

# NormalPredictor进行求解
algo = NormalPredictor()
# 定义K折交叉验证迭代器，K=3
kf = KFold(n_splits=3)
for trainset, testset in kf.split(data):
    algo.fit(trainset)
    predictions = algo.test(testset)
    accuracy.rmse(predictions, verbose=True)
uid = str(196)
iid = str(302)
pred = algo.predict(uid, iid, r_ui=4, verbose=True)

K折交叉验证
将训练集数据划分为K份，使用其中的K-1份作为训练集，剩余一份作为测试集
K次误差的平均值作为泛化误差
所有数据都做过训练和测试，更好的利用数据
K越大，平均误差作为泛化误差的结果就越可靠，但花费的时间也时间也越长


```

SlopeOne算法

```
SlopeOne算法：
由 Daniel Lemire在 2005 年提出的一个Item-Based 的协同过滤推荐算法
最大优点在于算法很简单, 易于实现, 效率高且推荐准确度较高。
C对商品2的评分=4-((5-3)+(4-3))/2=2.5

SlopeOne算法：
Step1，计算Item之间的评分差的均值，记为评分偏差（两个item都评分过的用户）

Step2，根据Item间的评分偏差和用户的历史评分，预测用户对未评分的item的评分

Step3，将预测评分排序，取topN对应的item推荐给用户

SlopeOne算法：
Step1，计算Item之间的评分差的均值
b与a：((3.5-5)+(5-2)+(3.5-4.5))/3=0.5/3
c与a：((4-2)+(1-4.5))/2=-1.5/2
d与a：((2-2)+(4-4.5))/2=-0.5/2
c与b：((4-5)+(1-3.5))/2=-3.5/2
d与b：((2-5)+(4-3.5))/2=-2.5/2
d与c：((2-4)+(4-1))/2=1/2

SlopeOne算法：
Step2，预测用户A对商品c和d的评分
a对c评分=((-0.75+5)+(-1.75+3.5))/2=3
a对d评分=((-0.25+5)+(-1.25+3.5))/2=3.5
Step3，将预测评分排序，推荐给用户
推荐顺序为{d, c}

Daniel Lemire and Anna Maclachlan. Slope one predictors for online rating-based collaborative filtering. 2007. http://arxiv.org/abs/cs/0702144.

加权算法 Weighted Slope One
如果有100个用户对Item1和Item2都打过分, 有1000个用户对Item3和Item2也打过分，显然这两个rating差的权重是不一样的，因此计算方法为：
(100*(Rating 1 to 2) + 1000(Rating 3 to 2)) / (100 + 1000)

SlopeOne算法的特点：
适用于item更新不频繁，数量相对较稳定
item数<<user数
算法简单，易于实现，执行效率高
依赖用户行为，存在冷启动问题和稀疏性问题

# 使用SlopeOne算法
algo = SlopeOne()
algo.fit(train_set)
# 对指定用户和商品进行评分预测
uid = str(196) 
iid = str(302) 
pred = algo.predict(uid, iid, r_ui=4, verbose=True)

# 相似度计算，使用皮尔逊相似度计算法，使用ItemCF的相似度计算
sim_options = {'name': 'pearson_baseline', 'user_based': False}
# 使用KNNBaseline算法，一种CF算法
algo = KNNBaseline(sim_options=sim_options)
algo.fit(train_set)
#获得电影名称信息数据
rid_to_name, name_to_rid = read_item_names()
#获得Toy Story电影的电影ID
toy_story_raw_id = name_to_rid['Toy Story (1995)']
print(toy_story_raw_id)
#通过Toy Story电影的电影ID获取该电影的推荐内部id
toy_story_inner_id = algo.trainset.to_inner_iid(toy_story_raw_id)
print(toy_story_inner_id)

```

对MovieLens进行推荐

```
```


# Project & Paper相关
MNIST的十种解法

```
算法	工具
Logistic Regression	from sklearn.linear_model import LogisticRegression
CART，ID3（决策树）	from sklearn.tree import DecisionTreeClassifier
LDA	from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  
朴素贝叶斯	from sklearn.naive_bayes import BernoulliNB
SVM	from sklearn import svm
KNN	from sklearn.neighbors import KNeighborsClassifier
Adaboost	from sklearn.ensemble import  AdaBoostClassifier
XGBoost	from xgboost import XGBClassifier
TPOT	from tpot import TPOTClassifier
keras	import keras

pip国内的镜像：
阿里云 http://mirrors.aliyun.com/pypi/simple/
中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/
豆瓣(douban) http://pypi.douban.com/simple/
清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/
中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/

使用方法：
pip install tensorflow -i https://pypi.tuna.tsinghua.edu.cn/simple

TPOT https://github.com/EpistasisLab/tpot （6.2K）
TPOT可以解决：特征选择，模型选择，但不包括数据清洗
处理小规模数据非常快，大规模数据非常慢。可以先抽样小部分，使用TPOT

TPOT：
目前只能做有监督学习
支持的分类器主要有贝叶斯、决策树、集成树、SVM、KNN、线性模型、xgboost
支持的回归器主要有决策树、集成树、线性模型、xgboost
数据预处理：二值化、聚类、降维、标准化、正则化等
特征选择：基于树模型、基于方差、基于F-值的百分比
可以通过export()方法把训练过程导出为形式为sklearn pipeline的.py文件

# 使用TPOP对iris进行分类
from tpot import TPOTClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),
    iris.target.astype(np.float64), train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_iris_pipeline.py')

# 使用TPOT自动机器学习工具对MNIST进行分类
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
import numpy as np

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data.astype(np.float64),
    iris.target.astype(np.float64), train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_iris_pipeline.py')

算法	准确率	备注
Logistic Regression	96.00%	
CART，ID3（决策树）	85.78%	
LDA	93.78%	
朴素贝叶斯	88.22%	BernoulliNB
SVM	98.67%	
KNN	97.56%	
Adaboost	96.89%	CART决策树作为弱分类器
XGBoost	94.89%	
TPOT	98.67%	Best pipeline: LogisticRegression(GaussianNB(PolynomialFeatures(VarianceThreshold(input_matrix, threshold=0.0005), degree=2, include_bias=False, interaction_only=False)), C=1.0, dual=True, penalty=l2)
keras	97.90%	LeNet-5网络

Mercari 是一款在日本很受欢迎的购物应用程序（日本跳蚤市场排名第一），之前开发自己的ML模型，在照片上传的用户界面上推荐 12 个主要品牌的品牌名称
虽然他们自己在 TensorFlow 上训练的模型达到了 75% 的精度，但是使用Google AutoML Vision 解决方案对图像进行分类，精度可以高达91.3%，提升了15%
原因是AutoML Vision 的高级模式因为拥有 50,000 张训练图像（用户的图像）


```

80%的时间都花在数据清洗上

```

0	1	2	3	4	5	6	7	8	9
1.0	Micky Mouse	56.0	70kgs	72	69	71	-	-	-
2.0	Donald Duck	34.0	150lbs	-	-	-	85	84	76
3.0	Mini Mouse	16.0	Nan	-	-	-	65	69	72
4.0	Scrooge McDuck	36.3	78kgs	78	79	72	-	-	-
5.0	Pink Panther	54.0	90kgs	-	-	-	69	79	75
6.0	Huey McDuck	52.0	85kgs	-	-	-	68	75	72
7.0	Dewey McDuck	19.0	56kgs	-	-	-	71	78	75
8.0	Scoopy Doo	32.0	78kgs	78	76	75	-	-	-
NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
9.0	Huey McDuck	52.0	188lbs	-	-	-	68	75	72
10.0	Louie McDuck	12.0	40kgs	-	-	-	92	95	87

ID	Name	Age	Weight	m1	m2	m3	f1	f2	f3
1.0	Micky Mouse	56.0	70kgs	72	69	71	-	-	-
2.0	Donald Duck	34.0	150lbs	-	-	-	85	84	76
3.0	Mini Mouse	16.0	Nan	-	-	-	65	69	72
4.0	Scrooge McDuck	36.3	78kgs	78	79	72	-	-	-
5.0	Pink Panther	54.0	90kgs	-	-	-	69	79	75
6.0	Huey McDuck	52.0	85kgs	-	-	-	68	75	72
7.0	Dewey McDuck	19.0	56kgs	-	-	-	71	78	75
8.0	Scoopy Doo	32.0	78kgs	78	76	75	-	-	-
NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
9.0	Huey McDuck	52.0	188lbs	-	-	-	68	75	72
10.0	Louie McDuck	12.0	40kgs	-	-	-	92	95	87

```

数据清洗的准则

```
好的数据质量，应该满足“完全合一”
完整性：数据是否存在空值，字段是否完善，是否有漏掉
全面性：观察某一列的全部数值及特征值，是否存在单位、字段名与数值不匹配
合法性：数据的类型、内容、大小的合法性。
唯一性：数据是否存在重复记录

问题1：缺失值
在数据中有些年龄、体重数值是缺失的，这往往是因为数据量较大，在过程中，有些数值没有采集到。通常我们可以采用以下三种方法：
删除：删除数据缺失的记录；
均值：使用当前列的均值；
高频：使用当前列出现频率最高的数据。
比如我们想对df['Age']中缺失的数值用平均年龄进行填充，可以这样写：
df['Age'].fillna(df['Age'].mean(), inplace=True)

问题2：空行
我们发现数据中有一个空行，除了 index 之外，全部的值都是 NaN。Pandas 的 read_csv() 并没有可选参数来忽略空行，这样，我们就需要在数据被读入之后再使用 dropna() 进行处理，删除空行。
# 删除全空的行
df.dropna(how='all',inplace=True) 

问题：列数据的单位不统一
weight列的数值，有的单位是千克（kgs），有的单位是磅（lbs）。
这里统一将磅（lbs）转化为千克（kgs）：

# 获取 weight 数据列中单位为 lbs 的数据
rows_with_lbs = df['weight'].str.contains('lbs').fillna(False)
# 将 lbs转换为 kgs, 2.2lbs=1kgs
for i,lbs_row in df[rows_with_lbs].iterrows():
	# 截取从头开始到倒数第三个字符之前，即去掉lbs。
	weight = int(float(lbs_row['weight'][:-3])/2.2)
	df.at[i,'weight'] = '{}kgs'.format(weight) 
	
问题：非ASCII字符
如果文本中存在非 ASCII 的字符。我们还需要进行删除或者替换。
这里使用对非ASCII字符进行删除方式

# 删除非 ASCII 字符
df['name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)

问题：一列有多个参数（可选）
可以将Name分成last name + first name
也可以进行保留。

# 切分名字，删除源数据列
df[['first_name','last_name']] = df['name'].str.split(expand=True)
df.drop('name', axis=1, inplace=True)

默认采用的空格进行分割，相当于df['name'].str.split(' ', expand=True)

问题2：重复数据
我们校验一下数据中是否存在重复记录。如果存在重复记录，就使用 Pandas 提供的 drop_duplicates() 来删除重复数据。

# 删除重复数据行
df.drop_duplicates(['first_name','last_name'],inplace=True)

FirstName	LastName	Age	Weight	m1	m2	m3	f1	f2	f3
Micky	Mous	56.0	70kgs	72	69	71			
Donald	Duck	34.0	68kgs				85	84	76
Mini	Mouse	16.0	71kgs				65	69	72
Scrooge	McDuck	36.3	78kgs	78	79	72			
Pink	Panther	54.0	90kgs				69	79	75
Huey	McDuck	52.0	85kgs				68	75	72
Dewey	McDuck	19.0	56kgs				71	78	75
Scoopy	Doo	32.0	78kgs	78	76	75			
Huey	McDuck	52.0	85kgs				68	75	72
Louie	McDuck	12.0	40kgs				92	95	87
```

如何使用Python对数据进行清洗

```
```

Project：对Titanic数据进行数据清洗

```
泰坦尼克海难是著名的十大灾难之一，究竟多少人遇难，各方统计的结果不一。现在我们可以得到部分的数据：
GitHub地址：https://github.com/cystanford/Titanic_Data
数据集格式为csv，一共有两个文件：
train.csv：训练集，包含特征信息，分类结果（存活与否）；
test.csv：测试集，只包含特征信息

字段	描述
PassengerId	乘客编号
Survived	是否幸存
Pclas，有些特征标注的英文	船票等级
Name	乘客姓名
Sex	乘客性别
SibSp	亲戚数量（兄妹、配偶数）
Parch	亲戚数量（父母、子女数）
Ticket	船票号码
Fare	船票价格
Cabin	船舱
Embarked	登陆港口

我们需要对数据进行预处理：
数据探索
缺失值
数值为英文的情况
特征选择

print(train_data.info())

数据探索：
print(train_data.describe())
展示了数据的概要：
count 数量
mean 平均值
std 标准差
min 最小值

25% 第一四分位数 (Q1)
50% 中位数
75% 第三四分位数 (Q3)
max 最大值

数据探索：
print(train_data.describe(include=['O']))
查看离散数据类型的分布

数据探索：
# 查看前5条数据
print(train_data.head())
# 查看后5条数据
print(train_data.tail())

缺失值处理：
# 使用平均年龄来填充年龄中的nan值
train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)
test_data['Age'].fillna(test_data['Age'].mean(),inplace=True)
# 使用票价的均值填充票价中的nan值
train_data['Fare'].fillna(train_data['Fare'].mean(), inplace=True)
test_data['Fare'].fillna(test_data['Fare'].mean(),inplace=True)

缺失值处理：
print(train_data['Embarked'].value_counts())
# 使用登录最多的港口来填充登录港口的nan值
train_data['Embarked'].fillna('S', inplace=True)
test_data['Embarked'].fillna('S',inplace=True)

特征选择：
# 特征选择
features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']
train_features = train_data[features]
train_labels = train_data['Survived']
test_features = test_data[features]
dvec=DictVectorizer(sparse=False)
train_features=dvec.fit_transform(train_features.to_dict(orient='record'))
print(dvec.feature_names_)
```

怎样阅读更高效

```
```

课程相关Paper

```
Paper Reading是个痛苦&收获的过程
文字太多，代码就一句话的事
经典&有影响力的论文都是英语，阅读障碍
原理看不懂，遇到不会的地方就放弃了
论文的价值
严谨性，一篇论文的产生可能需要3-6个月的周期
完整性，从问题背景，他人方案，解决方案，实验结果，后续工作
启发性，在阅读过程中会带有自己的思考，新idea的产生

阅读论文方法
先看Survey（对于新领域）
对研究的内容有个深入广泛的了解，知道最新的研究进展（state of the art），奠基性的成果有哪些（经典paper，数据集benchmark），还有哪些未来值得研究的地方，哪些技术比较成熟可以直接使用
关注重要的国际会议
相比来说文章比期刊更短，时效性更强。应用性更强
作者的代码及Blog
国外很多论文都提供源代码，如果没有在网站上，可以写email给作者
很多作者是大学教授，或者在读博士生，会有个人主页，关于他论文list及研究方向

阅读论文方法
粗读
一次性读完，即使有很多不懂的地方
如果一遍没读懂，可以多读几遍，不要放弃。
如果是经典的论文，往往有其他人的解读，可以作为理解的补充
粗读：不需要一次性都读明白，可以多读几遍
精读
带着想法读论文，可能会产生新的idea，记录下来
对论文自己做个summary或者思维导图，转化为自己的知识，语言体系
精读：针对重要的部分，反复读直到理解为止
跳读
找到一篇文章中找到重要的部分，进行精读
跳读：带着问题去思考，在文章中寻找自己的答案

如何下载论文
学校图书馆内网

百度学术/Google  Scholar

arXiv.org
一个收录科学文献预印本的在线数据库，每个人都可以免费地访问全文数据
超过50万篇文章，每个月增长5000篇
包含：数学，物理，计算机，非线性科学，定量生物学，定量财务以及统计学等

今天课程的相关Paper：
1、Large-scale Parallel Collaborative Filtering for the Netflix Prize, 2008
http://machinelearning202.pbworks.com/w/file/fetch/60922097/netflix_aaim08(submitted).pdf
als-wr论文

2、Matrix factorization techniques for recommender systems, [J]. Computer 2009
http://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Recommender-Systems.pdf
矩阵分解模型

今天课程的相关Paper：
3、Slope One Predictors for Online Rating-Based Collaborative Filtering, Computer Science, 2007
https://arxiv.org/abs/cs/0702144
提出Slope One协同过滤算法，易于实现和维护，论文中提到了3种算法：Slope One,  Weighted Slope One, Bi-Polar Slope One

4、Factor in the Neighbors Scalable and Accurate Collaborative Filtering, Acm Transactions on Knowledge Discovery from Data, 2010
https://www.lri.fr/~sebag/COURS/BleiNgJordan2003.pdf
Surprise中的BaselineOnly算法基于这篇论文

1、The Youtube video recommendation system, Acm Conference on Recommender Systems 2010
http://www.inf.unibz.it/~ricci/ISR/papers/p293-davidson.pdf
最早的一篇YouTube的推荐系统的文章，2010年整个系统比较简略，值得一读

2、Mastering the game of Go without human knowledge，2017
https://www.gwern.net/docs/rl/2017-silver.pdf （DeepMind发表在Nature上的关于AlphaGO的论文）

3、Practical Lessons from Predicting Clicks on Ads at Facebook,2014 
http://quinonero.net/Publications/predicting-clicks-facebook.pdf （Facebook经典CTR预估）

4、Factorization Machines, 2010
https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf （FM论文）

5、Field-aware Factorization Machines for CTR Prediction，2016
https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf （关于FFM算法的论文）

6、DeepFM: A Factorization-Machine based Neural Network for CTR Prediction，2017
https://arxiv.org/abs/1703.04247 （DeepFM论文）

7、Wide & Deep Learning for Recommender Systems，2016
https://arxiv.org/abs/1606.07792 （Wide&Deep论文）

8、DeepWalk Online Learning of Social Representations, 2014 KDD
https://classes.cs.uoregon.edu/17S/cis607bddl/papers/Perozzi.pdf （关于DeepWalk算法论文）

9、Node2Vec: Scalable Feature Learning for Networks, 2016 KDD
https://arxiv.org/abs/1607.00653 （关于Node2Vec论文）

10、Semi-Supervised Classification with Graph Convolutional Networks, 2017
https://arxiv.org/abs/1609.02907 （关于GCN图卷积神经网络）

11、Learning to rank for information retrieval
http://exp.newsmth.net/attachment/0dd952f21955fdfede030d8f5e9a9360 （关于Learning to rank）

12、Real-time Personalization using Embeddings for Search Ranking at Airbnb,2018
https://www.kdd.org/kdd2018/accepted-papers/view/real-time-personalization-using-embeddings-for-search-ranking-at-airbnb （关于AirBnb个性化推荐论文）

13、Deep Interest Network for Click-Through Rate Prediction,2018 
https://arxiv.org/abs/1706.06978 （淘宝关于CTR预估的论文）

14、Deep Interest Evolution Network for Click-Through Rate Prediction,2018
https://arxiv.org/abs/1809.03672 （淘宝关于CTR预估的论文）

15、Deep Session Interest Network for Click-Through Rate Prediction,  IJCAI 2019
https://arxiv.org/abs/1905.06482（淘宝关于CTR预估的论文）

16、AutoML A Survey of the State of the Art, KDD 2019
https://arxiv.org/abs/1908.00709（AutoML综述）


```

ALS-WR Paper Reading

```
```
