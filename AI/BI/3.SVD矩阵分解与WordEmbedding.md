# SVD 矩阵分解与 WordEmbedding

```
TIPS：思考工程中的解决方案
很多事物之间都有相关性
实践出来，你会收获更多
简单的原理可以解决大部分问题，个性化的部分需要更复杂的运算
工程中需要考虑泛化能力，使用更多的特征，更高维度的计算框架
本地设备受到局限时，如何使用云端工具加速运行


聚类是无监督的学习，具体含义需要我们指定

Thinking：聚类的K取多少适合？


K-Means 手肘法：
# 统计不同K取值的误差平方和
sse = []
for k in range(1, 11):
	# kmeans算法
	kmeans = KMeans(n_clusters=k)
	kmeans.fit(train_x)
	# 计算inertia簇内误差平方和，sum of the squared errors
	sse.append(kmeans.inertia_)
x = range(1, 11)
plt.xlabel('K')
plt.ylabel('SSE')
plt.plot(x, sse, 'o-')
plt.show()

K-Means 在图像分割中的应用：
# 用K-Means对图像进行2聚类
kmeans =KMeans(n_clusters=2)
kmeans.fit(img)
label = kmeans.predict(img)
# 将图像聚类结果，转化成图像尺寸的矩阵
label = label.reshape([width, height])
# 创建个新图像pic_mark，用来保存图像聚类的结果，并设置不同的灰度值
pic_mark = image.new("L", (width, height))
for x in range(width):
    for y in range(height):
        # 根据类别设置图像灰度, 类别0 灰度值为255， 类别1 灰度值为127
        pic_mark.putpixel((x, y), int(256/(label[x][y]+1))-1)
        
        
K-Means图像分割的不足：
按照图像的灰度值定义距离，对图像的文理，内容缺乏理解

采用深度学习模型：
100层 Tiramisu进行图像分割
（全卷积 DenseNets)

```


# SVD矩阵分解

矩阵分解方法

 ```
协同过滤是推荐系统的主流思想之一：
基于邻域的协同过滤
UserCF
ItemCF
基于模型的协同过滤
隐语义模型（LFM, Latent Factor Model）
矩阵分解（MF）
LDA, LSA, pLSA
基于贝叶斯网络
基于SVM


矩阵分解：
将矩阵拆解为多个矩阵的乘积
在推荐系统中，协同过滤=>基于模型=>隐语义模型=>SVD
矩阵分解方法：
EVD（特征值分解）
SVD（奇异值分解）
求解近似矩阵分解的最优化问题
ALS
SGD

矩阵的特征分解：
特征分解，是将矩阵分解为特征值和特征向量表示的矩阵之积的方法，也称为谱分解
N 维非零向量 v 是 N×N 的矩阵 A 的特征向量，当且仅当下式成立

           为特征值（标量），v为特征值      对应的特征向量。特征向量被施以线性变换 A 只会使向量伸长或缩短，而方向保持不变

 ```

普通矩阵的矩阵分解

```

import numpy as np
A = np.array([[5,3],
	        [1,1]])
lamda, U = np.linalg.eig(A)
print('矩阵A: ')
print(A)
print('特征值: ',lamda)
print('特征向量')
print(U)

```

对称矩阵的矩阵分解

```

矩阵分解中的问题
很多矩阵都是非对称的
矩阵A不是方阵，即维度为m*n
我们可以将它转化为对称的方阵，因为：

```

奇异值分解SVD

```

我们可以得到为奇异值分解

P为左奇异矩阵，m*m维
Q为右奇异矩阵，n*n维
Λ对角线上的非零元素为特征值λ1, λ2, ... , λk
在推荐系统中
左奇异矩阵：User矩阵
右奇异矩阵：Item矩阵

from scipy.linalg import svd
import numpy as np
from scipy.linalg import svd
A = np.array([[1,2],
	    [1,1],
	    [0,0]])
p,s,q = svd(A,full_matrices=False)
print('P=', p)
print('S=', s)
print('Q=', q)

如何理解SVD的作用
对于特征值矩阵，我们如果只包括某部分特征值，结果会怎样？
矩阵A：大小为1440*1080的图片
Step1，将图片转换为矩阵
Step2，对矩阵进行奇异值分解，得到p,s,q
Step3，包括特征值矩阵中的K个最大特征值，其余特征值设置为0
Step4，通过p,s',q得到新的矩阵A'，对比A'与A的差别

如何理解矩阵分解
from scipy.linalg import svd
import matplotlib.pyplot as plt
# 取前k个特征，对图像进行还原
def get_image_feature(s, k):
	# 对于S，只保留前K个特征值
	s_temp = np.zeros(s.shape[0])
	s_temp[0:k] = s[0:k]
	s = s_temp * np.identity(s.shape[0])
	# 用新的s_temp，以及p,q重构A
	temp = np.dot(p,s)
	temp = np.dot(temp,q)
	plt.imshow(temp, cmap=plt.cm.gray, interpolation='nearest')
	plt.show()

# 加载256色图片
image = Image.open('./256.bmp') 
A = np.array(image)
# 显示原图像
plt.imshow(A, cmap=plt.cm.gray, interpolation='nearest')
plt.show()
# 对图像矩阵A进行奇异值分解，得到p,s,q
p,s,q = svd(A, full_matrices=False)
# 取前k个特征，对图像进行还原
get_image_feature(s, 5)
get_image_feature(s, 50)
get_image_feature(s, 500)
```

使用SVD进行降维

```
```

传统SVD在推荐系统中的应用

```

传统SVD在推荐系统中的应用
我们可以通过k来对矩阵降维
第i个用户对第j个物品的评分
完整的SVD，可以将M无损的分解成为三个矩阵
为了简化矩阵分解，我们可以使用k，远小于min(m,n)，对矩阵M近似还原

传统SVD在使用上的局限：
SVD分解要求矩阵是稠密的 => 矩阵中的元素不能有缺失
所以，类似于数据清洗，我们需要先对矩阵中的缺失元素进行补全
先有鸡，还是先有蛋。实际上传统SVD更适合做降维


```

SVD算法家族：FunkSVD, BiasSVD, SVD++

```

FunkSVD算法思想：
我们需要设置k，来对矩阵近似求解
矩阵补全以后，再预测，实际上噪音大。矩阵分解之后的还原，只需要关注与原来矩阵中有值的位置进行对比即可，不需要对所有元素进行对比

存在的问题：
矩阵往往是稀疏的，大量缺失值 => 计算量大
填充方式简单粗暴 => 噪音大

解决思路：
避开稀疏问题，而且只用两个矩阵进行相乘
损失函数=P和Q矩阵乘积得到的评分，与实际用户评分之差
让损失函数最小化 => 最优化问题

Step1，通过梯度下降法，求解P和Q使得损失函数最小化
Step2，通过P和Q将矩阵补全
Step3，针对某个用户i，查找之前值缺失的位置，按照补全值从大到小进行推荐

BiasSVD算法原理：
用户有自己的偏好(Bias)，比如乐观的用户打分偏高
商品也有自己的偏好(Bias)，比如质量好的商品，打分偏高
将与个性化无关的部分，设置为偏好(Bias)部分

优化目标函数
在迭代过程中，bi,bj的初始值可以设置为0向量，然后进行迭代：

最终得到P和Q



SVD++算法原理：
在BiasSVD算法基础上进行了改进，考虑用户的隐式反馈
隐式反馈：没有具体的评分，但可能有点击，浏览等行为
对于某一个用户i，假设他的隐式反馈item集合为I(i)
用户i对商品j对应的隐式反馈修正值为
用户i所有的隐式反馈修正值之和为
优化目标函数：

在考虑用户隐式反馈的情况下，最终得到P和Q

biasSVD算法
使用Surprise工具中的SVD
参数:
n_factors: k值，默认为100
n_epochs：迭代次数，默认为20
biased：是否使用biasSVD，默认为True
verbose:输出当前epoch，默认为False
reg_all:所有正则化项的统一参数，默认为0.02
reg_bu：bu的正则化参数，reg_bi：bi的正则化参数
reg_pu：pu的正则化参数，reg_qi：qi的正则化参数

funkSVD算法
使用Surprise工具中的SVD
参数:
n_factors: k值，默认为100
n_epochs：迭代次数，默认为20
biased：是否使用biasSVD，设置为False
verbose:输出当前epoch，默认为False
reg_all:所有正则化项的统一参数，默认为0.02
reg_bu：bu的正则化参数，reg_bi：bi的正则化参数
reg_pu：pu的正则化参数，reg_qi：qi的正则化参数

SVD++算法
使用Surprise工具中的SVDpp
参数:
n_factors: k值，默认为20
n_epochs：迭代次数，默认为20
verbose:输出当前epoch，默认为False
reg_all:所有正则化项的统一参数，默认为0.02
reg_bu：bu的正则化参数，reg_bi：bi的正则化参数
reg_pu：pu的正则化参数，reg_qi：qi的正则化参数
reg_yj：yj的正则化参数


```

Surprise工具中的SVD

```
```

Google Colab编辑器

```
```

对MovieLens进行推荐

```

数据集：MovieLens
下载地址：https://www.kaggle.com/jneupane12/movielens/download
主要使用的文件：ratings.csv
格式：userId, movieId, rating, timestamp
记录了用户在某个时间对某个movieId的打分情况
我们需要补全评分矩阵，然后对指定用户，比如userID为1-5进行预测

# 使用funkSVD
algo = SVD(biased=False)
# 定义K折交叉验证迭代器，K=3
kf = KFold(n_splits=3)
for trainset, testset in kf.split(data):
    algo.fit(trainset)
    predictions = algo.test(testset)
    accuracy.rmse(predictions, verbose=True)
uid = str(196)
iid = str(302)
# 输出uid对iid的预测结果
pred = algo.predict(uid, iid, r_ui=4, verbose=True)

https://colab.research.google.com
Google的云端Jupyter编辑器，计算效率高
存储在Google 云端硬盘，可以共享
支持GPU/TPU加速（Tesla K80 or Tesla T4）
可以使用Keras、Tensorflow和Pytorch
Colab最多12个小时环境就会重置

Google  Colab使用：
上传文件到Google云盘
挂在Google云硬盘
from google.colab import drive
drive.mount('/content/drive')
更改运行目录
import os
os.chdir("/content/drive/My Drive/Colab Notebooks/")
可以选择GPU或TPU进行加速

# 创建序贯模型
model = Sequential()
# 第一层卷积层：6个卷积核，大小为5∗5, relu激活函数
model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))
# 第二层池化层：最大池化
model.add(MaxPooling2D(pool_size=(2, 2)))
……
# 传入训练数据进行训练
model.fit(train_x, train_y, batch_size=128, epochs=2, verbose=1, validation_data=(test_x, test_y))
# 对结果进行评估
score = model.evaluate(test_x, test_y)
print('误差:%0.4lf' %score[0])
print('准确率:', score[1])

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.models import Sequential
……
# 传入训练数据进行训练
model.fit(train_x, train_y, batch_size=128, epochs=2, verbose=1, validation_data=(test_x, test_y))
# 对结果进行评估
score = model.evaluate(test_x, test_y)
print('误差:%0.4lf' %score[0])
print('准确率:', score[1])


from deepctr.models import DeepFM
from deepctr.inputs import SparseFeat,get_feature_names
data = pd.read_csv("ratings.csv")
sparse_features = ["userId", "movieId", 'timestamp']
target = ['rating']
# 对特征标签进行编码
for feature in sparse_features:
    lbe = LabelEncoder()
    data[feature] = lbe.fit_transform(data[feature])
# 计算每个特征中的 不同特征值的个数
fixlen_feature_columns = [SparseFeat(feature, data[feature].nunique()) for feature in sparse_features]
linear_feature_columns = fixlen_feature_columns
dnn_feature_columns = fixlen_feature_columns
feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

# 将数据集切分成训练集和测试集
train, test = train_test_split(data, test_size=0.2)
train_model_input = {name:train[name].values for name in feature_names}
test_model_input = {name:test[name].values for name in feature_names}
# 使用DeepFM进行训练
model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')
model.compile("adam", "mse", metrics=['mse'], )
history = model.fit(train_model_input, train[target].values, batch_size=256, epochs=1, verbose=True, validation_split=0.2, )
# 使用DeepFM进行预测
pred_ans = model.predict(test_model_input, batch_size=256)
# 输出RMSE或MSE
mse = round(mean_squared_error(test[target].values, pred_ans), 4)
rmse = mse ** 0.5
print("test RMSE", rmse)

数据集：MovieLens_Sample
数据集存在大量离散特征，比如性别，年龄，邮编等
MF对评分矩阵分解成User矩阵和Item矩阵，可以处理评分预测问题，无法利用其它特征
如果我们想要利用各种分类特征，对离散特征进行oneshot编码，就会存在矩阵非常稀疏的情况，此时需要采用FM
FM，Factorization Machine，因子分解机，2010年提出，在CTR预估和推荐领域广泛使用
FM矩阵将用户和物品都进行了one-hot编码作为了评分预测的特征，特征维度非常巨大而且稀疏

data = pd.read_csv("movielens_sample.txt")
sparse_features = ["movie_id", "user_id", "gender", "age", "occupation", "zip"]
target = ['rating']
……
train, test = train_test_split(data, test_size=0.2)
train_model_input = {name:train[name].values for name in feature_names}
test_model_input = {name:test[name].values for name in feature_names}
# 使用DeepFM进行训练
model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')
model.compile("adam", "mse", metrics=['mse'], )
history = model.fit(train_model_input, train[target].values,batch_size=256, epochs=1, verbose=True, validation_split=0.2, )
# 使用DeepFM进行预测
pred_ans = model.predict(test_model_input, batch_size=256)

将数据集加载到Colab
Github数据集
!git clone https://github.com/JameyWoo/myDataSet.git
!pip install git+https://github.com/shenweichen/DeepCTR.git
Kaggle数据集
Step1，生成Kaggle API
Step2，找到自己的username和token
Step3，创建kaggle文件夹，配置kaggle.json
Step4，使用kaggle api下载数据
Step5，解压数据并使用


Colab + Kaggle实战
数据集（信用卡违约率检测）
https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset
Kaggle数据加载

# 安装kaggle
!pip install -U -q kaggle
# 创建kaggle.json
!mkdir -p ~/.kaggle
!echo '{"username":"cystanford","key":"7b91422ebb9b6eef133f27f95362456b"}' > ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json

# 设置数据集文件夹
!mkdir -p credit_data
# 下载数据
!kaggle datasets download -d uciml/default-of-credit-card-clients-dataset -p credit_data
# 解压数据
!unzip  /content/credit_data/default-of-credit-card-clients-dataset.zip

数据集（信用卡违约率检测）
https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset
对信用卡使用数据进行建模，预测用户是否下个月产生违约 => 分类问题
机器学习算法有很多，比如SVM、决策树、随机森林和KNN => 该使用哪个模型
使用GridSearchCV工具，帮我们找到每个分类器的最优参数和最优分数，最终找到最适合数据集的分类器和此分类器的参数

# 数据加载
data = data = pd.read_csv('./UCI_Credit_Card.csv')
# 数据探索
print(data.shape) # 查看数据集大小
print(data.describe()) # 数据集概览
# 查看下一个月违约率的情况
next_month = data['default.payment.next.month'].value_counts()
print(next_month)
df = pd.DataFrame({'default.payment.next.month': next_month.index,'values': next_month.values})

plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签
plt.figure(figsize = (6,6))
plt.title('信用卡违约率客户\n (违约：1，守约：0)')
sns.set_color_codes("pastel")
sns.barplot(x = 'default.payment.next.month', y="values", data=df)
locs, labels = plt.xticks()
plt.show()
# 特征选择，去掉ID字段、最后一个结果字段即可
data.drop(['ID'], inplace=True, axis =1) #ID这个字段没有用
target = data['default.payment.next.month'].values
columns = data.columns.tolist()
columns.remove('default.payment.next.month')
features = data[columns].values

# 30%作为测试集，其余作为训练集
train_x, test_x, train_y, test_y = train_test_split(features, target, test_size=0.30, stratify = target, random_state = 1)
# 构造各种分类器
classifiers = [
    SVC(random_state = 1, kernel = 'rbf'),    
    DecisionTreeClassifier(random_state = 1, criterion = 'gini'),
    RandomForestClassifier(random_state = 1, criterion = 'gini'),
    KNeighborsClassifier(metric = 'minkowski'),
]


# 分类器名称
classifier_names = [
            'svc', 
            'decisiontreeclassifier',
            'randomforestclassifier',
            'kneighborsclassifier',
]
# 分类器参数
classifier_param_grid = [
            {'svc__C':[1], 'svc__gamma':[0.01]},
            {'decisiontreeclassifier__max_depth':[6,9,11]},
            {'randomforestclassifier__n_estimators':[3,5,6]} ,
            {'kneighborsclassifier__n_neighbors':[4,6,8]},
]

# 对具体的分类器进行GridSearchCV参数调优
def GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, param_grid, score = 'accuracy'):
    response = {}
    gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, scoring = score)
    # 寻找最优的参数 和最优的准确率分数
    search = gridsearch.fit(train_x, train_y)
    print("GridSearch最优参数：", search.best_params_)
    print("GridSearch最优分数： %0.4lf" %search.best_score_)
    predict_y = gridsearch.predict(test_x)
    print("准确率 %0.4lf" %accuracy_score(test_y, predict_y))
    response['predict_y'] = predict_y
    response['accuracy_score'] = accuracy_score(test_y,predict_y)
    return response
 
 # 调用GridSearchCV_work，寻找最优分类器及参数
for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):
    pipeline = Pipeline([
            ('scaler', StandardScaler()),
            (model_name, model)
    ])
    result = GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, model_param_grid , score = 'accuracy')
    
    estimator	想要采用的分类器，比如随机森林、决策树、SVM、KNN等
param_grid	想要优化的参数及取值，输入的是字典或者列表的形式
cv	"交叉验证的折数，默认为None，代表使用三折交叉验证。
也可以为整数，代表的是交叉验证的折数"
scoring	"准确度的评价标准，默认为None，也就是需要使用score函数。
可以设置具体的评价标准，比如accuray，f1等"


Pipeline使用：
使用Pipeline管道机制进行流水线作业。因为在做机器学习之前，还需要一些准备过程，比如数据规范化，或者数据降维等。
GridSearchCV + Pipeline：
对于每一个模型，都进行Pipeline流水作业，找到最适合的参数
for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):
    pipeline = Pipeline([
            ('scaler', StandardScaler()),
            (model_name, model)
    ])
    result = GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, model_param_grid , score = 'accuracy')
 
 
```


# Word Embedding

基于内容的推荐

```

基于内容的推荐：
依赖性低，不需要动态的用户行为，只要有内容就可以进行推荐
系统不同阶段都可以应用
系统冷启动，内容是任何系统天生的属性，可以从中挖掘到特征，实现推荐系统的冷启动。一个复杂的推荐系统是从基于内容的推荐成长起来的
商品冷启动，不论什么阶段，总会有新的物品加入，这时只要有内容信息，就可以帮它进行推荐

物品表示 Item Representation：
为每个item抽取出features
特征学习Profile Learning：
利用一个用户过去喜欢（不喜欢）的item的特征数据，来学习该用户的喜好特征（profile）；
生成推荐列表Recommendation Generation：
通过用户profile与候选item的特征，推荐相关性最大的item。 

西雅图酒店数据集：
下载地址：https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Seattle_Hotels.csv
字段：name, address, desc
基于用户选择的酒店，推荐相似度高的Top10个其他酒店
方法：计算当前酒店特征向量与整个酒店特征矩阵的余弦相似度，取相似度最大的Top-k个

余弦相似度：
通过测量两个向量的夹角的余弦值来度量它们之间的相似性。
判断两个向量⼤致方向是否相同，方向相同时，余弦相似度为1；两个向量夹角为90°时，余弦相似度的值为0，方向完全相反时，余弦相似度的值为-1。
两个向量之间夹角的余弦值为[-1, 1]
给定属性向量A和B，A和B之间的夹角θ余弦值可以通过点积和向量长度计算得出

计算A和B的余弦相似度：
句子A：这个程序代码太乱，那个代码规范
句子B：这个程序代码不规范，那个更规范
Step1，分词
句子A：这个/程序/代码/太乱，那个/代码/规范
句子B：这个/程序/代码/不/规范，那个/更/规范
Step2，列出所有的词
这个，程序，代码，太乱，那个，规范，不，更
Step3，计算词频
句子A：这个1，程序1，代码2，太乱1，那个1，规范1，不0，更0
句子B：这个1，程序1，代码1，太乱0，那个1，规范2，不1，更1

计算A和B的余弦相似度：
Step4，计算词频向量的余弦相似度
句子A：（1，1，2，1，1，1，0，0）
句子B：（1，1，1，0，1，2，1，1）



结果接近1，说明句子A与句子B是相似的

什么是N-Gram（N元语法）：
基于一个假设：第n个词出现与前n-1个词相关，而与其他任何词不相关.
N=1时为unigram，N=2为bigram，N=3为trigram
N-Gram指的是给定一段文本，其中的N个item的序列
比如文本：A B C D E，对应的Bi-Gram为A B, B C, C D, D E
当一阶特征不够用时，可以用N-Gram做为新的特征。比如在处理文本特征时，一个关键词是一个特征，但有些情况不够用，需要提取更多的特征，采用N-Gram => 可以理解是相邻两个关键词的特征组合


plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
df = pd.read_csv('Seattle_Hotels.csv', encoding="latin-1")
# 得到酒店描述中n-gram特征中的TopK个
def get_top_n_words(corpus, n=1, k=None):
    # 统计ngram词频矩阵
    vec = CountVectorizer(ngram_range=(n, n), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    # 按照词频从大到小排序
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:k]
common_words = get_top_n_words(df['desc'], 1, 20)
df1 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
df1.groupby('desc').sum()['count'].sort_values().plot(kind='barh', title='去掉停用词后，酒店描述中的Top20单词')
plt.show()

# Bi-Gram
common_words = get_top_n_words(df['desc'], 2, 20)


# Tri-Gram
common_words = get_top_n_words(df['desc'], 3, 20)


def clean_text(text):
    # 全部小写
    text = text.lower()
    ……
    return text
df['desc_clean'] = df['desc'].apply(clean_text)
# 使用TF-IDF提取文本特征
tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')
tfidf_matrix = tf.fit_transform(df['desc_clean'])
print(tfidf_matrix)
print(tfidf_matrix.shape)
# 计算酒店之间的余弦相似度（线性核函数）
cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)
print(cosine_similarities)


# 基于相似度矩阵和指定的酒店name，推荐TOP10酒店
def recommendations(name, cosine_similarities = cosine_similarities):
    recommended_hotels = []
    # 找到想要查询酒店名称的idx
    idx = indices[indices == name].index[0]
    print('idx=', idx)
    # 对于idx酒店的余弦相似度向量按照从大到小进行排序
    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending = False)
    # 取相似度最大的前10个（除了自己以外）
    top_10_indexes = list(score_series.iloc[1:11].index)
    # 放到推荐列表中
    for i in top_10_indexes:
        recommended_hotels.append(list(df.index)[i])
    return recommended_hotels
print(recommendations('Hilton Seattle Airport & Conference Center'))
print(recommendations('The Bacon Mansion Bed and Breakfast'))

CountVectorizer：
将文本中的词语转换为词频矩阵
fit_transform：计算各个词语出现的次数
get_feature_names：可获得所有文本的关键词
toarray()：查看词频矩阵的结果。


    vec = CountVectorizer(ngram_range=(n, n), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    print('feature names:')
    print(vec.get_feature_names())
    print('bag of words:')
    print(bag_of_words.toarray())
    
    TF-IDF：
TF：Term Frequency，词频
一个单词的重要性和它在文档中出现的次数呈正比。
IDF：Inverse Document Frequency，逆向文档频率
一个单词在文档中的区分度。这个单词出现的文档数越少，区分度越大，IDF越大

TfidfVectorizer:
将文档集合转化为tf-idf特征值的矩阵
构造函数
analyzer：word或者char，即定义特征为词（word）或n-gram字符
ngram_range: 参数为二元组(min_n, max_n)，即要提取的n-gram的下限和上限范围
max_df：最大词频，数值为小数[0.0, 1.0],或者是整数，默认为1.0
min_df：最小词频，数值为小数[0.0, 1.0],或者是整数，默认为1.0
stop_words：停用词，数据类型为列表
功能函数：
fit_transform：进行tf-idf训练，学习到一个字典，并返回Document-term的矩阵，也就是词典中的词在该文档中出现的频次


# 使用TF-IDF提取文本特征
tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')
tfidf_matrix = tf.fit_transform(df['desc_clean'])
print(tfidf_matrix)
print(tfidf_matrix.shape)


基于内容的推荐：
Step1，对酒店描述（Desc）进行特征提取
N-Gram，提取N个连续字的集合，作为特征
TF-IDF，按照(min_df, max_df)提取关键词，并生成TFIDF矩阵
Step2，计算酒店之间的相似度矩阵
余弦相似度
Step3，对于指定的酒店，选择相似度最大的Top-K个酒店进行输出

什么是Embedding：
一种降维方式，将不同特征转换为维度相同的向量
离线变量转换成one-hot => 维度非常高，可以将它转换为固定size的embedding向量
任何物体，都可以将它转换成为向量的形式，从Trait #1到 #N
向量之间，可以使用相似度进行计算
当我们进行推荐的时候，可以选择相似度最大的


将Word进行Embedding：
我们将King与其他单词进行比较，可以看到Man和Woman更相近
同样有了向量，我们还可以进行运算
king-man+woman与queen的相似度最高


Word2Vec：
通过Embedding，把原先词所在空间映射到一个新的空间中去，使得语义上相似的单词在该空间内距离相近。
Word Embedding => 学习隐藏层的权重矩阵
输入测是one-hot编码
隐藏层的神经元数量为hidden_size（Embedding Size）
对于输入层和隐藏层之间的权值矩阵W，大小为[vocab_size, hidden_size]
输出层为[vocab_size]大小的向量，每一个值代表着输出一个词的概率


对于输入的one-hot编码：
在矩阵相乘的时候，选取出矩阵中的某一行，而这一行就是输入词语的word2vec表示
隐含层的节点个数 = 词向量的维数
隐层的输出是每个输入单词的Word Embedding
word2vec，实际上就是一个查找表


Word2Vec的两种模式：
Skip-Gram，给定input word预测上下文



CBOW，给定上下文，预测input word（与Skip-Gram相反）


Gensim工具
pip install gensim
开源的Python工具包
可以从非结构化文本中，无监督地学习到隐层的主题向量表达
每一个向量变换的操作都对应着一个主题模型
支持TF-IDF，LDA, LSA, word2vec等多种主题模型算法

使用方法：
建立词向量模型：word2vec.Word2Vec(sentences)
window,句子中当前单词和被预测单词的最大距离
min_count,需要训练词语的最小出现次数，默认为5
size,向量维度，默认为100
worker,训练使用的线程数，默认为1即不使用多线程
模型保存 model.save(fname)
模型加载 model.load(fname)


数据集：西游记
journey_to_the_west.txt
计算小说中的人物相似度，比如孙悟空与猪八戒，孙悟空与孙行者
方案步骤：
Step1，使用分词工具进行分词，比如NLTK,JIEBA
Step2，将训练语料转化成一个sentence的迭代器
Step3，使用word2vec进行训练
Step4，计算两个单词的相似度

# 字词分割，对整个文件内容进行字词分割
def segment_lines(file_list,segment_out_dir,stopwords=[]):
    for i,file in enumerate(file_list):
       segment_out_name=os.path.join(segment_out_dir,'segment_{}.txt'.format(i))
        with open(file, 'rb') as f:
            document = f.read()
            document_cut = jieba.cut(document)
            sentence_segment=[]
            for word in document_cut:
                if word not in stopwords:
                    sentence_segment.append(word)
            result = ' '.join(sentence_segment)
            result = result.encode('utf-8')
            with open(segment_out_name, 'wb') as f2:
                f2.write(result)

# 对source中的txt文件进行分词，输出到segment目录中
file_list=files_processing.get_files_list('./source', postfix='*.txt')
segment_lines(file_list, './segment')


# 将Word转换成Vec，然后计算相似度 
from gensim.models import word2vec
import multiprocessing
# 如果目录中有多个文件，可以使用PathLineSentences
sentences = word2vec.PathLineSentences('./segment')
# 设置模型参数，进行训练
model = word2vec.Word2Vec(sentences, size=100, window=3, min_count=1)
print(model.wv.similarity('孙悟空', '猪八戒'))
print(model.wv.similarity('孙悟空', '孙行者'))
# 设置模型参数，进行训练
model2 = word2vec.Word2Vec(sentences, size=128, window=5, min_count=5, workers=multiprocessing.cpu_count())
# 保存模型
model2.save('./models/word2Vec.model')
print(model2.wv.similarity('孙悟空', '猪八戒'))
print(model2.wv.similarity('孙悟空', '孙行者'))
print(model2.wv.most_similar(positive=['孙悟空', '唐僧'], negative=['孙行者']))

```

什么是N-Gram

```
```

余弦相似度计算

```
```

为酒店建立内容推荐系统

```
```

CountVectorizer与TfidfVectorizer

```
```

Word Embedding

```
```
