# 关联规则与新零售

```
推荐系统中的几种常用算法：
基于内容的推荐
内容特征表示，特征学习，推荐列表
基于协同过滤的推荐
群体智能，用户历史行为
基于关联规则的推荐
Transaction，频繁项集和关联规则挖掘

推荐系统中的几种常用算法：
基于效用的推荐
效用函数的定义
基于知识的推荐
知识图谱的创建
组合推荐
实际工作中经常采用
每种推荐算法都有自己的使用场景，可以综合考虑

TIPS：学习是个循序渐进的过程
小白第一次学习，就达到90% => 隐性大神
每次学习，都会有新的收获

```

# 关联规则
超市如何预知高中生怀孕？

```
美国明尼苏达州一家Target被客户投诉，一位中年男子指控Target将婴儿产品优惠券寄给他的女儿（高中生）。但没多久他却来电道歉，因为女儿经他逼问后坦承自己真的怀孕了。

关联规则：Association Rules，或者是 Basket Analysis
解释了：如果一个消费者购买了产品A，那么他有多大几率会购买产品B?

啤酒和尿布：
沃尔玛在分析销售记录时，发现啤酒和尿布经常一起被购买，于是他们调整了货架，把两者放在一起，结果真的提升了啤酒的销量。
原因解释：爸爸在给宝宝买尿布的时候，会顺便给自己买点啤酒？
沃尔玛是最早通过大数据分析而受益的传统零售企业，对消费者购物行为进行跟踪和分析。

支持度：是个百分比，指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的频率越大。
“牛奶”的支持度=4/5=0.8
“牛奶+面包”的支持度=3/5=0.6。

订单编号	购买的商品
1	牛奶、面包、尿布
2	可乐、面包、尿布、啤酒
3	牛奶、尿布、啤酒、鸡蛋
4	面包、牛奶、尿布、啤酒
5	面包、牛奶、尿布、可乐

置信度：是个条件概念
指的是当你购买了商品A，会有多大的概率购买商品B
置信度（牛奶→啤酒）=2/4=0.5
置信度（啤酒→牛奶）=2/3=0.67

提升度：商品A的出现，对商品B的出现概率提升的程度
如果我们单纯看置信度(可乐→尿布)=1，也就是说可乐出现的时候，用户都会购买尿布，那么当用户购买可乐的时候，就需要推荐尿布么？

提升度：商品A的出现，对商品B的出现概率提升的程度
提升度(A→B)=置信度(A→B)/支持度(B)
提升度的三种可能：
提升度(A→B)>1：代表有提升；
提升度(A→B)=1：代表有没有提升，也没有下降；
提升度(A→B)<1：代表有下降。

我们把上面案例中的商品用ID来代表，牛奶、面包、尿布、可乐、啤酒、鸡蛋的商品ID分别设置为1-6
Apriori算法就是查找频繁项集(frequent itemset)的过程
频繁项集：支持度大于等于最小支持度(Min Support)阈值的项集。
非频繁项集：支持度小于最小支持度的项集

先计算K=1项的支持度
商品项集	支持度
1	4/5
2	4/5
3	5/5
4	2/5
5	3/5
6	1/5

假设最小支持度=0.5，那么Item4和6不符合最小支持度的，不属于频繁项集
商品项集	支持度
1	4/5
2	4/5
3	5/5
5	3/5


在这个基础上，我们将商品两两组合，得到k=2项的支持度

商品项集	支持度
1，2	3/5
1，3	4/5
1，5	1/5
2，3	4/5
2，5	2/5
3，5	3/5

筛选掉小于最小值支持度的商品组合
商品项集	支持度
1，2	3/5
1，3	4/5
2，3	4/5
3，5	3/5

将商品进行K=3项的商品组合，可以得到：
商品项集	支持度
1，2，3	3/5
2，3，5	2/5
1，2，5	1/5

筛选掉小于最小值支持度的商品组合
商品项集	支持度
1，2，3	3/5

得到K=3项的频繁项集{1,2,3}，也就是{牛奶、面包、尿布}的组合。

Apriori算法的流程：
Step1，K=1，计算K项集的支持度；
Step2，筛选掉小于最小支持度的项集；
Step3，如果项集为空，则对应K-1项集的结果为最终结果。
否则K=K+1，重复1-3步。

使用工具包：
from efficient_apriori import apriori
或者：
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

from efficient_apriori import apriori
# 设置数据集
transactions = [('牛奶','面包','尿布'),
		('可乐','面包', '尿布', '啤酒'),
		('牛奶','尿布', '啤酒', '鸡蛋'),
		('面包', '牛奶', '尿布', '啤酒'),
		('面包', '牛奶', '尿布', '可乐')]
# 挖掘频繁项集和频繁规则
itemsets, rules = apriori(transactions, min_support=0.5,  min_confidence=1)
print("频繁项集：", itemsets)
print("关联规则：", rules)

频繁项集： {1: {('啤酒',): 3, ('尿布',): 5, ('牛奶',): 4, ('面包',): 4}, 2: {('啤酒', '尿布'): 3, ('尿布', '牛奶'): 4, ('尿布', '面包'): 4, ('牛奶', '面包'): 3}, 3: {('尿布', '牛奶', '面包'): 3}}
关联规则： [{啤酒} -> {尿布}, {牛奶} -> {尿布}, {面包} -> {尿布}, {牛奶, 面包} -> {尿布}]

万物皆Transaction：
超市购物小票（TransactionID => Item）
每部电影的分类（MovieID => 分类）
每部电影的演员（MovieID => 演员）


超市购物小票的关联关系
每笔订单的商品（TransactionID => Item）

BreadBasket数据集（21293笔订单）：
BreadBasket_DMS.csv
字段：Date（日期），Time（时间），Transaction（交易ID）Item（商品名称）
交易ID的范围是[1,9684]，存在交易ID为空的情况，同一笔交易中存在商品重复的情况。以外，有些交易没有购买商品（对应的Item为NONE）

# 数据加载
data = pd.read_csv('./BreadBasket_DMS.csv')
# 统一小写
data['Item'] = data['Item'].str.lower()
# 去掉none项
data = data.drop(data[data.Item == 'none'].index)

# 得到一维数组orders_series，并且将Transaction作为index, value为Item取值
orders_series = data.set_index('Transaction')['Item']
# 将数据集进行格式转换
transactions = []
temp_index = 0
for i, v in orders_series.items():
	if i != temp_index:
		temp_set = set()
		temp_index = i
		temp_set.add(v)
		transactions.append(temp_set)
	else:
		temp_set.add(v)
		
[{'scandinavian'}, {'hot chocolate', 'jam', 'cookies'}, {'muffin'}, {'bread', 'coffee', 'pastry'}, {'medialuna', 'pastry', 'muffin'}, {'tea', 'coffee', 'medialuna', 'pastry'}, {'bread', 'pastry'}, {'bread', 'muffin'}, {'scandinavian', 'medialuna'}……
transaction：订单数组
每笔订单为一个集合，去掉订单中的重复项

itemsets, rules = apriori(transactions, min_support=0.02,  min_confidence=0.5)
通过调整min_support，min_confidence可以得到不同的频繁项集和关联规则
min_support=0.02，min_confidence=0.5时
一共有33个频繁项集，8种关联规则

使用efficient_apriori工具包
效率较高，但返回参数较少

频繁项集： {1: {('alfajores',): 344, ('bread',): 3096, ('brownie',): 379, ('cake',): 983, ('coffee',): 4528, ('cookies',): 515, ('farm house',): 371, ('hot chocolate',): 552, ('juice',): 365, ('medialuna',): 585, ('muffin',): 364, ('pastry',): 815, ('sandwich',): 680, ('scandinavian',): 275, ('scone',): 327, ('soup',): 326, ('tea',): 1350, ('toast',): 318, ('truffles',): 192}, 2: {('bread', 'cake'): 221, ('bread', 'coffee'): 852, ('bread', 'pastry'): 276, ('bread', 'tea'): 266, ('cake', 'coffee'): 518, ('cake', 'tea'): 225, ('coffee', 'cookies'): 267, ('coffee', 'hot chocolate'): 280, ('coffee', 'juice'): 195, ('coffee', 'medialuna'): 333, ('coffee', 'pastry'): 450, ('coffee', 'sandwich'): 362, ('coffee', 'tea'): 472, ('coffee', 'toast'): 224}}
关联规则： [{cake} -> {coffee}, {cookies} -> {coffee}, {hot chocolate} -> {coffee}, {juice} -> {coffee}, {medialuna} -> {coffee}, {pastry} -> {coffee}, {sandwich} -> {coffee}, {toast} -> {coffee}]


from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
hot_encoded_df=data.groupby(['Transaction','Item'])['Item'].count().unstack().reset_index().fillna(0).set_index('Transaction')
hot_encoded_df = hot_encoded_df.applymap(encode_units)
frequent_itemsets = apriori(hot_encoded_df, min_support=0.02, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=0.5)

使用mlxtend.frequent_patterns工具包
效率较低，但返回参数较多


```

什么是关联规则

```
```

支持度、置信度和提升度

```
```

Apriori算法

```
```

BreadBasket：面包店购物清单的关联分析

```
```

MovieLens：电影分类中的关联分析

```


电影分类中的关联关系
每部电影的分类（MovieID => 分类）

数据集：MovieLens
下载地址：https://www.kaggle.com/jneupane12/movielens/download
主要使用的文件：movies.csv
格式：movieId	title	genres
记录了电影ID，标题和分类
我们可以分析下电影分类之间的频繁项集和关联规则
MovieLens 主要使用 Collaborative Filtering 和 Association Rules 相结合的技术，向用户推荐他们感兴趣的电影。

# 将genres进行one-hot编码（离散特征有多少取值，就用多少维来表示这个特征）
movies_hot_encoded = movies.drop('genres',1).join(movies.genres.str.get_dummies())
# 将movieId, title设置为index
movies_hot_encoded.set_index(['movieId','title'],inplace=True)
# 挖掘频繁项集，最小支持度为0.02
itemsets = apriori(movies_hot_encoded,use_colnames=True, min_support=0.02)
# 根据频繁项集计算关联规则，设置最小提升度为2
rules =  association_rules(itemsets, metric='lift', min_threshold=2)

	antecedents	consequents	antecedent support	consequent support	support	confidence	lift	leverage	conviction
9	frozenset({'Mystery'})	frozenset({'Thriller'})	0.055502603	0.153163722	0.029144365	0.525099075	3.428351502	0.02064338	1.783185154
8	frozenset({'Thriller'})	frozenset({'Mystery'})	0.153163722	0.055502603	0.029144365	0.190282432	3.428351502	0.02064338	1.16645289
15	frozenset({'Crime'})	frozenset({'Drama', 'Thriller'})	0.107742503	0.068480094	0.024965173	0.231711466	3.383632432	0.017586957	1.212461029
12	frozenset({'Drama', 'Thriller'})	frozenset({'Crime'})	0.068480094	0.107742503	0.024965173	0.364561028	3.383632432	0.017586957	1.404159228
7	frozenset({'Action'})	frozenset({'Adventure'})	0.129041719	0.08538016	0.035633111	0.276136364	3.234198251	0.024615508	1.263525054
6	frozenset({'Adventure'})	frozenset({'Action'})	0.08538016	0.129041719	0.035633111	0.417346501	3.234198251	0.024615508	1.494813439
16	frozenset({'Action'})	frozenset({'Sci-Fi'})	0.129041719	0.063897646	0.02349879	0.182102273	2.849905792	0.015253328	1.144522502
17	frozenset({'Sci-Fi'})	frozenset({'Action'})	0.063897646	0.129041719	0.02349879	0.367756741	2.849905792	0.015253328	1.377568316
0	frozenset({'Thriller'})	frozenset({'Crime'})	0.153163722	0.107742503	0.045164602	0.294877932	2.736876567	0.028662359	1.265394373
1	frozenset({'Crime'})	frozenset({'Thriller'})	0.107742503	0.153163722	0.045164602	0.419190201	2.736876567	0.028662359	1.458026844
5	frozenset({'Horror'})	frozenset({'Thriller'})	0.095718161	0.153163722	0.039335728	0.410953658	2.683100496	0.024675179	1.437639482
4	frozenset({'Thriller'})	frozenset({'Horror'})	0.153163722	0.095718161	0.039335728	0.256821446	2.683100496	0.024675179	1.216776014


电影演员中的关联关系
每部电影的演员列表（MovieID => 演员）

数据集：MovieActors
来源：movie_actors.csv
爬虫抓取 movie_actors_download.py
格式：title	actors
记录了电影标题和演员列表
我们可以分析下电影演员之间的频繁项集和关联规则

from selenium import webdriver
# 设置想要下载的导演 数据集
director = u'徐峥'
base_url = 'https://movie.douban.com/subject_search?search_text='+director+'&cat=1002&start='
# 下载指定页面的数据
def download(request_url):
# 将字典类型转化为DataFrame
movie_actors = pd.DataFrame(movie_actors, index=[0])
# DataFrame 行列转换
movie_actors = pd.DataFrame(movie_actors.values.T, index=movie_actors.columns, columns=movie_actors.index)
movie_actors.index.name = 'title'
movie_actors.set_axis(['actors'], axis='columns', inplace=True)
movie_actors.to_csv('./movie_actors.csv')

当没有现成的数据源时，可以通过爬虫进行数据抓取，保存在csv文件
爬虫抓取属于提升部分，不是本次课重点
重点理解：万物皆Transaction
掌握方法：挖掘数据集中的频繁项集和关联规则

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
# 数据加载
movies = pd.read_csv('./movie_actors.csv')
# 将genres进行one-hot编码（离散特征有多少取值，就用多少维来表示这个特征）
movies_hot_encoded = movies.drop('actors',1).join(movies.actors.str.get_dummies('/'))
# 将movieId, title设置为index
movies_hot_encoded.set_index(['title'],inplace=True)
# 挖掘频繁项集，最小支持度为0.05
itemsets = apriori(movies_hot_encoded,use_colnames=True, min_support=0.05)
# 按照支持度从大到小进行时候粗
itemsets = itemsets.sort_values(by="support" , ascending=False) 

pd.options.display.max_columns=100
# 根据频繁项集计算关联规则，设置最小提升度为2
rules =  association_rules(itemsets, metric='lift', min_threshold=2)
# 按照提升度从大到小进行排序
rules = rules.sort_values(by="lift" , ascending=False) 
#rules.to_csv('./rules.csv')

数据集：MarketBasket
下载地址：https://www.kaggle.com/dragonheir/basket-optimisation
该数据集为rawdata，没有记录TransactionID和列名
ToDo：统计交易中的频繁项集和关联规则

 Hints:
data = pd.read_csv('./Market_Basket_Optimisation.csv', header = None)

关联规则是基于transaction，而协同过滤基于用户偏好（评分）
商品组合使用的是购物篮分析，也就是Apriori算法，协同过滤计算的是相似度
关联规则没有利用“用户偏好”，而是基于购物订单进行的频繁项集挖掘

推荐使用场景：
当下的需求：
推荐的基础是且只是当前一次的购买/点击
长期偏好：
基于用户历史的行为进行分析，建立一定时间内的偏好排序

两种推荐算法的思考维度不同，很多时候，我们需要把多种推荐方法的结果综合起来做一个混合的推荐

不需要考虑用户一定时期内的偏好，而是基于Transaction
只要能将数据转换成Transaction，就可以做购物篮分析：
Step1、把数据整理成id=>item形式，转换成transaction
Step2、设定关联规则的参数（support、confident）挖掘关联规则
Step3、按某个指标（lift、support等）对以关联规则排序
最小支持度，最小置信度是实验出来的
最小支持度：
不同的数据集，最小值支持度差别较大。可能是0.01到0.5之间
可以从高到低输出前20个项集的支持度作为参考
最小置信度：可能是0.5到1之间
提升度：表示使用关联规则可以提升的倍数，是置信度与期望置信度的比值
提升度至少要大于1

基于关联规则的推荐算法：
Apriori算法
FPGrowth算法
PrefixSpan算法

Apriori在计算的过程中存在的不足：
可能产生大量的候选集。因为采用排列组合的方式，把可能的项集都组合出来了
每次计算都需要重新扫描数据集，计算每个项集的支持度

浪费了计算空间和时间

在Apriori算法基础上提出了FP-Growth算法：
创建了一棵FP树来存储频繁项集。在创建前对不满足最小支持度的项进行删除，减少了存储空间。
整个生成过程只遍历数据集2次，大大减少了计算量

理解：Apriori存在的不足，有更快的存储和搜索方式进行频繁项集的挖掘

创建项头表（item header table）
作用是为FP构建及频繁项集挖掘提供索引。
Step1、流程是先扫描一遍数据集，对于满足最小支持度的单个项（K=1项集）按照支持度从高到低进行排序，这个过程中删除了不满足最小支持度的项。
项头表包括了项目、支持度，以及该项在FP树中的链表。初始的时候链表为空。

Step2、对于每一条购买记录，按照项头表的顺序进行排序，并进行过滤。
构造FP树，根节点记为NULL节点
Step3、整个流程是需要再次扫描数据集，把Step2得到的记录逐条插入到FP树中。节点如果存在就将计数count+1，如果不存在就进行创建。同时在创建的过程中，需要更新项头表的链表。

Step4、通过FP树挖掘频繁项集
现在已经得到了一个存储频繁项集的FP树，以及一个项头表。可以通过项头表来挖掘出每个频繁项集。
挖掘从项头表最后一项“啤酒”开始。
从FP树种找到所有“啤酒”节点，向上遍历祖先节点，得到3条路径。对于每条路径上的节点，其count都设置为“啤酒”的count
具体的操作会用到一个概念，叫“条件模式基”
因为每项最后一个都是“啤酒”，因此我们把“啤酒”去掉，得到条件模式基，此时后缀模式是（啤酒）

假设{啤酒}的条件频繁集为{S1,S2,S3}，则{啤酒}的频繁集为{S1+{啤酒},S2+{啤酒},S3+{啤酒}}，此时的条件频繁项集为{{}, {尿布}}，所以啤酒的频繁项集为{啤酒}，{尿布，啤酒}

继续找项头表倒数第2项面包，求得“面包”的条件模式基
根据条件模式基，可以求得面包的频繁项集：{面包}，{尿布，面包}，{牛奶，面包}，{尿布，牛奶，面包}

继续找项头表倒数第3项面包，求得“牛奶”的条件模式基
根据条件模式基，可以求得面包的频繁项集：{牛奶}，{尿布，牛奶}

继续找项头表倒数第4项面包，求得“尿布”的条件模式基
根据条件模式基，可以求得尿布的频繁项集：{尿布}
所以全部的频繁项集为：
{啤酒}，{尿布，啤酒}
{面包}，{尿布，面包}，{牛奶，面包}，{尿布，牛奶，面包}
{牛奶}，{尿布，牛奶}
{尿布}

工具包
通过Python官方的第三方软件库
https://pypi.org/
import fptools as fp
Spark.mllib 提供并行FP-growth算法

支持度、置信度、提升度
最小支持度，最小置信度是实验出来的
基于关联规则的推荐算法：
Apriori算法
FPGrowth算法
PrefixSpan算法

万物皆Transaction：
超市购物小票（TransactionID => Item）
每部电影的分类（MovieID => 分类）
每部电影的演员（MovieID => 演员）
关联规则与协同过滤：
关联规则是基于transaction，而协同过滤基于用户偏好（评分）
商品组合使用的是购物篮分析，也就是Apriori算法，协同过滤计算的是相似度
关联规则没有利用“用户偏好”，而是基于购物订单进行的频繁项集挖掘

```

MovieActors：电影演员中的关联分析

```
```

关联规则与协同过滤的区别

```
```

关联规则中的最小支持度、最小置信度该如何确定

```
```


# 分布式推荐系统
什么是Hadoop

```
Hadoop：
Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序
Hadoop框架的核心设计：HDFS和MapReduce，一句话来说Hadoop就是存储加计算（HDFS为海量的数据提供了存储，而MapReduce为海量的数据提供了计算）
Hadoop由以下几个项目构成

Hadoop框架的主要模块：
HDFS，Hadoop Distributed File System，Hadoop分布式文件系统
MapReduce，一个计算框架。它以分布式和并行的方式处理大量的数据。当你对所有年龄> 18的用户在上述1 GB文件上执行查询时，将会有“8个映射”函数并行运行，以在其128 MB拆分文件中提取年龄> 18的用户，然后reduce函数将运行以将所有单独的输出组合成单个最终结果
YARN，用于作业调度和集群资源管理的框架
Hadoop Common，Hadoop体系最底层的一个模块，为Hadoop各子项目提供各种工具，如：配置文件和日志操作等

```

MapReduce与HDFS

```

Thinking：我们有1TB的数据，如何统计单词数？如何建立倒排索引？
这其实是很复杂的过程，如果有10TB，或者1PB数据怎么办呢？这时候单机就没办法做，变得很复杂需要想办法来解决。
Map的本质实际上是拆解，比如说有辆红色的小汽车，把它拆成零件了，这就是Map

Reduce就是组合，我们有很多汽车零件，还有很多其他各种装置零件，把他们拼装成变形金刚，这就是Reduce

MapReduce的6个部分：Input, Split, Map, Shuffle, Reduce, Finalize
split过程，输入很多文档，文档的每一行有很多不同的单词，分割之后方便找不同的worker来进行处理
Shuffle，将map的输出作为reduce的输入的过程就是shuffle，通过Shuffle把数据拉过来之后，方便后续Reduce进行计算。
Shuffle的本意是打乱的意思，在MapReduce中，Shuffle是洗牌的逆过程，即将map端的无规则输出按指定的规则“打乱”成具有一定规则的数据，相同的Key必须发送到同一个Reduce端处理
Reduce，把相同的数据放一起，计数统计，比如Car有3个就写3，River是2个就写2

MapReduce：
Map，输入每一行的ID是key，这一行的单词是value。这样后面就可以统计每个单词出现的次数
Reduce，输入的还是各个单词，但后面是一个串，表示在里面出现的次数，比如1,1,1。sum的过程就是把value（这里是1）累计到一起
MapReduce可以帮我们统计关键词和它的出现次数

MapReduce架构：
Master Worker，作为用户的代理来协调整个过程，比如让第一个Worker去拿0号数据，另一个Worker拿1号数据等等 => 分配数据的过程
每个Map Worker会在本地把数据切分开吧，写到本地的缓存或者硬盘上
Master让Reduce Worker去拿数据，他们就会从Map Worker的本地拿到数据，然后做完Reduce将结果写到最终的文件里（Finalize）

HDFS：
Hadoop分布式文件系统（HDFS）是一种分布式文件系统，
HDFS具有高度容错能力，数据自动保存多个副本。如果一个副本丢失后，它可以自动分配到其它节点作为新的副本
可以部署在低成本硬件上，通过多副本机制，提高可靠性 
数据规模：能够处理的数据规模可以达到GB，TB，甚至PB级别的数据

HDFS：
NameNode和DataNode是HDFS的两个主要组件
元数据存储在NameNode上，数据存储在DataNode的集群上
NameNode不仅要管理存储在HDFS上内容的元数据，而且要记录一些事情，比如哪些节点是集群的一部分，某个文件有几份副本等。它还要决定当集群的节点宕机或者数据副本丢失的时候系统需要做什么
存储在HDFS上的每份数据片有多份副本(replica)保存在不同的服务器上。
在本质上，NameNode是HDFS的Master(主服务器)，DataNode是Slave(从服务器)

客户端发送写请求给NameNode，要将a.log文件写入到HDFS，执行流程：
Step1，客户端发消息给NameNode，说要将a.log写入
Step2，NameNode发消息给客户端，让客户端写到DataNode A、B和D，并直接联系DataNode B
Step3，客户端发消息给DataNode B，让它保存一份a.log文件，并且发送副本给DataNode A和DataNode D
Step4，DataNode B发消息给DataNode A，让它保存a.log，并且发送一份副本给DataNode D
Step5，DataNode A发消息给DataNode D，让它保存a.log
Step6，DataNode D发确认消息给DataNode A
Step7，DataNode A发确认消息给DataNode B
Step8，DataNode B发确认消息给客户端，表示写入完成

客户端发送读请求给NameNode，想要读取a.log文件，具体的执行流程：
Step1，客户端询问NameNode它应该从哪里读取文件
Step2，NameNode发送数据块的信息给客户端。数据块信息包含了保存着文件副本的DataNode的IP地址，以及DataNode在本地硬盘查找数据块所需要的数据块ID
Step3，客户端检查数据块信息，联系相关的DataNode，请求数据块
Step4，DataNode返回文件内容给客户端，然后关闭连接，完成读操作
客户端并行从不同的DataNode中获取一个文件的数据块，然后联结这些数据块，拼成完整的文件

编写mapper.py：
import sys
for line in sys.stdin:
    line = line.strip()
    words = line.split()
    for word in words:
        print("%s\t%s" % (word, 1))
        
编写reducer.py：
import sys
countMap = {}
for line in sys.stdin:
    line = line.strip()
    word, count = line.split('\t')
    try:
        count = int(count)
    except ValueError:  #count如果不是数字的话，直接忽略掉
        continue
    if word not in countMap:
        countMap[word] = count
    else:
        countMap[word] = countMap[word] + count
for key in countMap:
    print("%s\t%s" % (key, countMap[key]))
    
Hadoop Streaming
Hadoop提供的一种编程工具，允许用户用任何可执行程序或脚本（比如Python, Ruby）作为mapper和reducer
mapper进程，hadoop将用户提交的mapper可执行脚本作为一个单独的进程加载起来
hadoop不断地将文件片段转换为行，传递到mapper进程中，mapper进程通过标准输入的方式逐行获取数据，然后将它转换为键值对，再通过标准输出的形式将这些键值对输出出去
reducer进程，hadoop将用户提交的reducer可执行脚本作为一个单独的进程加载起来
hadoop不断地将键值对(按键排序)按照一对一行的方式传递到reducer进程中，reducer进程通过标准输入的方式按行获取这些键值对，进行自定义计算后将结果进行输出

Hadoop Streaming使用方式
$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/.../hadoop-streaming.jar [genericOptions] [streamingOptions]
hadoop jar /home/software/hadoop-2.7.7/share/hadoop/tools/lib/hadoop-streaming-2.7.7.jar 
-input ./input/* 
-output ./output 
-mapper "python mapper.py" 
-reducer "python reducer.py"
mapper.py和reduce.py需要加上执行权限，chmod +x mapper.py，这两个py文件不用放在HDFS上，放在本地即可

使用命令将文本上传到HDFS中
# 创建文件夹
hdfs dfs -mkdir input
# 将*.txt 放到input目录
hdfs dfs -put *.txt input
# 查看input目录中的文件
hdfs dfs -ls input

Hadoop Streaming使用
执行后就会开启MapReduce任务，执行完成后会在MapReduce上生成/output目录，里面有两个文件:

part-00000是结果文件，_SUCCESS文件大小为0
cat part--00000 可以将结果显示出来

pyspark:
pyspark = python + spark
在pandas、numpy进行数据处理时，一次性将数据读入内存中，当数据很大时内存溢出，无法处理；此外，很多执行算法是单线程处理，不能充分利用cpu性能
spark的核心概念之一是shuffle，它将数据集分成数据块，好处是：
在读取数据时，不是将数据一次性全部读入内存中，而是分片，用时间换空间进行大数据处理
极大的利用了CPU资源
支持分布式结构，弹性拓展硬件资源。

pyspark:
在数据结构上Spark支持dataframe、sql和rdd模型
算子和转换是Spark中最重要的两个动作
算子好比是盖房子中的画图纸，转换是搬砖盖房子。有时候我们做一个统计是多个动作结合的组合拳，spark常将一系列的组合写成算子的组合执行，执行时，spark会对算子进行简化等优化动作，执行速度更快

pyspark操作:
对数据进行切片（shuffle）
config("spark.default.parallelism", 3000)
假设读取的数据是20G，设置成3000份，每次每个进程（线程）读取一个shuffle，可以避免内存不足的情况
设置程序的名字
appName("taSpark")
读文件
data = spark.read.csv(cc,header=None, inferSchema="true")

配置spark context
Spark 2.0版本之后只需要创建一个SparkSession即可
from pyspark.sql import SparkSession
spark=SparkSession \
        .builder \
        .appName('hotel_rec_app') \
        .getOrCreate()
        
# Spark+python 进行wordCount
from pyspark.sql import SparkSession
 spark = SparkSession\
    .builder\
    .appName("PythonWordCount")\
    .master("local[*]")\
    .getOrCreate()
# 将文件转换为RDD对象
lines = spark.read.text("input.txt").rdd.map(lambda r: r[0])


counts = lines.flatMap(lambda x: x.split(' ')) \
              .map(lambda x: (x, 1)) \
              .reduceByKey(lambda x, y: x + y)
output = counts.collect()
for (word, count) in output:
    print("%s: %i" % (word, count))
spark.stop()

DataFrame类似于Python中的数据表，允许处理大量结构化数据
DataFrame优于RDD，同时包含RDD的功能
# 从集合中创建RDD
rdd = spark.sparkContext.parallelize([
    (1001, "张飞", 8341, "坦克"),
    (1002, "关羽", 7107, "战士"),
    (1003, "刘备", 6900, "战士")
])

# 指定模式, StructField(name,dataType,nullable)
# name: 该字段的名字，dataType：该字段的数据类型，nullable: 指示该字段的值是否为空
from pyspark.sql.types import StructType, StructField, LongType, StringType  # 导入类型
schema = StructType([
    StructField("id", LongType(), True),
    StructField("name", StringType(), True),
    StructField("hp", LongType(), True), #生命值
    StructField("role_main", StringType(), True)
])
# 对RDD应用该模式并且创建DataFrame
heros = spark.createDataFrame(rdd, schema)
heros.show()

# 利用DataFrame创建一个临时视图
heros.registerTempTable("HeroGames")
# 查看DataFrame的行数
print(heros.count())
# 使用自动类型推断的方式创建dataframe
data = [(1001, "张飞", 8341, "坦克"),
        (1002, "关羽", 7107, "战士"),
        (1003, "刘备", 6900, "战士")]
df = spark.createDataFrame(data, schema=['id', 'name', 'hp', 'role_main'])
print(df) #只能显示出来是DataFrame的结果
df.show() #需要通过show将内容打印出来
print(df.count())

3
DataFrame[id: bigint, name: string, hp: bigint, role_main: string]
|  id|name|  hp|role_main|
+----+-------+-----+-------------+
|1001|张飞|8341|     坦克|
|1002|关羽|7107|     战士|
|1003|刘备|6900|     战士|
+----+-------+-----+-------------+

从CSV文件中读取
heros = spark.read.csv("./heros.csv", header=True, inferSchema=True)
heros.show()
从MySQL中读取
df = spark.read.format('jdbc').options(
url='jdbc:mysql://localhost:3306/wucai?useUnicode=true&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=Asia/Shanghai',
    dbtable='heros',
    user='root',
    password='passw0rdcc4'
    ).load()
print('连接JDBC，调用Heros数据表')
df.show()

# 传入SQL语句
sql="(SELECT id, name, hp_max, role_main FROM heros WHERE role_main='战士') t"
df = spark.read.format('jdbc').options(
url='jdbc:mysql://localhost:3306/wucai?useUnicode=true&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=Asia/Shanghai',
    dbtable=sql,
    user='root',
    password='passw0rdcc4' 
    ).load()
df.show()

西雅图酒店数据集：
下载地址：https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Seattle_Hotels.csv
字段：name, address, desc
基于用户选择的酒店，推荐相似度高的Top10个其他酒店
方法：计算当前酒店特征向量与整个酒店特征矩阵的余弦相似度，取相似度最大的Top-k个


TF-IDF：
TF：Term Frequency，词频
一个单词的重要性和它在文档中出现的次数呈正比。
IDF：Inverse Document Frequency，逆向文档频率
一个单词在文档中的区分度。这个单词出现的文档数越少，区分度越大，IDF越大

sklearn中的TF-IDF：
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer
CountVectorizer，只考虑词汇在文本中出现的频率
TfidfTransformer，统计CountVectorizer中每个词语的tf-idf权值。除了考量某词汇在文本出现的频率，还关注包含这个词汇的所有文本的数量
TfidfVectorizer，把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值

常用函数：
get_feature_names(), 得到所有文本的关键字
fit_transform(), 训练并且转换

使用sklearn中的TFIDF：
df = pd.read_csv('Seattle_Hotels.csv', encoding="latin-1")
# 数据探索
print(df.head())
print('数据集中的酒店个数：', len(df))
def print_description(index):
    example = df[df.index == index][['desc', 'name']].values[0]
    if len(example) > 0:
        print(example[0])
        print('Name:', example[1])
print('第10个酒店的描述：')
print_description(10)



# 建模
df.set_index('name', inplace = True)
# 使用TF-IDF提取文本特征
tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 1), min_df=0.01, stop_words='english')
tfidf_matrix = tf.fit_transform(df['desc'])
print(tfidf_matrix)
print(len(tf.get_feature_names()))
# 计算酒店之间的余弦相似度（线性核函数）
cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)
indices = pd.Series(df.index) #df.index是酒店名称

  (0, 1229)	0.13273308659550206
  (0, 389)	0.10992926735081532
  (0, 1154)	0.100329867504614
  (0, 1332)	0.09776472929991799
  (0, 905)	0.13273308659550206
……
  (151, 658)	0.061890899779281056
  (151, 625)	0.18359365972432462
  (151, 636)	0.033870139545787124
  (151, 1095)	0.027359314745524686
1417


# 基于相似度矩阵和指定的酒店name，推荐TOP10酒店
def recommendations(name, cosine_similarities = cosine_similarities):
    recommended_hotels = []
    # 找到想要查询酒店名称的idx
    idx = indices[indices == name].index[0]
    print('idx=', idx)
    # 对于idx酒店的余弦相似度向量按照从大到小进行排序
    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending = False)
    # 取相似度最大的前10个（除了自己以外）
    top_10_indexes = list(score_series.iloc[1:11].index)
    # 放到推荐列表中
    for i in top_10_indexes:
        recommended_hotels.append(list(df.index)[i])
    return recommended_hotels
print(recommendations('Hilton Seattle Airport & Conference Center'))
print(recommendations('The Bacon Mansion Bed and Breakfast'))

spark mllib库（spark3.0版本后废弃）
针对的对象是RDD
支持常见的算法，包括 分类、回归、聚类和协同过滤
mllib在2.0版本之后是维护状态，即只修复BUG，不更新新功能
spark ml库（官方推荐）
针对的对象是DataFrame
功能更全面更灵活，ml在DataFrame上的抽象级别更高，数据和操作耦合度更低，使用起来像sklearn
Spark安装很多坑，需要慢慢来
建议大家直接使用Spark ML库

分词
tokenizer，得到的分词结果转换数字向量
from pyspark.ml.feature import Tokenizer
# 将desc进行分词
tokenizer = Tokenizer(inputCol="doc_text", outputCol="doc_words")
df = tokenizer.transform(df)

TF-IDF：
Spark ML库中，TF-IDF被分成两部分，即TF (+hashing) 和 IDF
HashingTF，在哈希的同时会统计各个词条的词频
IDF，在一个数据集上应用它的fit()方法，产生一个IDFModel。这个IDFModel 接收特征向量（由HashingTF产生），然后计算每一个词在文档中出现的频次。调用IDFModel的transform方法，可以得到每一个单词对应的TF-IDF 度量值


from pyspark.ml.feature import Tokenizer, HashingTF, IDF
# 加载数据
documents = spark.createDataFrame([
    (0, "我 非常 喜欢 看 电视剧", "data1"),
    (1, "我 喜欢 看 电视剧", "data2"),
    (2, "我 喜欢 吃 苹果","data3"),
    (3, "我 喜欢 吃 苹果 看 电视剧","data4")], ["id", "doc_text", "other"])
# 转化为视图
documents.registerTempTable("doc_table")
df= spark.sql("SELECT id, doc_text FROM doc_table")
# 将desc进行分词
tokenizer = Tokenizer(inputCol="doc_text", outputCol="doc_words")
df = tokenizer.transform(df)
df.show()

# 计算每篇文档的TF-IDF
hashingTF = HashingTF(inputCol='doc_words', outputCol="doc_words_tf")
#hashingTF = HashingTF()
tf = hashingTF.transform(df).cache()
idf = IDF(inputCol='doc_words_tf', outputCol="doc_words_tfidf").fit(tf)
tfidf = idf.transform(tf).cache()
print('\n 每个文档的TFIDF')
tfidf.select('doc_words_tfidf').show(truncate=False)

在pyspark中提供了4中数据规范化：
Normalizer，作用范围是每一行，使每一个行向量的范数变换为一个单位范数
StandardScaler，将特征标准化为单位标准差
MinMaxScaler，将每一维特征线性地映射到指定的区间[min, max]，通常是[0, 1]
MaxAbsScaler，将每一维的特征变换到[-1, 1]闭区间上，通过除以每一维特征上的最大的绝对值，它不会平移整个分布，也不会破坏原来每一个特征向量的稀疏性
from pyspark.ml.feature import Normalizer
normalizer = Normalizer(inputCol="doc_words_tfidf", outputCol="norm")
tfidf = normalizer.transform(tfidf)

P阶范数：
当p取1，2，∞的时候分别是以下几种最简单的情形： 
1-范数(L1)：║x║1=│x1│+│x2│+…+│xn│ 
2-范数(L2)：║x║2=（│x1│²+│x2│²+…+│xn│²）然后开根号
∞-范数(L∞)：║x║∞=max（│x1│，│x2│，…，│xn│） 
Normalizer默认使用的是L2范数

pyspark SQL中的内置函数
pyspark.sql.functions里有许多常用的函数，可以满足日常绝大多数的数据处理需求
同时支持自己写的UDF
import pyspark.sql.functions as psf
引用SQL表中的数据类型
from pyspark.sql.types import IntegerType, DoubleType

使用PySpark对以下文本计算TF-IDF
documents = spark.createDataFrame([
    (0, "我 非常 喜欢 看 电视剧", "data1"),
    (1, "我 喜欢 看 电视剧", "data2"),
    (2, "我 喜欢 吃 苹果","data3"),
    (3, "我 喜欢 吃 苹果 看 电视剧","data4")], ["id", "doc_text", "other"])
    
使用PySpark为酒店建立内容推荐系统：
Step1，从CSV中加载数据，并进行数据探索
Step2，对desc字段进行分词
Step3，去掉停用词
add_stopwords = ['the', 'of', 'in', 'a', 'an', 'at', 'as', 'on', 'for', 'it', 'we', 'you', 'want', 'up', 'to', 'if', 'are', 'is', 'and', 'our', 'with', 'from', '-', 'your', 'so']
Step4，计算每篇文档的TF-IDF
Step5，数据规范化
Step6，计算酒店之间的相似度
Step7，根据用户指定的酒店进行推荐Top10

电影推荐系统：
数据集：MovieLens
ratings.csv，movies.csv
movie数据表



user数据表

Hadoop是一个开源的大数据框架，解决了两个核心问题
数据存储问题 => HDFS分布式文件系统
分布式计算问题 => MapReduce
Spark提供了丰富的算子，同时效率更高
Hadoop中对于数据计算只提供了Map和Reduce两个操作
Spark内存中的数据分析速度比Hadoop快近100倍
pyspark使用：
pyspark可以在单机上使用，解决大数据问题，用时间换空间进行大数据处理
Spark的机器学习算法库使用 mllib, ml库（官方推荐）
数据规模大的情况，可以使用Spark ML，否则可以用sklearn
```

用Python实现wordcount

```
```

Hadoop Streaming with Python

```
```

Apache Spark基础

```
```

Python与Spark使用

```
```

Project A：酒店推荐系统

```
```

Project B：电影推荐系统

```
```
