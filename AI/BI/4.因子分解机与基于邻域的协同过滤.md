# 因子分解机与基于邻域的协同过滤

```
TIPS：MAS学习法
Multi-dimensional，当我们接触越来越多的内容，越需要总结和分析
Ask，带着问题思考，一个好的问题是收获的开始
Sharing，分享是一种学习方式

Occam's Razor: 
The simplest solution is always the best
机器学习中的正则项

```

# 因子分解机

MF的局限性

```
矩阵分解：
将矩阵拆解为多个矩阵的乘积
矩阵分解方法：
EVD（特征值分解）
SVD（奇异值分解）
求解近似矩阵分解的最优化问题
ALS（交替最小二乘法）：ALS-WR
SGD（随机梯度下降）：FunkSVD, BiasSVD, SVD++

奇异值分解可以对矩阵进行无损分解
在实际中，我们可以抽取前K个特征，对矩阵进行降维
SVD在降维中有效，抽取不同的K值（10%的特征包含99%的信息）
在评分预测中使用funkSVD，只根据实际评分误差进行目标最优化
在funkSVD的基础上，加入用户/商品偏好 => BiasSVD
在BiasSVD的基础上，考虑用户的隐式反馈 => SVD++
以上的MF，我们都只考虑user和item特征，但实际上一个预测问题包含的特征维度可能很多

什么是预测问题：
在机器学习中，可以把预测问题看成是估计一个函数

n维实特征向量        映射到目标域T
对于训练集，
回归问题：T为实数
二分类问题：T为{0，1}
n的维度往往很大，不只是二维，即userID, itemID

对于电影评分系统，特征维度也不仅仅是userID, itemID，还有其他维度：
S={(A, TI, 2010-1, 5),
(A, NH, 2010-2, 3),
(A, SW, 2010-4, 1),
(B, SW, 2009-5, 4),
(B, ST, 2009-8, 5),
(C, TI, 2009-9, 1),
(C, SW, 2009-12, 5),
...
}

U={Alice(A), Bob(B), Charlie(C), ...}
I={Titanic(TI), Notting Hill(NH), Star Wars(SW), Star Trek(ST),...}
用户在什么时间，对哪部电影，打了多少分
我们需要预测y'，即某个用户，在某个时刻，对某个电影的打分
```

FM算法

```

FM与MF的区别：
FM矩阵将User和Item都进行了one-hot编码作为特征，使得特征维度非常巨大且稀疏
矩阵分解MF是FM的特例，即特征只有User ID 和Item ID的FM模型
矩阵分解MF只适用于评分预测，进行简单的特征计算，无法利用其他特征 
FM引入了更多辅助信息（Side information）作为特征
FM在计算二阶特征组合系数的时候，使用了MF

FM中的特征交叉：
CTR（Click Through Rate）预估是个二分类问题
线性模型无法学习到特征之间的相关性，比如“ Country=USA” +“Day=26/11/15” ，“ Country=China”+“Day=19/2/15”，对用户的Click有正面影响

libFM（FM软件）
下载地址：https://github.com/srendle/libfm
使用文档：http://www.libfm.org/libfm-1.42.manual.pdf
FM论文作者Steffen Rendle提供的工具（2010年）
在KDD CUP 2012，以及Music Hackathon中都取得不错的成绩
不仅适用于推荐系统，还可以用于机器学习（分类问题）
实现三种优化算法：SGD，ALS，MCMC
支持2种输入数据格式：文本格式（推荐）和二进制格式

使用libFM自带的libsvm格式转换
triple_format_to_libfm.pl （perl文件）
-target 目标变量
-delete_column 不需要的变量
perl triple_format_to_libfm.pl -in ratings.dat -target 2 -delete_column 3 -separator "::"
自动将.dat文件 => .libfm文件


使用libFM训练FM模型
-train 指定训练集，libfm格式或者二进制文件
-test 指定测试集，libfm格式或者二进制文件
-task，说明任务类型classification还是regression
dim，指定k0，k1，k2，
-iter，迭代次数，默认100
-method，优化方式，可以使用SGD, SGDA, ALS, MCMC，默认为MCMC
-out，指定输出文件
libFM -task r -train ratings.dat.libfm -test ratings.dat.libfm -dim '1,1,8' -out out.txt

使用libFM进行分类
Titanic数据集，train.csv和test.csv
Step1，对train.csv和test.csv进行处理
去掉列名，针对test.csv增加虚拟target列（值设置为1）
Step2，将train.csv, test.csv转换成libfm格式
perl triple_format_to_libfm.pl -in ./titanic/train.csv -target 1 -delete_column 0 -separator ","
perl triple_format_to_libfm.pl -in ./titanic/test.csv -target 1 -delete_column 0 -separator ","

Titanic数据集，train.csv和test.csv
Step3，使用libfm进行训练，输出结果文件 titanic_out.txt
libFM -task c -train ./titanic/train.csv.libfm -test ./titanic/test.csv.libfm -dim '1,1,8' -out titanic_out.txt

FFM算法：
Field-aware Factorization Machines for CTR Prediction
https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf
通过引入field的概念，FFM把相同性质的特征归于同一个field，比如“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征代表日期，放到同一个field中
当“Day=26/11/15”与Country特征，Ad_type特征进行特征组合时，使用不同的隐向量（Field-aware），这是因为Country特征和Ad_type特征，本身的field不同


对于FM算法：

每个特征有唯一的一个隐向量表示，这个隐向量被用来学习与其他任何特征之间的影响。
w(ESPN)用来学习与Nike的隐性影响w(ESPN)*w(Nike)，同时也用来学习与Male的影响w(ESPN)*w(Male)。但是Nike和Male属于不同的领域，它们的隐性影响（latent effects）应该是不一样的。
对于FFM算法：

每个特征会有几个不同的隐向量，fj 是第 j 个特征所属的field



FM算法：
每个特征只有一个隐向量
FM是FFM的特例


FFM算法
每个特征有多个隐向量
使用哪个，取决于和哪个向量进行点乘

FFM算法：

隐向量的长度为 k，FFM的二次参数有 nfk 个，多于FM模型的 nk 个
由于隐向量与field相关，FFM二次项并不能够化简，计算复杂度是 
                               FFM的k值一般远小于FM的k值
特征格式：field_id:feat_id:value
field_id代表field编号，feat_id代表特征编号，value是特征值。
如果特征为数值型，只需分配单独的field编号，比如评分，item的历史CTR/CVR等。
如果特征为分类（categorical）特征，需要经过One-Hot编码成数值型，编码产生的所有特征同属于一个field。特征值是0或1，比如性别、商品的品类id等


libFFM
https://github.com/ycjuan/libffm
xlearn
https://xlearn-doc-cn.readthedocs.io/en/latest/index.html
提供Python接口
支持LR，FM，FFM算法等
运行效率高，比libfm, libffm快


criteo_ctr数据集
展示广告CTR预估比赛
欧洲大型重定向广告公司Criteo的互联网广告数据集（4000万训练样本，500万测试样本）
原始数据：https://labs.criteo.com/2013/12/download-terabyte-click-logs/
small_train.txt 和 small_test.txt文件
（FFM数据格式，200条记录）


import xlearn as xl
# 创建FFM模型
ffm_model = xl.create_ffm()
# 设置训练集和测试集
ffm_model.setTrain("./small_train.txt")
ffm_model.setValidate("./small_test.txt")
# 设置参数，任务为二分类，学习率0.2，正则项lambda: 0.002，评估指标 accuracy
param = {'task':'binary', 'lr':0.2, 'lambda':0.002, 'metric':'acc'}
# FFM训练，并输出模型
ffm_model.fit(param, './model.out')
# 设置测试集，将输出结果转换为0-1
ffm_model.setTest("./small_test.txt")
ffm_model.setSigmoid()
ffm_model.predict("./model.out", "./output.txt")


xlearn工具输入数据格式：
LR ，FM 算法： CSV 或者 libsvm
FFM 算法：libffm 格式


DeepFM算法：
DeepFM: A Factorization-Machine based Neural Network for CTR Prediction，2017
https://arxiv.org/abs/1703.04247
FM可以做特征组合，但是计算量大，一般只考虑2阶特征组合
如何既考虑低阶（1阶+2阶），又能考虑到高阶特征 => DeepFM=FM+DNN
设计了一种end-to-end的模型结构 => 无须特征工程
在各种benchmark和工程中效果好
Criteo点击率预测, 4500万用户点击记录，90%样本用于训练，10%用于测试
Company*游戏中心，10亿记录，连续7天用户点击记录用于训练，之后1天用于测试

DeepFM = FM + DNN：
提取低阶(low order)特征 => 因子分解机FM
既可以做1阶特征建模，也可以做2阶特征建模
提取高阶(high order)特征 => 神经网络DNN
end-to-end，共享特征输入
对于特征i，wi是1阶特征的权重，
Vi表示该特征与其他特征的交互影响，输入到FM模型中可以获得特征的2阶特征表示，输入到DNN模型得到高阶特征。

FM模型
1阶特征建模
通过隐向量点乘 => 得到2阶特征组合



```

FM与MF的区别是什么

```
```

FFM算法

```
```

DeepFM算法

```

Deep模型
原始特征向量维度非常大，高度稀疏，为了更好的发挥DNN模型学习高阶特征的能力，设计子网络结构（从输入层=>嵌入层），将原始的稀疏表示特征映射为稠密的特征向量。
Input Layer => Embedding Layer
不同field特征长度不同，但是子网络输出的向量具有相同维度k
利用FM模型的隐特征向量V作为网络权重初始化来获得子网络输出向量

什么是Embedding：
一种降维方式，将不同特征转换为维度相同的向量
在推荐系统中，对于离线变量我们需要转换成one-hot => 维度非常高，可以将其转换为embedding向量
原来每个Field i维度很高，都统一降成k维embedding向量

方法：接入全连接层，对于每个Field只有一个位置为1，其余为0，因此得到的embedding就是图中连接的红线，对于Field 1来说就是
FM模型和Deep模型中的子网络权重共享，也就是对于同一个特征，向量Vi是相同的

DeepFM中的模块：
Sparse Features，输入多个稀疏特征
Dense Embeddings
对每个稀疏特征做embedding，学习到他们的embedding向量(维度相等，均为k），因为需要将这些embedding向量送到FM层做内积。同时embedding进行了降维，更好发挥Deep Layer的高阶特征学习能力
FM Layer
一阶特征：原始特征相加
二阶特征：原始特征embedding后的embedding向量两两内积
Deep Layer，将每个embedding向量做级联，然后做多层的全连接，学习更深的特征
Output Units，将FM层输出与Deep层输出进行级联，接一个dense层，作为最终输出结果

激活函数
relu和tanh比sigmoid更适合
relu对于所有深度模型来说更适合

Dropout
设置为0.6-0.9之间最适合

神经元个数
每层的神经元个数越多，未必越好
设置为200-400个比较适合

隐藏层层数
层数越多，起初效果明显，之后效果不明显，甚至更差，容易过拟合
3层比较适合

执行时间
不论是CPU还是GPU，DeepFM的执行时间都是最短的

网络形状
固定(200-200-200),增长(100-200-300), 下降(300-200-100), 菱形(150-300-150).
采用固定的形状效果最好

DeepCTR工具
https://github.com/shenweichen/DeepCTR
实现了多种CTR深度模型
与Tensorflow 1.4和2.0兼容

数据集：MovieLens_Sample
包括了多个特征：user_id, movie_id, rating, timestamp, title, genres, gender, age, occupation, zip
使用DeepFM，计算RMSE值

data = pd.read_csv("movielens_sample.txt")
sparse_features = ["movie_id", "user_id", "gender", "age", "occupation", "zip"]
target = ['rating']
……
train, test = train_test_split(data, test_size=0.2)
train_model_input = {name:train[name].values for name in feature_names}
test_model_input = {name:test[name].values for name in feature_names}
# 使用DeepFM进行训练
model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')
model.compile("adam", "mse", metrics=['mse'], )
history = model.fit(train_model_input, train[target].values,batch_size=256, epochs=1, verbose=True, validation_split=0.2, )
# 使用DeepFM进行预测
pred_ans = model.predict(test_model_input, batch_size=256)


```

工具：libFM，libFFM，xlearn

```
```

使用FM对Movielens进行推荐

```
```



# 基于邻域的协同过滤

什么是邻居

```
协同过滤是推荐系统的经典算法之一
基于邻域的协同过滤(neighborhood-based)
UserCF：给用户推荐和他兴趣相似的其他用户喜欢的物品
ItemCF：给用户推荐和他之前喜欢的物品相似的物品

相似的邻居：
固定数量的邻居K-neighborhoods
不论距离原理，只取固定的K个最近的邻居
k-Nearest Neighbor，KNN
基于相似度门槛的邻居，落在以当前点为中心，距离为 K 的区域中的所有点都作为邻居

基于用户相似度，与基于物品相似度的区别：
于用户相似度是基于评分矩阵中的行向量相似度求解
基于项目相似度计算式基于评分矩阵中列向量相似度求解


基于用户的协同过滤（UserCF）
利用行为的相似度计算用户的相似度
Step1，找到和目标用户兴趣相似的用户集合
Jaccard相似度计算

余弦相似度

改进的相似度



基于用户的协同过滤（UserCF）
Step2，用户u对物品i的相似度，等价于K个邻居对物品i的兴趣度


Step3，为用户u生成推荐列表
把和用户兴趣相同的k个邻居，喜欢的物品进行汇总，去掉用户u已经喜欢过的物品，剩下按照从大到小进行推荐

基于物品的协同过滤（ItemCF）
利用行为的相似度计算物品的相似度
Step1，计算物品之间相似度

基于物品的协同过滤（ItemCF）
Step2，用户u对物品i的兴趣度，等价于物品i的K个邻居物品，受到用户u的兴趣度

Step3，为用户u生成推荐列表
和用户历史上感兴趣的物品越相似的物品，越有可能在用户的推荐列表中获得比较高的排名
预测用户u对物品的兴趣度，去掉用户u已经喜欢过的物品，剩下按照从大到小进行推荐


Surprise工具
https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html
基于邻域的协同过滤算法
knns.KNNBasic
knns.KNNWithMeans
knns.KNNWithZScore
knns.KNNBaseline

knns.KNNBasic
k，邻域的个数 默认为40
sim_options设置邻域参数，包括：
user_based，是否为基于用户的协同过滤，默认为True，也可以设置为False
name，相似度计算方式，默认为MSD，也可设置为cosine，pearson，pearson_baseline
min_support，最小支持度，对用户或者商品进行筛选
shrinkage：收缩参数（仅与Pearson correlation相似度相关）。 默认值为100

相似度度量标准	度量标准说明
1：cosine	用户（items）之间的cosine 相似度
2：msd	用户（items）之间的均方差误差
3：pearson	用户（items）之间的皮尔逊相关系数
4：pearson_baseline	计算用户（item）之间的（缩小的）皮尔逊相关系数，使用基准值进行居中而不是平均值。

knns.KNNBasicWithMeans
k，邻域的个数 默认为40
sim_options设置邻域参数，包括：
user_based，是否为基于用户的协同过滤，默认为True，也可以设置为False
name，相似度计算方式，默认为MSD，也可设置为cosine，pearson，pearson_baseline
min_support，最小支持度，对用户或者商品进行筛选
shrinkage：收缩参数（仅与Pearson correlation相似度相关）。 默认值为100

knns.KNNBasicWithZScore
k，邻域的个数 默认为40
sim_options设置邻域参数，包括：
user_based，是否为基于用户的协同过滤，默认为True，也可以设置为False
name，相似度计算方式，默认为MSD，也可设置为cosine，pearson，pearson_baseline
min_support，最小支持度，对用户或者商品进行筛选
shrinkage：收缩参数（仅与Pearson correlation相似度相关）。 默认值为100

knns.KNNBaseline
k，邻域的个数 默认为40
sim_options设置邻域参数，包括：
user_based，是否为基于用户的协同过滤，默认为True，也可以设置为False
name，相似度计算，推荐使用pearson_baseline
bsl_options，baseline优化方式，可以设置SGD,ALS等优化算法
min_support，最小支持度，对用户或者商品进行筛选
shrinkage：收缩参数（仅与Pearson correlation相似度相关）。 默认值为100

ALS和SGD作为优化方法，应用于很多优化问题
Factor in the Neighbors: Scalable and Accurate Collaborative Filtering，ACM Transactions on Knowledge Discovery from Data, 2010
Baseline算法：基于统计的基准预测线打分
bui 预测值
bu 用户对整体的偏差
bi 商品对整体的偏差

Baseline算法
使用ALS进行优化
Step1，固定bu，优化bi
Step2，固定bi，优化bu

from surprise import KNNWithMeans
from surprise import Dataset, Reader
# 数据读取
reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)
data = Dataset.load_from_file('./ratings.csv', reader=reader)
trainset = data.build_full_trainset()
# UserCF 计算得分
algo = KNNWithMeans(k=50, sim_options={'user_based': True})
algo.fit(trainset)
uid = str(196)
iid = str(302)
pred = algo.predict(uid, iid)
print(pred)

from surprise import KNNWithMeans
from surprise import Dataset, Reader
# 数据读取
reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)
data = Dataset.load_from_file('./ratings.csv', reader=reader)
trainset = data.build_full_trainset()
# ItemCF 计算得分
# 取最相似的用户计算时，只取最相似的k个
algo = KNNWithMeans(k=50, sim_options={'user_based': False, 'name': 'cosine'})
algo.fit(trainset)
uid = str(196)
iid = str(302)
pred = algo.predict(uid, iid)
print(pred)


```

UserCF

```
```

ItemCF

```
```

Surprise中的邻域算法

```
```

Baseline算法

```
```

基于邻域的协同过滤的特点

```
```

UserCF与ItemCF的区别

```
```
