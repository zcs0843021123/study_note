# 时间序列分析

```
2020.06-2020.07  交通流量预测
主要技术：Python / ARMA / ARIMA/ Prophet
项目简介：通过JetRail过去2年内乘客数据，预测未来7个月的乘客数量，为票价制定，旅客运输提供数据决策依据，通过数据分析可以了解在不同日期（星期几），不同月份的旅客数量趋势，并对可能存在的高峰期进行提前准备
主要工作：对收集的JetRail乘客数据进行数据清洗，对缺失值进行线性差值，采用ARMA/ ARIMA对时序数据进行分析，并使用Prophet将时序数据进行拆解，分析不同日期，不同月份的乘客数量变化，对于重大的事件（重大比赛，节日）等进行预测，对未来不同时间点的乘客数量有更好的预测准确率
https://github.com/xxx/jetrail_prediction

```

# 时间序列模型

```
Project A：沪市指数预测
沪市指数的历史数据（从1990年12月19日到2020年3月12日），shanghai_index_1990_12_19_to_2020_03_12.csv
请你编写代码对沪市指数未来3个月（截止到2020年6月31日）的变化进行预测（将数据转化为按月统计即可）


```

什么是时间序列预测

```
时间序列：
建立了观察结果与时间变化的关系，能帮预测未来一段时间内的结果变化情况
时间序列分析与回归分析的区别：
在选择模型前，我们需要确定结果与变量之间的关系。回归分析训练得到的是目标变量y与自变量x（一个或多个）的相关性，然后通过新的自变量x来预测目标变量y。而时间序列分析得到的是目标变量y与时间的相关性
回归分析擅长的是多变量与目标结果之间的分析，即便是单一变量，也往往与时间无关。而时间序列分析建立在时间变化的基础上，它会分析目标变量的趋势、周期、时期和不稳定因素等。这些趋势和周期都是在时间维度的基础上，是我们要观察的重要特征

时间序列：
按照时间顺序组成的数字序列
历史悠久，在中国古代的农业社会中，人们就将一年中不同时间节点和天气的规律总结了下来，形成了二十四节气，也就是从时间序列中观察天气和太阳的规律（只是当时没有时间序列模型和相应工具）
时间序列在金融、经济、商业领域拥有广泛的应用
机器学习模型，包括AR、MA、ARMA、ARIMA
神经网络模型，用LSTM进行时间序列预测

时间序列及分解：
平稳序列，stationary series
基本上不存在趋势（Trend）的序列，各观察值基本上在某个固定的水平上波动
非平稳序列，non-stationary series
包含趋势、季节性或周期性的序列，可以只有一种成分，也可能是多种成分的组合

时间序列及分解：
趋势（trend）：时间序列在长时期内呈现出来的某种持续上升或持续下降的变动，也称长期趋势
季节性（seasonality）：时间序列在一年内重复出现的周期波动。销售旺季，销售淡季，旅游旺季、旅游淡季
季节，可以是任何一种周期性变化，不一定是一年中的四季
含有季节成分的序列可能含有趋势，也可能不含有趋势
周期性（cyclicity）：通常是由经济环境的变化引起
不同于趋势变动，不是朝着单一方向的持续运动，而是涨落相间的交替波动
不同于季节变动，季节变动有比较固定的规律，变动周期大多为一年。周期性的循环波动无固定规律，变动周期多在一年以上，且周期长短不一

随机性（Irregular），指受偶然因素影响所形成的的不规则波动，在时间序列中无法预估
随机性是不规则波动，除去趋势、周期性、季节性的偶然性波动

因素	举例
长期趋势 Trend(T)	国内生产总值
季节变动 Season(S)	冰淇淋、暖宝宝、羽绒服、裙子等销售
周期性 Cyclic(C)	太阳黑子数量变化
随机性 Irregular(I)	股票市场受到突然的利好、利空等信息的影响，影响股价产生的波动


statsmodels工具：
statsmodels工具包提供统计计算，包括描述性统计以及统计模型的估计和推断
statsmodels主要包括如下子模块：
回归模型：线性回归，广义线性模型，线性混合效应模
方差分析（ANOVA）
时间序列分析：AR，ARMA，ARIMA等

import statsmodels.api as sm
# 数据加载
data = pd.read_csv('shanghai_index_1990_12_19_to_2019_12_11.csv', usecols=['Timestamp', 'Price'])
data.Timestamp = pd.to_datetime(data.Timestamp)
data = data.set_index('Timestamp')
data['Price'] = data['Price'].apply(pd.to_numeric, errors='ignore')
# 进行线性插补缺漏值
data.Price.interpolate(inplace=True)
#  返回三个部分 trend（趋势），seasonal（季节性）和residual (残留)
result = sm.tsa.seasonal_decompose(data.Price, freq=288)
result.plot()
plt.show()

AR模型：
Auto Regressive，中文叫自回归模型
认为过去若干时刻的点通过线性组合，再加上白噪声就可以预测未来某个时刻的点
日常生活环境中就存在白噪声，在数据挖掘的过程中，可以把它理解为一个期望为0，方差为常数的纯随机过程
AR模型存在一个阶数p，称为AR(p)模型，也叫作p阶自回归模型。指的是通过这个时刻点的前p个点，通过线性组合再加上白噪声来预测当前时刻点的值

AR是线性时间序列分析模型中最简单的模型，通过前面部分的数据与后面部分的数据之间的相关关系来建立回归方程：

AR(p)，表示p阶的自回归过程，     为自回归系数
     表示白噪声，是时间序列中的数值的随机波动。这些波动会相互抵消，即累计为0
如果只有一个时间记录点时，则为AR(1)，即一阶自回归过程：

MA模型：
Moving Average，中文叫做滑动平均模型
与AR模型大同小异，AR模型是历史时序值的线性组合，MA是通过历史白噪声进行线性组合来影响当前时刻点
MA模型中的历史白噪声是通过影响历史时序值，从而间接影响到当前时刻点的预测值
MA模型存在一个阶数q，称为MA(q)模型，也叫作q阶移动平均模型
AR和MA模型都存在阶数，在AR模型中，用p表示，在MA模型中用q表示，这两个模型大同小异，与AR模型不同的是MA模型是历史白噪声的线性组合

MA模型，通过前面通过将一段时间序列中白噪声序列进行加权和，可以得到移动平均方程：

MA(q)表示q阶移动平均过程，   为移动回归系数，        为不同时间点的白噪声
Xt 为第t天的股票价格，而Ut为第t天的新闻影响，当天的股票价格受当天的新闻影响，也受昨天的新闻影响（但影响力要弱些，所以要乘上系数）

ARMA模型：
Auto Regressive Moving Average，中文叫做自回归滑动平均模型
AR模型和MA模型的混合，相比AR模型和MA模型，它有更准确的估计
ARMA模型存在p和q两个阶数，称为ARMA(p,q)模型：

自回归模型结合了两个模型的特点，AR解决当前数据与后期数据之间的关系，MA则可以解决随机变动，即噪声问题

ARIMA模型：
Auto Regressive Integrated Moving Average模型，中文叫差分自回归滑动平均模型，也叫求合自回归滑动平均模型
相比于ARMA，ARIMA多了一个差分的过程，作用是对不平稳数据进行差分平稳，在差分平稳后再进行建模
ARIMA的原理和ARMA模型一样。相比于ARMA(p,q)的两个阶数，ARIMA是一个三元组的阶数(p,d,q)，称为ARIMA(p,d,q)模型，其中d是差分阶数
AR，MA是ARMA的特殊形式，而ARMA是ARIMA的特殊形式

ARIMA模型步骤：
Step1，观察时间序列数据，是否为平稳序列
Step2，对于非平稳时间序列要先进行d阶差分运算，化为平稳时间序列
Step3，使用ARIMA（p, d, q）模型进行训练拟合，找到最优的(p, d, q)，及训练好的模型
Step4，使用训练好的ARIMA模型进行预测，并对差分进行还原
ARIMA 用差分将不平稳数据先变得平稳，再用ARMA模型


关于差分：
差分=序列之间做差值，目的是为了得到平稳的序列，也就是去掉前面数值的影响
一次差分为序列之间做一次差值，二次差分为在一次差分的基础上在做一次差分
                             若x=[1,4,9,16,25….]（x有二次趋势）
一次差分的结果为[4-1,9-4,16-9,25-16…] = [3,5,7,9,11…]，此时x序列仍不平稳，有一次上升的趋势
再做一次差分为[2,2,2,2…] ，此时x为平稳序列

ARMA工具：
from statsmodels.tsa.arima_model import ARMA
ARMA(endog,order,exog=None)
endog：endogenous variable，代表内生变量，又叫非政策性变量，它是由模型决定的，不被政策左右，可以说是我们想要分析的变量，或者说是我们这次项目中需要用到的变量
order：代表是p和q的值，也就是ARMA中的阶数
exog：exogenous variables，代表外生变量。外生变量和内生变量一样是经济模型中的两个重要变量。相对于内生变量而言，外生变量又称作为政策性变量，在经济机制内受外部因素的影响，不是我们模型要研究的变量

如果我们想要创建ARMA(7,0)模型，可以写成：ARMA(data,(7,0))，其中data是我们想要观察的变量，(7,0)代表(p,q)的阶数。
fit函数，进行拟合
predict(start, end)函数，进行预测，其中start为预测的起始时间，end为预测的终止时间

# 用ARMA进行时间序列预测
from statsmodels.tsa.arima_model import ARMA
# 创建数据
data = [3821, 4236, 3758, 6783, 4664, 2589, 2538, 3542, 4626, 5886, 6233, 4199, 3561, 2335, 5636, 3524, 4327, 6064, 3912, 1356, 4305, 4379, 4592, 4233, 4281, 1613, 1233, 4514, 3431, 2159, 2322, 4239, 4733, 2268, 5397, 5821, 6115, 6631, 6474, 4134, 2728, 5753, 7130, 7860, 6991, 7499, 5301, 2808, 6755, 6658, 6944, 6372, 8380, 7366, 6352, 8333, 8281, 11548, 10823, 13642, 9973, 6723, 13416, 12205, 13942, 9590, 11693, 9276, 6519, 6863, 8237, 10122, 8646, 9749, 5346, 4836, 9806, 7502, 9387, 11078, 9832, 6886, 4285, 8351, 9725, 11844, 12387, 10666, 7072, 6429]
data=pd.Series(data)
data_index = sm.tsa.datetools.dates_from_range('1901','1990')

# 绘制数据图
data.index = pd.Index(data_index)
data.plot(figsize=(12,8))
plt.show()
# 创建ARMA模型# 创建ARMA模型
arma = ARMA(data,(7,0)).fit()
print('AIC: %0.4lf' %arma.aic)
# 模型预测
predict_y = arma.predict('1990', '2000')
# 预测结果绘制
fig, ax = plt.subplots(figsize=(12, 8))
ax = data.ix['1901':].plot(ax=ax)
predict_y.plot(ax=ax)
plt.show()

AIC准则，也叫作赤池消息准则，是衡量统计模型拟合好坏的一个标准，数值越小代表模型拟合得越好

使用ARMA工具对沪市指数进行预测：
Step1，数据加载&探索
按照不同的时间尺度（天，月，季度，年）可以将数据压缩，得到不同尺度的数据，然后做可视化呈现。这4个时间尺度上，我们选择“月”作为预测模型的时间尺度
df_month = df.resample('M').mean()
Step2，模型选择&训练，在给定范围内，选择最优的超参数
创建ARMA时间序列模型。我们并不知道p和q取什么值时，模型最优，因此可以给它们设置一个区间范围，比如都是range(0,3)，然后计算不同模型的AIC数值，选择最小的AIC数值对应的那个ARMA模型
Step3，模型预测，可视化呈现
用这个最优的ARMA模型预测未来3个月的沪市指数走势，并将结果做可视化呈现。


# 数据加载
df = pd.read_csv('./shanghai_index_1990_12_19_to_2020_03_12.csv')
df = df[['Timestamp', 'Price']]
# 将时间作为df的索引
df.Timestamp = pd.to_datetime(df.Timestamp)
df.index = df.Timestamp
# 数据探索
print(df.head())
# 按照月，季度，年来统计
df_month = df.resample('M').mean()
print(df_month)
df_Q = df.resample('Q-DEC').mean()
df_year = df.resample('A-DEC').mean()

# 按照天，月，季度，年来显示沪市指数的走势
fig = plt.figure(figsize=[15, 7])
plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签
plt.suptitle('沪市指数', fontsize=20)
plt.subplot(221)
plt.plot(df.Price, '-', label='按天')
plt.legend()
plt.subplot(222)
plt.plot(df_month.Price, '-', label='按月')
……
plt.plot(df_Q.Price, '-', label='按季度')
……
plt.plot(df_year.Price, '-', label='按年')
plt.legend()
plt.show()

# 设置参数范围
ps = range(0, 3)
qs = range(0, 3)
parameters = product(ps, qs)
parameters_list = list(parameters)
# 寻找最优ARMA模型参数，即best_aic最小
results = []
best_aic = float("inf") # 正无穷
for param in parameters_list:
    try:
        model = ARMA(df_month.Price,order=(param[0], param[1])).fit()
    except ValueError:
        print('参数错误:', param)
        continue
    aic = model.aic
    if aic < best_aic:
        best_model = model
        best_aic = aic
        best_param = param
    results.append([param, model.aic])
print('最优模型: ', best_model.summary())

# 设置future_month，需要预测的时间date_list
future_month = 3
last_month = pd.to_datetime(df_month2.index[len(df_month2)-1])
date_list = []
for i in range(future_month):
    # 计算下个月有多少天
    year = last_month.year
    month = last_month.month
    if month == 12:
        month = 1
        year = year+1
    else:
        month = month + 1
    next_month_days = calendar.monthrange(year, month)[1]
    #print(next_month_days)
    last_month = last_month + timedelta(days=next_month_days)
    date_list.append(last_month)
print('date_list=', date_list)

date_list= [Timestamp('2020-04-30 00:00:00', freq='M'), Timestamp('2020-05-31 00:00:00', freq='M'), Timestamp('2020-06-30 00:00:00', freq='M')]

# 添加未来要预测的3个月
future = pd.DataFrame(index=date_list, columns= df_month.columns)
df_month2 = pd.concat([df_month2, future])
df_month2['forecast'] = best_model.predict(start=0, end=len(df_month2))
# 第一个元素不正确，设置为NaN
df_month2['forecast'][0] = np.NaN
print(df_month2)
# 沪市指数预测结果显示
plt.figure(figsize=(30,7))
df_month2.Price.plot(label='实际指数')
df_month2.forecast.plot(color='r', ls='--', label='预测指数')
plt.legend()
plt.title('沪市指数（月）')
plt.xlabel('时间')
plt.ylabel('指数')
plt.show()


# 设置参数范围
ps = range(0, 5)
qs = range(0, 5)
ds = range(1, 2) #使用1阶差分
for param in parameters_list:
    try:
        model = sm.tsa.statespace.SARIMAX(df_month.Price,
                                order=(param[0], param[1], param[2]),
                                enforce_stationarity=False,
                                enforce_invertibility=False).fit()
    except ValueError:
        print('参数错误:', param)
        continue
    aic = model.aic
    if aic < best_aic:
    ……
# 输出最优模型
print('最优模型: ', best_model.summary())


# 添加未来要预测的3个月
future = pd.DataFrame(index=date_list, columns= df_month.columns)
df_month2 = pd.concat([df_month2, future])
# get_prediction得到的是区间，使用predicted_mean
df_month2['forecast'] = best_model.get_prediction(start=0, end=len(df_month2)).predicted_mean
# 沪市指数预测结果显示
plt.figure(figsize=(30,7))
df_month2.Price.plot(label='实际指数')
df_month2.forecast.plot(color='r', ls='--', label='预测指数')
plt.legend()
plt.title('沪市指数（月）')
plt.xlabel('时间')
plt.ylabel('指数')
plt.show()

LSTM：
LSTM，Long Short-Term Memory，长短记忆网络
可以避免常规RNN的梯度消失，在工业界有广泛应用
引入了三个门函数：输入门（Input Gate）、遗忘门（Forget Gate）和输出门（Output Gate）来控制输入值、记忆值和输出值
输入门决定了当前时刻网络的状态有多少信息需要保存到内部状态中，遗忘门决定了过去的状态信息有多少需要丢弃 => 输入门和遗忘门是LSTM能够记忆长期依赖的关键
输出门决定当前时刻的内部状态有多少信息需要输出给外部状态。


一个LSTM单元在每个时间步会接收三个输入：
      当前时刻的输入，
      自上一时刻的内部状态 
      上一时刻的外部状态  
      和       同时作为三个“门”的输入
      是Logistic函数
      
Keras的LSTM层：
keras.layers.recurrent.LSTM(units, activation, return_sequences, input_shape)
units，输出维度
activation，激活函数
return_sequences，默认False，若为True则返回整个序列，否则仅返回输出序列的最后一个输出
input_shape，是一个二元组(n_steps_in, n_features)，n_steps_in 为输入的 X 每次考虑的时间步，n_features 为输入有几个序列


# LSTM设置
n_steps = 3          #输入时间步
n_features = 1    #输入特征维度
n_steps_out = 1 # 输出时间步
model.add( LSTM(50,  activation='relu',  input_shape = (n_steps, n_features)) )
model.add(Dense(1)) #输出只有1个时间步
model.compile(optimizer='adam', loss='mse')

训练集：
X，       y
[[10 15]
 [20 25]
 [30 35]] 65
[[20 25]
 [30 35]
 [40 45]] 85
[[30 35]
 [40 45]
 [50 55]] 105
[[40 45]
 [50 55]
 [60 65]] 125
…

# LSTM设置
n_steps = 3          #输入时间步
n_features = 2    #输入特征维度
model.add( LSTM(50,  activation='relu',  input_shape = (n_steps, n_features)) )
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')


训练集：
X,          y
[[10 15 25]
 [20 25 45]
 [30 35 65]] [40 45 85]
[[20 25 45]
 [30 35 65]
 [40 45 85]] [ 50  55 105]
[[ 30  35  65]
 [ 40  45  85]
 [ 50  55 105]] [ 60  65 125]
[[ 40  45  85]
 [ 50  55 105]
 [ 60  65 125]] [ 70  75 145]
…


预测输入：
X，
70, 75, 145
80, 85, 165
90, 95, 185

# LSTM设置
n_steps = 3          #输入时间步
n_features = 3    #输入特征维度
n_steps_out = 1 # 输出时间步
model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))
model.add(Dense(n_features)) #不是1
model.compile(optimizer='adam', loss='mse')

训练集：
X,                 y
[10 20 30] [40 50]
[20 30 40] [50 60]
[30 40 50] [60 70]
[40 50 60] [70 80]
…


预测输入：
X，
[70, 80, 90]

# LSTM设置
n_steps_in = 3 #输入时间步
n_steps_out = 2 #输出时间步
n_features = 1 #输入特征维度
model = Sequential()
model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))
model.add(LSTM(100, activation='relu'))
model.add(Dense(n_steps_out))
model.compile(optimizer='adam', loss='mse')


训练集：
X,              y
[[10 15]
 [20 25]
 [30 35]] [65 
                 85]
[[20 25]
 [30 35]
 [40 45]] [ 85
                105]
[[30 35]
 [40 45]
 [50 55]] [105 
                 125]
…


预测输入：
X，
[40 45]
[50 55]
[60 65]

# LSTM设置
n_steps_in = 3 #输入时间步
n_steps_out = 2 #输出时间步
n_features = 2 #输入特征维度
model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))
model.add(LSTM(100, activation='relu'))
model.add(Dense(n_steps_out))
model.compile(optimizer='adam', loss='mse')

训练集：
X,                                y
[[10 15 25]
 [20 25 45]
 [30 35 65]]     [[ 40  45  85]
                           [ 50  55 105]]
[[20 25 45]
 [30 35 65]
 [40 45 85]]     [[ 50  55 105]
                           [ 60  65 125]]
[[ 30  35  65]
 [ 40  45  85]
 [ 50  55 105]] [[ 60  65 125]
                           [ 70  75 145]]
…


预测输入：
X，
[[ 40  45  85]
 [ 50  55 105]
 [ 60  65 125]]

# LSTM设置
n_steps_in = 3 #输入时间步
n_steps_out = 2 #输出时间步
n_features = 3 #输入特征维度

model.add(LSTM(200, activation='relu', input_shape=(n_steps_in, n_features)))
model.add(RepeatVector(n_steps_out))
model.add(LSTM(200, activation='relu', return_sequences=True))
model.add(TimeDistributed(Dense(n_features)))
model.compile(optimizer='adam', loss='mse')

TimeDistributed的使用：
TimeDistributed是Keras中的包装器
model = Sequential()
# 将模型的shape由(None, 10,16) 变成 (None, 10, 8)
model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))
TD层和Dense配合使用，主要应用于一对多，多对多的情况
input_shape = (10,16)，表示步长是10，特征维度为16，
首先使用TimeDistributed(Dense(8), input_shape = (10,16)) 把每一步的维度为16变成8，不改变步长的大小
若该层的batch输入shape为(50, 10, 16)，则这一层之后的输出shape为(50, 10, 8)

RepeatVector的使用：
RepeatVector(n) 将输入重复n次
model = Sequential()
# 原来模型的output_shape = (None, 32)，`None` 是batch 维度
model.add(Dense(32, input_dim=32))
# 使用RepeatVector，将模型output_shape 设置为 (None, 3, 32)
model.add(RepeatVector(3))
如果输入的形状为(None,32)，经过添加RepeatVector(3)层之后，输出变为(None,3,32), RepeatVector不改变我们的步长，改变我们的每一步的维数（即：属性长度）

# 设置观测数据input(X)的步长（时间步），epochs，batch_size
timesteps_in = 3
timesteps_out = 3
epochs = 500
batch_size = 100
data = pd.read_csv('./shanghai_index_1990_12_19_to_2019_12_11.csv')
data_set = data[['Price']].values.astype('float64')
# 转化为可以用于监督学习的数据
train_x, label_y, test_data= get_train_set(data_set, timesteps_in=timesteps_in, timesteps_out=timesteps_out)
# 使用LSTM进行训练、预测
lstm_model(data_set, train_x, label_y, epochs, batch_size, test_data, timesteps_out=timesteps_out)


# 转化为可以用于监督学习的数据
def get_train_set(data_set, timesteps_in, timesteps_out=1):
    train_data_set = np.array(data_set)
    reframed_train_data_set = np.array(series_to_supervised(train_data_set, timesteps_in, timesteps_out).values)
    print(reframed_train_data_set)
    print(reframed_train_data_set.shape)
    train_x, train_y = reframed_train_data_set[:, :-timesteps_out], reframed_train_data_set[:, -timesteps_out:]
    # 将数据集重构为符合LSTM要求的数据格式,即 [样本数，时间步，特征]
    train_x = train_x.reshape((train_x.shape[0], timesteps_in, 1))
    # test_data是用于计算correlation与spearman-correlation而存在
    test_data = train_data_set[train_x.shape[0] + timesteps_out:, :]
    #pd.DataFrame(reframed_train_data_set).to_csv('temp.csv')
    return train_x, train_y, test_data
    
# 将时间序列数据转换为适用于监督学习的数据
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    cols, names = list(), list()
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]
    # 预测序列 (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]
        else:
            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]
    agg = concat(cols, axis=1)
    agg.columns = names
    # 去掉NaN行
    if dropnan:
        agg.dropna(inplace=True)
    return agg
    
# 使用LSTM进行预测
def lstm_model(source_data_set, train_x, label_y, input_epochs, input_batch_size, test_data, timesteps_out):
    model = Sequential()  
    # 第一层, 隐藏层神经元节点个数为128, 返回整个序列
    model.add(LSTM(128, return_sequences=True, activation='tanh', input_shape=(train_x.shape[1], train_x.shape[2])))
    # 第二层，隐藏层神经元节点个数为128, 只返回序列最后一个输出
    model.add(LSTM(128, return_sequences=False))
    model.add(Dropout(0.5))
    # 第三层 因为是回归问题所以使用linear
    model.add(Dense(timesteps_out, activation='linear'))
    model.compile(loss='mean_squared_error', optimizer='adam')
    # LSTM训练 input_epochs次数
    res = model.fit(train_x, label_y, epochs=input_epochs, batch_size=input_batch_size, verbose=2, shuffle=False)
    # 模型预测
    test_predict = model.predict(train_x)
    # 串联多个迭代对象，形成更大的迭代对象
    test_data_list = list(chain(*test_data))
    test_predict_list = list(chain(*test_predict))


    print(model.summary())
    plot_img(source_data_set, test_predict)
# 呈现原始数据，训练结果，验证结果，预测结果
def plot_img(source_data_set, train_predict):
    plt.figure(figsize=(24, 8))
    # 原始数据蓝色
    plt.plot(source_data_set[:, -1], c='b')
    # 训练数据绿色
    plt.plot([x for x in train_predict], c='g')
    plt.legend()
    plt.show()
    
    
    # 设置观测数据input(X)的步长（时间步），epochs，batch_size
timesteps_in = 3
timesteps_out = 3
epochs = 3000
batch_size = 100
# 使用LSTM进行训练、预测
lstm_model(data_set, train_x, label_y, epochs, batch_size, timesteps_out=timesteps_out)


# 设置观测数据input(X)的步长（时间步），epochs，batch_size
timesteps_in = 3
timesteps_out = 3
epochs = 10000
batch_size = 100
# 使用LSTM进行训练、预测
lstm_model(data_set, train_x, label_y, epochs, batch_size, timesteps_out=timesteps_out)

PM2.5数据集：
https://archive.ics.uci.edu/ml/datasets/Beijing%20PM2.5%20Data
PM2.5的分级标准为：
优 35微克（ug）/每立方
良 35~75微克（ug）/每立方
轻度污染 75~115微克（ug）/每立方
中度污染 115~150微克（ug）/每立方
重度污染 150~250微克（ug）/每立方
严重污染 250及以上微克（ug）/每立方

DEWP: 露点
TEMP: 温度
PRES:气压
cbwd: 组合风向
Iws: 风速
Is：积累雪量
Ir：积累雨量

# 将原始数据格式中的year, month, day, hour进行合并，并保存新的文件pollution.csv
from datetime import datetime
# 数据加载
def parse(x):
	return datetime.strptime(x, '%Y %m %d %H')
dataset = read_csv('./raw.csv',  parse_dates = [['year', 'month', 'day', 'hour']], index_col=0, date_parser=parse)
dataset.drop('No', axis=1, inplace=True)
# 列名替换
dataset.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']
dataset.index.name = 'date'
# 缺失值填充
dataset['pollution'].fillna(0, inplace=True)
# 去掉第一天数据（前24小时）
dataset = dataset[24:]
dataset.to_csv('pollution.csv')


# 数据加载
dataset = read_csv('pollution.csv', header=0, index_col=0)
values = dataset.values
print(values)
print(values.shape)
# 设置需要可视化的列表
groups = [0, 1, 2, 3, 4, 5, 6, 7]
i = 1
# 数据探索EDA
plt.figure()
for group in groups:
	plt.subplot(len(groups), 1, i)
	plt.plot(values[:, group])
	plt.title(dataset.columns[group], y=0.5, loc='right')
	i += 1
plt.show()

[[129.0 -16 -4.0 ... 1.79 0 0]
 [148.0 -15 -4.0 ... 2.68 0 0]
 [159.0 -11 -5.0 ... 3.57 0 0]
 ...
 [10.0 -22 -3.0 ... 242.7 0 0]
 [8.0 -22 -4.0 ... 246.72 0 0]
 [12.0 -21 -3.0 ... 249.85 0 0]]
(43800, 8)


# 将分类特征wnd_dir 进行标签编码
encoder = LabelEncoder()
values[:,4] = encoder.fit_transform(values[:,4])
# 设置数据类型均为flast32
values = values.astype('float32')
pd.DataFrame(values).to_csv('values.csv')
# 0-1规范化
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)
print(scaled)

# 将时间序列数据转换为适合监督学习的数据
reframed = series_to_supervised(scaled, 1, 1)
reframed.to_csv('reframed-1.csv')
# 去掉不需要预测的列，即var2(t)	var3(t)	var4(t)	var5(t)	var6(t)	var7(t)	var8(t)
reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)
reframed.to_csv('reframed-2.csv')

# 训练集80%，测试集20%
values = reframed.values
n_train_hours = int(len(values) * 0.8)
train = values[:n_train_hours, :]
test = values[n_train_hours:, :]
# :-1表示从0到数组最后一位，-1表示数组最后一位
train_X, train_y = train[:, :-1], train[:, -1]
test_X, test_y = test[:, :-1], test[:, -1]
# 转换成3D格式 [样本数, 时间步, 特征数]
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))

# 设置网络模型
model = Sequential()
model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dense(1))
model.compile(loss='mae', optimizer='adam')
# 模型训练
result = model.fit(train_X, train_y, epochs=10, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)
# 模型预测
train_predict = model.predict(train_X)
test_predict = model.predict(test_X)

# 绘制训练损失和测试损失
plt.plot(result.history['loss'], label='train')
plt.plot(result.history['val_loss'], label='test')
plt.show()

Train on 35039 samples, validate on 8760 samples
Epoch 1/10
 - 5s - loss: 0.0354 - val_loss: 0.0313
Epoch 2/10
 - 3s - loss: 0.0143 - val_loss: 0.0146
Epoch 3/10
 - 3s - loss: 0.0140 - val_loss: 0.0134
Epoch 4/10
 - 3s - loss: 0.0140 - val_loss: 0.0133
Epoch 5/10
 - 3s - loss: 0.0139 - val_loss: 0.0132
Epoch 6/10
 - 3s - loss: 0.0140 - val_loss: 0.0133
Epoch 7/10
 - 3s - loss: 0.0139 - val_loss: 0.0133
Epoch 8/10
 - 3s - loss: 0.0140 - val_loss: 0.0133
Epoch 9/10
 - 3s - loss: 0.0139 - val_loss: 0.0134
Epoch 10/10
 - 3s - loss: 0.0139 - val_loss: 0.0135

 
 print(model.summary())
# 绘制预测结果与实际结果对比
plot_img(values, train_predict, test_predict)


Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 50)                11800     
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 51        
=================================================================
Total params: 11,851
Trainable params: 11,851
Non-trainable params: 0

时间序列是结构化数据，每个时间戳就有一个数值
研究时间序列可以方便我们对未来进行预测，或者异常检测
应用场景：
金融股票价格预测，资金流入流出预测
运维中的异常检测，日活月活时间序列异常检测
预测方法：
统计学方法 ARMA, ARIMA
from statsmodels.tsa.arima_model import ARMA
statsmodels提供统计计算，包括描述性统计、统计模型的估计和推断
使用RNN/LSTM进行预测
keras.layers.recurrent.LSTM(units, activation, return_sequences, input_shape)


```

AR、MA、ARMA、ARIMA模型

```
```

使用ARMA/ARIMA对沪市指数进行预测

```
```

使用LSTM进行预测

```
```

Project A：对沪市指数走势进行预测

```
```

Project B：对北京PM2.5值进行预测

```
```




# Facebook Prophet

时间序列预测工具 prophet

```
Project ：沪市指数预测
沪市指数的历史数据（从1990年12月19日到2020年3月12日）
请你编写代码对沪市指数未来3个月（截止到2020年6月31日）的变化进行预测（将数据转化为按月统计即可）

ARMA/ARIMA 统计模型的不足:
ARMA，要求时序数据是稳定的，现实数据很难符合
ARIMA，模型为线性模型，无法处理非线性关系，同时要求数据点的间隔等长，比如X1和X2间隔一个小时，那么X2和X3也间隔一个小时
如果数据缺失，则需要使用插值等方法来预估缺失值，然后再使用预估值来进行参数拟合，这样会引入噪音

prophet:
facebook开源的时间序列预测工具https://facebook.github.io/prophet/
Prophet是一个基于相加模型（ additive model）的时间预测，可以精准的拟合非线性的周期趋势
对yearly、weekly和daily的周期性使用非线性拟合，亮点在于Prophet模型还添加了holidays（影响因子），可以很好的对节日（比如十一、春节等）带来的活跃数据的突变进行预测

prophet的优势：
处理数据丢失问题
趋势迁移问题（shifts in the trend）
异常的数据点（outliers）
prophet模型： y(t)=g(t)+s(t)+h(t)+e
g(t)代表趋势项，用来表示时间序列中非周期性的变化
s(t)代表周期项，用来表示时间序列中的周期变化
h(t)代表活动效果项，用来表达时间序列中的一些异常活动，例节假日，购物节等
e 用来表示不能被模型所描述的异常误差

prophet模型： y(t)=g(t)+s(t)+h(t)+e
g(t)：趋势项在Prophet中有两种实现方法，第一种是饱和增长模型，第二种是分段线性模型
s(t)：运用傅里叶级数作为周期项，使得预测模型具有灵活的周期效应
h(t)：实际上，很多节日不是在一年中固定的一天发生，
比如母亲节（五月的第二个礼拜天），中国的春节
在Prophet模型中，用户在进行预测前可以向模型输入活动表格 => Prophet的预测会更加准确

prophet 工具使用：
pip3 install fbprophet
遵循 sklearn 库的使用接口，
模型拟合，fit （一般1-5秒）
model = Prophet()
model.fit(df)
模型预测，predict
forecast = model.predict(future)

prophet模型：
Trend趋势，对时间序列中的趋势部分拟合分段线性函数，线性拟合会将特殊点和缺失数据的影响降到最小
饱和增长
通常情况下，增长会有最大容量限制，比如未来12个月某app在某地区的下载量，最大下载量要小于等于该地区手机用户总数
基于这样的领域知识，分析师可以定义模型的容量限制为C(t)
突变点，随着突变点数量的增多，拟合变得更灵活。在研究趋势成分时，分析师要面临两个基本问题，即过拟合与欠拟合
changepoint_prior_scale参数，可以调整趋势的灵活性，解决过拟合/欠拟合，参数值越大，拟合的时间序列曲线越灵活

参数	描述
growth	‘linear’或‘logistic’用来规定线性或逻辑曲线趋势
changepoints	包括潜在突变点的日期列表（不指明则默认为自动识别）
n_changepoints	若不指定突变点，则需要提供自动识别的突变点数量
changepoint_prior_scale	设定自动突变点选择的灵活性

prophet模型：
季节性，拟合并预测季节的效果，基于傅里叶级数提出了一个灵活的模型：


P代表周期，年度的P=365.25，周数据的P=7
对季节性建模，在给定N的情况下，估计参数[a1,b1……aN, bN]
傅里叶阶数N是重要的参数，用来定义模型中是否考虑高频变化：
如果分析师认为高频变化的成分只是噪声，可以将N取较低值
如果不是噪音，可以将N设置为较高值，提升预测精度

prophet模型：
活动效果项，即节假日和大事件
他们都是重要的时间因素，比如中国传统春节，在这个期间人们会购买大量新商品
允许分析师使用过去和未来事件的自定义列表，这些大事件前后的日期将会被单独考虑，并且通过拟合附加的参数模拟节假日和事件的效果

参数	描述
yearly_seasonality	周期为年的季节性
weekly_seasonality	周期为周的季节性
daily_seasonality	周期为日的季节性
holidays	内置的节假日名称和日期
seasonality_priori_scale	改变季节模型的强度
holiday_prior_scale	改变假日模型的强度

Project : 佩顿·曼宁维基百科访问流量预测
数据集，维基百科上面对美国橄榄球运动员佩顿·曼宁的日访问记录，2905条数据（2007年12月10日到2016年1月20日）
Prophet 的输入量通常包含两列的数据框：ds 和 y 
ds 列包含日期（YYYY-MM-DD）或者是具体的时间点（YYYY-MM-DD HH:MM:SS）
y 列是数值变量，表示我们希望去预测的量

ds	y
2007/12/10	9.590761139
2007/12/11	8.519590316
2007/12/12	8.183676583
2007/12/13	8.072467369
2007/12/14	7.893572074
2007/12/15	7.783640596
2007/12/16	8.414052432
……	……
2016/1/17	9.273878393
2016/1/18	10.33377535
2016/1/19	9.125871215
2016/1/20	8.891374009


Prophet工具使用：
make_future_dataframe方法，将未来的日期扩展指定的天数，得到一个数据框。默认情况下，做会自动包含历史数据的日期，因此也可以用来查看模型对于历史数据的拟合效果
predict 方法，对每一行future 日期得到预测值（yhat ）预测 forecast 创建的对象应当是新的DataFrame，其中包含一列预测值 yhat ，以及成分的分析和置信区间
plot_components方法，查看预测的成分分析
查看forecast都有哪些列：print(forecast.columns)


成分分析的绘制：
trend趋势，来自trend字段
yearly趋势，来自yearly字段
weekly趋势，来自weekly字段
因为是加法模型，所以：
forecast['additive_terms'] = forecast['weekly'] + forecast['yearly']
forecast['yhat']  = forecast['trend'] +  forecast['additive_terms'] 
forecast['yhat']  = forecast['trend'] +forecast['weekly'] + forecast['yearly']
如果有节假日因素，那么
forecast['yhat']  = forecast['trend'] +forecast['weekly'] + forecast['yearly'] + forecast['holidays']

查看forecast.tail()
会发现'multiplicative_terms', 'multiplicative_terms_lower', 'multiplicative_terms_upper'这3列为空，因为是加法模型
成分分析趋势解读
weekly中的Monday为0.035的意思就是，在trend的基础上，加0.035
Saturday为-0.3的意思就是，在trend的基础上，减0.3
因此，weekly这条线的高低反应了销量的趋势
Thinking：一年这种哪个月份，销量最高？


预测饱和增长
Prophet在预测增长情况时，会存达到极值，比如总人口数等，这里称为承载能力（carrying capacity），这时上限就是趋于饱和
新建一列 cap 来指定承载能力的大小，通常情况下这个值应当通过市场规模的数据或专业知识来决定，比如
df['cap'] = 8.5
注意：DataFrame每行都必须指定 cap值，但不一定是恒定值，如果市场规模在不断地增长，那么 cap 也可以是不断增长的序列
预测饱和减少（市场的最低floor）
logistic增长模型还可以处理饱和最小值，方法与指定最大值的列的方式相同


趋势突变点
真实的时间序列数据往往存在一些突变点
Prophet 将自动监测到这些点，并对趋势做适当地调整
默认下， Prophet 会识别出 25 个潜在的突变点（均匀分布在在前 80% 的时间序列数据中），绝大多数突变点并不会包含在建模过程中
# 显示突变点的位置
from fbprophet.plot import add_changepoints_to_plot
fig = m.plot(forecast)
a = add_changepoints_to_plot(fig.gca(), m, forecast)

指定突变点的位置
使用 changepoints 参数
m = Prophet(changepoints=['2014-01-01'])
m.fit(df)
future = m.make_future_dataframe(periods=365)
forecast = m.predict(future)
m.plot(forecast)

对节假日建模
创建一个新的DataFrame，包含两列（节假日 holiday 和日期戳 ds ）
注意：这个DataFrame必须包含所有出现的节假日（不仅是历史数据集中，还是要预测的时期中的）
比如，所有佩顿 · 曼宁参加过的季后赛 与 决赛日期
playoffs = pd.DataFrame({
  'holiday': 'playoff',
  'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16',
                        '2010-01-24', '2010-02-07', '2011-01-08',
                        '2013-01-12', '2014-01-12', '2014-01-19',
                        '2014-02-02', '2015-01-11', '2016-01-17',
                        '2016-01-24', '2016-02-07']),
  'lower_window': 0,
  'upper_window': 1,
})
superbowls = pd.DataFrame({
  'holiday': 'superbowl',
  'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']),
  'lower_window': 0,
  'upper_window': 1,
})
holidays = pd.concat((playoffs, superbowls))


对节假日建模
这个DataFrame创建好了，就可以通过传入 holidays 
m = Prophet(holidays=holidays)
m.fit(df)
future = m.make_future_dataframe(periods=365)
forecast = m.predict(future)
可以通过 forecast 数据框，展示节假日效应
print(forecast[(forecast['playoff'] + forecast['superbowl']).abs() > 0][['ds', 'playoff', 'superbowl']][-10:])

             ds   playoff  superbowl
2190 2014-02-02  1.217571   1.230312
2191 2014-02-03  1.898042   1.466063
2532 2015-01-11  1.217571   0.000000
2533 2015-01-12  1.898042   0.000000
2901 2016-01-17  1.217571   0.000000
2902 2016-01-18  1.898042   0.000000
2908 2016-01-24  1.217571   0.000000
2909 2016-01-25  1.898042   0.000000
2922 2016-02-07  1.217571   1.230312
2923 2016-02-08  1.898042   1.466063

Project D：交通流量预测
JetRail高铁的乘客数量预测
数据集：jetrail.csv，根据过往两年的数据（2012 年 8 月至 2014 年 8月），需要用这些数据预测接下来 7 个月的乘客数量
以每天为单位聚合数据集

m = Prophet(yearly_seasonality=True, seasonality_prior_scale=0.1)
# 预测未来7个月，213天
future = m.make_future_dataframe(periods=213)

ID	Datetime	Count
0	25-08-2012 00:00	8
1	25-08-2012 01:00	2
2	25-08-2012 02:00	6
3	25-08-2012 03:00	2
4	25-08-2012 04:00	2
5	25-08-2012 05:00	2
……	……	……
18286	25-09-2014 22:00	580
18287	25-09-2014 23:00	534

Project ：沪市指数预测
沪市指数的历史数据（从1990年12月19日到2020年3月12日）
请你编写代码对沪市指数未来3个月（截止到2020年6月31日）的变化进行预测（将数据转化为按月统计即可）

Prophet 针对的是商业预测任务
优点：不需要特征工程就能得到趋势，季节因素和节假日因素
不足：无法利用更多的信息，如在预测商品的销量时，无法利用商品的信息，门店的信息，促销的信息等
传入prophet的数据分为两列 ds 和 y
ds表示时间戳（pandas的日期格式）
y表示true value，也是需要预测的值（数值型）
带holidays参数的prophet
m = Prophet(holidays=holidays)

生成未来日期
future = model.make_future_dataframe(periods=365)
future为时间轴，在原有基础上增加365天（会包括之前的历史时间戳）
模型训练 model.fit(df)
模型预测 forecast = model.predict(future)
成分分析，plot_components(forecast)
绘制trend, weekly, yearly趋势图，会包括之前历史时间戳预测

forecaset字段包括：
ds, 时间轴
trend, trend_lower, trend_upper, 趋势
yhat, yhat_lower, yhat_upper,  预测值
weekly, weekly_lower, weekly_upper, 星期趋势
yearly, yearly_lower, yearly_upper,  年趋势
additive_terms, additive_terms_lower, additive_terms_upper, 加法模型趋势（星期趋势+年趋势），即 forecast['additive_terms'] = forecast['weekly'] + forecast['yearly']
multiplicative_terms, multiplicative_terms_lower, multiplicative_terms_upper，乘法模型趋势，如果使用加法模型时，multiplicative_terms为空

趋势变化点 model.changepoints
趋势变化点：时间序列经常会在轨迹中发生突然变化，可以自动检测出这些点
当模型训练完之后，就可以找到趋势变化点，默认为25个，分布在前80%的时间序列中
可以使用参数n_changepoints设置潜在变化点的数量，比如
model= prophet (n_changepoints=30)
可以使用参数changepoint_range设置前多少的时间序列来寻找潜在变化点，比如在时间序列的前90%处寻找潜在的变化点
model = Prophet(changepoint_range=0.9)
人工指定突变点的位置：
model = Prophet(changepoints=['2014-01-01'])

指定预测类型
 growth='linear'或growth = "logistic" 
默认的增长趋势为linear
如果使用growth="logistic"，就需要指定cap，因为预测时需要用到cap，可以不指定floor，因为logistic默认的最小饱和值是0
m = Prophet(growth='logistic') 
df['cap'] = 6 # 不设置会报错
模型的学习方式
默认情况下为加性的，如果改成乘性的(multiplicative)，需要设置seasonality_mode='multiplicative'

Prophet中的参数设置：
Capacity，在增量函数是逻辑回归函数的时候，需要设置的容量值
Change Points：通过 n_changepoints 和 changepoint_range 来设置时间序列的变化点
季节性和节假日，可以根据实际的业务需求来指定相应的节假日
光滑参数： 
changepoint_prior_scale 设置趋势项的灵活度，即跟随性，默认为0.05，值越大，拟合的跟随性越好，可能会过拟合
seasonality_prior_scale 用来控制季节项的灵活度
holidays_prior_scale 用来控制节假日的灵活度

Project：网络影响力分析与可视化
技术点：不同渠道的data aggregation，影响力权重，综合指数，可视化dashboard
Project：金融数据分析与风险控制
技术点：GBDT/XGBoost/LightGBM，时间序列分析，风控模型
Project：新零售数据分析与销售策略制定
技术点：BCG四象限分析，产品关联分析，apriori，BI报表

数据源：产品表.csv，订单表.csv，客户.csv，日期表.csv
通过数据分析，找出与某个产品关联度高的产品（按照关联客户占比，购买金额划分）
=> 制定销售策略

Thinking：
领导让你对产品之间的关联分析进行可视化呈现，你会怎么做？
我们要处理一个数据分析项目，一般分为几个步骤

数据分析的架构（3A）：
Data Aggregation
Data Analysis
Data Activation

数据中台
数据分析
业务自动化

BI工具：
Power BI
Tableau
FineBI
Metabase 
Superset（由Airbnb数据部门开源）


数据源：产品表.csv，订单表.csv，客户.csv，日期表.csv
找出与某个产品关联度高的产品（按照关联客户占比，购买金额划分）

Step1，可视化布局：
插入文本框，显示BI报表的标题
插入切片器，用于筛选
时间切片器、关联产品切片器
插入表，用于显示具体的数据
比如 产品A，产品A客户数，产品A的销售额，同时购买A和B的客户数，关联客户比
插入散点图，用于显示四象限（关联客户比，A客户购买B的金额）

Step2，新建度量值：
如果你不想对每行都计算，而是想获取行的汇总值时，可以使用“度量值”
与计算列的区别：
计算列是在数据模型刷新的时候，使用表格当前行的数据作为上下文，不会随着用户对透视表的操作而改变
度量值，是对聚合数据进行操作，使用透视表中的上下文
比如，在透视表中，使用表格的行列进行筛选，数据源表格数据通过这些筛选条件进行聚合和计算（也就是度量值使用用户操作的上下文进行的筛选聚合）
（数据透视表是一种可以快速汇总大量数据的交互式方法）

数据分析表达式 (DAX) ：
逻辑判断函数
AND(<logical1>,<logical2>)  
Thinking：A =IF(AND(10 > 9, -10 < -1), "Both true", "One or more false") 应该为多少？

IF(logical_test,<value_if_true>, value_if_false)
SWITCH(<expression>,条件1,值1,条件2,值2)

函数	说明
SUM()	求和
MAX()	最大值
MIN()	最小值
DISTINCTCOUNT()	去重后列所包含的元素个数
COUNTROWS()	表的行数
COUNT()	列中包含数字的单元的数目
COUNTA()	列中不为空的单元的数目
COUNTBLANK()	列中空白单元的数目

CALCULATE(<expression>,<filter1>,<filter2>...)
作用：在指定筛选器filter所修改的上下文中对表达式求值
返回值：作为表达式结果的值
To Do：在表中插入 产品数量，产品数量1，产品数量2，产品数量3

产品编号	产品名称	品牌	类别	零售价格
1001	途昂	大众	SUV	359900
1002	途观	大众	SUV	220000
1003	朗逸	大众	轿车	115900
2001	昂科威	通用	SUV	189900
2002	昂科拉	通用	SUV	155900
2003	英朗	通用	轿车	152700
3001	Q5	奥迪	SUV	399600
3002	Q7	奥迪	SUV	699800
3003	A6L	奥迪	轿车	348200

产品数量 = COUNTROWS('产品表')
产品数量1 = CALCULATE([产品数量])
产品数量2 = CALCULATE([产品数量],'产品表'[品牌]="大众")
产品数量3 = CALCULATE([产品数量],ALL('产品表'))

CALCULATETABLE，计算并返回表（不是标量值）
AVERAGEX，在表中计算表达式的平均值
关联客户占比 平均值 = AVERAGEX(ALL('产品表'[产品名称]),[关联客户占比])
NATURALINNERJOIN，内部连接，返回两个表公共列的交集及其他列

DAX 的列表函数:
FILTER，比如FILTER (ALL (Table), Condition)
VALUES ( <TableName Or ColumnName> )
VALUES 是一个表函数，可以用列名或表名作为参数，VALUES 返回参数在当前筛选上下文中的所有可见值。
①建立虚拟表,该表与所筛选的原表有关联,无需建立关系
②在使用filter，calculate,countrows, sumx, topn这些函数，如果需要引用表而不是列时，values就可以将列转换成表

ALL，函数目的正好与FILTER函数相反
ALL(表) 或者ALL(列,列,...)
ALL('产品表') 返回整个产品表
ALL('订单'[产品名称]) 返回订单表中不重复的产品名称
DISTINCT(表[列])，返回不重复值的列表
DISTINCT('订单表'[客户ID])，从事实表中提取某一列，生成维度表
COUNTROWS(DISTINCT('订单表'[客户ID]))，计算不重复的客户ID数量

SELECTEDVALUE(表[列], 备用值(可选参数))
SELECTEDVALUE，上下文过滤为一个值时，返回该值
参数：
第一个参数必须：表的列，不能是表达式
第二个参数是可选，当上下文过滤后为空，或者多个值时，返回该备用值，不填时默认为BLANK()

通常用于度量值，获取外部上下文筛选器，比如动态指标分析时，使用它获取外部切片器的信息
当切片器选中“销售额”时，计算度量值[销售额]
当切片器选中“利润”时，计算度量值[利润]

指标数据 = 
SWITCH(TRUE(), SELECTEDVALUE('数据表'[分析指标])="销售额",[销售额],
SELECTEDVALUE('数据表'[分析指标])="利润",[利润],[销售额]
)

Thinking：新建度量值“产品AB是否一致”
SELECTEDVALUE，上下文过滤为一个值时，返回该值
参数：
产品AB是否一致 = IF(SELECTEDVALUE('产品表'[产品名称])=SELECTEDVALUE('关联产品表'[产品名称]),1,0)

USERELATIONSHIP实现两个表之间的关系调用
不返回任何值，该函数只在计算期间启用指定的关系，使用时不区分两个列参数的前后顺序
DIVIDE(分子,分母,[替换结果])
安全除法函数，[替换结果]默认不需要输入即为空白（BLANK），也可以替换成其它结果

绘制BI报表的标题：
选择文本框，输入文字“产品关联分析”
插入切片器（关联产品B）：
字段：产品名称（来自关联产品表）
可以重命名为“关联产品B”
插入表（显示 产品A和产品B的关联度）
值：产品A，产品A客户数
同时购买A和B的客户数
产品A的销售额，A客户购买B的金额

数据表加载
再次加载“产品表”，命名为“关联产品表”
关联产品与订单表 是虚线关系
日期表中的日期与 订单表中的订单日期建立关系

销售额 = SUM('订单表'[销售金额])
客户数 = COUNTROWS(VALUES('订单表'[客户ID]))
产品AB是否一致 = IF(SELECTEDVALUE('产品表'[产品名称])=SELECTEDVALUE('关联产品表'[产品名称]),1,0)
产品A的销售额 = IF([产品AB是否一致],BLANK(),[销售额])
产品A客户数 = IF([产品AB是否一致],BLANK(),[客户数])

同时购买A和B的客户数 = 
VAR Bcustomer=
CALCULATETABLE(
VALUES('订单表'[客户ID]),
USERELATIONSHIP('关联产品表'[产品ID],'订单表'[产品ID]),
ALL('产品表')
)
RETURN IF([产品AB是否一致],BLANK(),CALCULATE([客户数],Bcustomer))
关联客户占比 = DIVIDE([同时购买A和B的客户数],[客户数])
关联客户占比 平均值 = AVERAGEX(ALL('产品表'[产品名称]),[关联客户占比])

关联产品B的销售额 = CALCULATE([销售额],USERELATIONSHIP('关联产品表'[产品ID],'订单表'[产品ID]),ALL('产品表') )
A客户购买B的金额 = 
VAR Acustomer=CALCULATETABLE(VALUES('订单表'[客户ID]))
VAR Bcustomer=CALCULATETABLE(VALUES('订单表'[客户ID]),USERELATIONSHIP('关联产品表'[产品ID],'订单表'[产品ID]),ALL('产品表'))
RETURN IF([产品AB是否一致],BLANK(),CALCULATE([关联产品B的销售额],NATURALINNERJOIN(Acustomer,Bcustomer)))
A客户购买B的金额 平均值 = AVERAGEX(ALL('产品表'[产品名称]),[A客户购买B的金额])

绘制散点图：
详细信息：产品名称
X轴：A客户购买B的金额
Y周：关联客户占比
绘制四象限：在分析按钮中，选择平均值线
打开类别标签
格式->形状->三角形
数据颜色->将默认颜色修改为fx，格式模式 字段值，一句为 散点图配色

度量值：散点图配色
散点图配色 = 
SWITCH(TRUE(),
    AND([A客户购买B的金额]>=[A客户购买B的金额 平均值],[关联客户占比]>=[关联客户占比 平均值]),"#FF6440",
    AND([A客户购买B的金额]>=[A客户购买B的金额 平均值],[关联客户占比]<[关联客户占比 平均值]),"#FFBC00",
    AND([A客户购买B的金额]<[A客户购买B的金额 平均值],[关联客户占比]>=[关联客户占比 平均值]),"#00B74A",
    AND([A客户购买B的金额]<[A客户购买B的金额 平均值],[关联客户占比]<[关联客户占比 平均值]),"#7277D8"
)

如何打造好的DashBoard体验：
限定在一个屏幕可以查看的内容，太多的信息/数据会造成读者的混乱
考虑从上到下的阅读习惯，上面放关键的KPI，下面放些细节信息
限定仪表板包含3到5个关键的KPI,图表或图形
提供足够的上下文，将有关联性的图形放在一起
避免没有任何解释的单一数字出现
不要为了漂亮而选择没必要的图形
```

饱和增长

```
```

突变点

```
```

节日与大事件

```
```

Project C：页面流量预测

```
```

Project D：交通流量预测

```
```



